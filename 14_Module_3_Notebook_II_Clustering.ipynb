{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zackives/upenn-cis5450-hw/blob/main/14_Module_3_Notebook_II_Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cy6hpgnJY-Rp"
      },
      "source": [
        "# Finding Clusters\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autograder setup"
      ],
      "metadata": {
        "id": "E7pUp7HAwQp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW WHO\n",
        "#TO ASSIGN POINTS TO YOU IN OUR BACKEND\n",
        "STUDENT_ID = 99999999 # YOUR PENN-ID GOES HERE AS AN INTEGER##PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW WHO"
      ],
      "metadata": {
        "id": "WGM-vv4owbBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile notebook-config.yaml\n",
        "\n",
        "grader_api_url: 'https://23whrwph9h.execute-api.us-east-1.amazonaws.com/default/Grader23'\n",
        "grader_api_key: 'flfkE736fA6Z8GxMDJe2q8Kfk8UDqjsG3GVqOFOa'"
      ],
      "metadata": {
        "id": "OxKJ5J_awSOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%set_env HW_ID=cis5450_25f_HW9"
      ],
      "metadata": {
        "id": "RfX8o5jjwe0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install penngrader-client"
      ],
      "metadata": {
        "id": "S3n0LqhuwW2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from penngrader.grader import *\n",
        "\n",
        "grader = PennGrader('notebook-config.yaml', os.environ['HW_ID'], STUDENT_ID, STUDENT_ID)"
      ],
      "metadata": {
        "id": "dEHXUnRqwcP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Looking at Glass... ... Again\n",
        "\n",
        "Recall the glass dataset from the PCA notebook."
      ],
      "metadata": {
        "id": "obNjyVNtCzvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Glass data from https://archive.ics.uci.edu/ml/machine-learning-databases/glass/"
      ],
      "metadata": {
        "id": "Qr_7sk7SC1yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://archive.ics.uci.edu/static/public/42/glass+identification.zip"
      ],
      "metadata": {
        "id": "ZUoaLM-FDHAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip glass+identification.zip"
      ],
      "metadata": {
        "id": "tBHrw9Z1EAjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load into a dataframe, with the header in row 0\n",
        "import pandas as pd\n",
        "\n",
        "glass_df = pd.read_csv('glass.data',header=None,names=['ID','RefractiveIndex','Na','Mg','Al','Si','K','Ca','Ba','Fe','Label'])\n",
        "\n",
        "glass_df"
      ],
      "metadata": {
        "id": "dLvTxx84DJ4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glass_df.set_index('ID')\n",
        "glass_types_df = glass_df[['Label']]\n",
        "\n",
        "# We don't really need these\n",
        "glass_df = glass_df.drop(columns=['ID', 'Label'])\n",
        "\n",
        "display(glass_df)\n",
        "display(glass_types_df)"
      ],
      "metadata": {
        "id": "zbNgDiFuFIE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unsupervised Machine Learning!\n",
        "\n",
        "We typically set up machine learning problems as follows.\n",
        "\n",
        "1. Convert from categorical and other values into numeric values\n",
        "2. Convert from dataframes to arrays\n",
        "3. Separate out any classes / labels (like glass type)\n",
        "\n",
        "We will call the *input data*  $X$ and the *labels* $y$."
      ],
      "metadata": {
        "id": "DZ778RCnHsku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the problem\n",
        "\n",
        "X = glass_df.to_numpy()\n",
        "y = glass_types_df.to_numpy()"
      ],
      "metadata": {
        "id": "u-iBcIhnHonu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running PCA\n",
        "\n",
        "We'll use, for the first time, a standard sckikit-learn 'flow': create a model, `fit` it, and `transform` the data."
      ],
      "metadata": {
        "id": "4lnupjGiI1ZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Standardizing the features based on unit variance\n",
        "X = StandardScaler().fit_transform(X)\n",
        "\n",
        "print (X.shape)\n",
        "print(X)"
      ],
      "metadata": {
        "id": "coF57-uXlOjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=6)\n",
        "X_embedded = pca.fit_transform(X)[:,1:3]"
      ],
      "metadata": {
        "id": "6_vgEtYoI_8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering Algorithm"
      ],
      "metadata": {
        "id": "OtGO3Bvv7AvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Algorithm, in Full Detail\n",
        "\n",
        "Let's start with the basic algorithm.  We'll split it into three components:\n",
        "1. The clustering coefficient or `error` function -- how far away are points?\n",
        "2. The function to get the most appropriate cluster for a point, `get_nearest`.\n",
        "3. The main K-Means algorithm.\n",
        "\n",
        "For this version we will initialize with randomly chosen points in the dataset, then iteratively recompute until we reach convergence (which is detected when every point remains in its current cluster)."
      ],
      "metadata": {
        "id": "qiWbxf1d7GRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from random import randint\n",
        "from scipy.spatial import distance\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def error(x,c):\n",
        "  \"\"\"\n",
        "  Error (distance, clustering coefficient) between a point x\n",
        "  and the centroid c.  We are using Euclidean distance, which\n",
        "  can also be thought of as the sum of the squared error.\n",
        "  \"\"\"\n",
        "  err = np.linalg.norm(x - c)\n",
        "\n",
        "  return err\n",
        "\n",
        "def get_nearest(c_list, x):\n",
        "  nearest = -1\n",
        "  nearest_error = np.inf\n",
        "  for i, v in enumerate(c_list):\n",
        "    if error(x, v) < nearest_error:\n",
        "      nearest_error = error(x, v)\n",
        "      nearest = i\n",
        "  return nearest\n",
        "\n",
        "def kmeans(X, k, show):\n",
        "  \"\"\"\n",
        "  Simple k-means algorithm\n",
        "  \"\"\"\n",
        "  # Initialize the centroids to random points\n",
        "  # in the data\n",
        "  centroids = np.zeros((k,2))\n",
        "  cluster_assignments = [0 for i in range(len(X))]\n",
        "  for i in range(0, k):\n",
        "    centroids[i] = X[randint(0, X.shape[0])]\n",
        "\n",
        "  if show:\n",
        "    print('Initialized centroids to: ')\n",
        "    print(centroids)\n",
        "\n",
        "  iteration = 1\n",
        "  changed = True\n",
        "  while changed:\n",
        "    if show:\n",
        "      print ('Iteration %d'%iteration)\n",
        "      iteration = iteration + 1\n",
        "\n",
        "    # Nothing happened in this iteration, by default\n",
        "    changed = False\n",
        "    # Assign points to clusters\n",
        "    for i,x in enumerate(X):\n",
        "      nearest = get_nearest(centroids, x)\n",
        "      # We changed a cluster mapping!\n",
        "      if nearest != cluster_assignments[i]:\n",
        "        changed = True\n",
        "      cluster_assignments[i] = nearest\n",
        "\n",
        "    if changed:\n",
        "      # Recompute clusters\n",
        "      for i in range(len(centroids)):\n",
        "        points = [j for j,v in enumerate(cluster_assignments) if v == i]\n",
        "        if show:\n",
        "          print ('Cluster %d'%i)\n",
        "        X_subset = np.array([[X[i,0],X[i,1]] for i in points])\n",
        "\n",
        "        if len(X_subset):\n",
        "          centroids[i][0] = np.sum(X_subset[:, 0]) / len(points)\n",
        "          centroids[i][1] = np.sum(X_subset[:, 1]) / len(points)\n",
        "          print (centroids[i])\n",
        "    elif show:\n",
        "      print('Converged!')\n",
        "\n",
        "  return (centroids, np.array(cluster_assignments))\n",
        "\n",
        "\n",
        "k = 2\n",
        "plt.scatter(X_embedded[:,0], X_embedded[:,1], c='red', marker='o', s=50)\n",
        "plt.grid()\n",
        "plt.show()\n",
        "centroids, assignments = kmeans(X_embedded, k, True)\n",
        "\n",
        "assignments"
      ],
      "metadata": {
        "id": "0w5lNNhw7HB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting along with the Centroids\n",
        "clust0 = np.array([x for i, x in enumerate(X_embedded) if assignments[i] == 0])\n",
        "clust1 = np.array([x for i, x in enumerate(X_embedded) if assignments[i] == 1])\n",
        "\n",
        "plt.scatter(clust0[:,0], clust0[:,1], c='red', marker='o', s=50)\n",
        "plt.scatter(clust1[:,0], clust1[:,1], c='black', marker='o', s=50)\n",
        "plt.scatter(centroids[:,0], centroids[:,1], marker='*', s=50, c='g')"
      ],
      "metadata": {
        "id": "nfFRQfw07Ket"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "km = KMeans(2, init='random', n_init=1, max_iter=300, random_state=0)\n",
        "km.fit(X_embedded)"
      ],
      "metadata": {
        "id": "bvG98TzA7K85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "km.cluster_centers_"
      ],
      "metadata": {
        "id": "w6uNAjuL7OCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "km.labels_"
      ],
      "metadata": {
        "id": "hfduqfEy7P6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting along with the Centroids\n",
        "clust0 = np.array([x for i, x in enumerate(X_embedded) if km.labels_[i] == 0])\n",
        "clust1 = np.array([x for i, x in enumerate(X_embedded) if km.labels_[i] == 1])\n",
        "\n",
        "plt.scatter(clust0[:,0], clust0[:,1], c='red', marker='o', s=50)\n",
        "plt.scatter(clust1[:,0], clust1[:,1], c='black', marker='o', s=50)\n",
        "plt.scatter(centroids[:,0], centroids[:,1], marker='*', s=50, c='g')"
      ],
      "metadata": {
        "id": "6tEV7-Xi7QR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### k-Means in SQL\n",
        "\n",
        "For this one we'll use the college dataset from Kaggle.  Here we have both private and public universities, as well as many statistics.\n",
        "\n",
        "The hope is that we can find 2 natural clusters.  Let's see!"
      ],
      "metadata": {
        "id": "Y4O_EDJr7TPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%set_env SPARK_VERSION=3.5.6"
      ],
      "metadata": {
        "id": "X-L1b1boriqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Let's install Apache Spark on Colab\n",
        "\n",
        "!wget -nc https://downloads.apache.org/spark/spark-$SPARK_VERSION/spark-$SPARK_VERSION-bin-hadoop3.tgz\n",
        "!tar xf spark-$SPARK_VERSION-bin-hadoop3.tgz\n",
        "!pip install findspark\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-\" + os.environ['SPARK_VERSION'] + \"-bin-hadoop3\""
      ],
      "metadata": {
        "id": "brYGHdUcruWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "spark = SparkSession.builder.appName('Clustering').getOrCreate()"
      ],
      "metadata": {
        "id": "dw4YS6OTrzMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample data\n",
        "km_data = pd.DataFrame(pd.read_csv('https://www.cis.upenn.edu/~zives/college-data.csv'))\n",
        "km_data['private'] = km_data['private'] == 'Yes'\n",
        "\n",
        "km_data.info()\n"
      ],
      "metadata": {
        "id": "dp7kYFsS7XSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "km_schema = StructType([StructField(\"index\", IntegerType(), True),\n",
        "                        StructField(\"private\", FloatType(), True),\n",
        "                        StructField(\"apps\", FloatType(), True),\n",
        "                        StructField(\"accept\", FloatType(), True),\n",
        "                        StructField(\"enroll\", FloatType(), True),\n",
        "                        StructField(\"top10perc\", FloatType(), True),\n",
        "                        StructField(\"top25perc\", FloatType(), True),\n",
        "                        StructField(\"f_undergrad\", FloatType(), True),\n",
        "                        StructField(\"p_undergrad\", FloatType(), True),\n",
        "                        StructField(\"outstate\", FloatType(), True),\n",
        "                        StructField(\"room_board\", FloatType(), True),\n",
        "                        StructField(\"books\", FloatType(), True),\n",
        "                        StructField(\"personal\", FloatType(), True),\n",
        "                        StructField(\"phd\", FloatType(), True),\n",
        "                        StructField(\"terminal\", FloatType(), True),\n",
        "                        StructField(\"s_f_ratio\", FloatType(), True),\n",
        "                        StructField(\"perc_alumni\", FloatType(), True),\n",
        "                        StructField(\"expend\", FloatType(), True),\n",
        "                        StructField(\"grad_rate\", FloatType(), True)\n",
        "                        ])\n",
        "\n",
        "# Standardizing the features based on unit variance\n",
        "km_data_2 = StandardScaler().fit_transform(km_data)\n",
        "km_data_2_df = pd.DataFrame(km_data_2)\n",
        "km_data_2_df.reset_index(inplace=True)\n",
        "#km_data_2_df.info()\n",
        "#print(km_data_2_df)\n",
        "km_data_2_df.info()\n",
        "\n",
        "km_data_sdf = spark.createDataFrame(km_data_2_df, km_schema)\n",
        "km_data_sdf.show()\n",
        "\n",
        "#km_data.info()\n"
      ],
      "metadata": {
        "id": "-Po8l-UR7X8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "km_data_sdf.schema"
      ],
      "metadata": {
        "id": "p6COTBeX7Zqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = 2\n",
        "km_data_sdf.createOrReplaceTempView('km_data')\n",
        "\n",
        "def initialize():\n",
        "  ######################\n",
        "  ## Initialize clusters.  It's better to do this randomly, but we'll use the\n",
        "  ## first k=2 rows\n",
        "  km_cluster_centroids_sdf = spark.sql(\"\"\"\n",
        "            SELECT index as cluster_id,apps,accept,enroll,top10perc,top25perc,f_undergrad,p_undergrad,outstate,room_board,books,personal,phd,terminal,s_f_ratio,perc_alumni,expend,grad_rate\n",
        "            FROM km_data TABLESAMPLE (\"\"\" + str(k) + ' ROWS)')\n",
        "\n",
        "  km_cluster_centroids_sdf.show()\n",
        "  km_cluster_centroids_sdf.createOrReplaceTempView('km_cluster_centroids')\n",
        "  return km_cluster_centroids_sdf"
      ],
      "metadata": {
        "id": "oqF_uXoT7bQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def assign_clusters():\n",
        "      # Distance between each point & each centroid\n",
        "    km_data_to_cluster_dist_sdf =\\\n",
        "      spark.sql(\"\"\"\n",
        "                SELECT c.cluster_id,k.index,(pow(c.apps - k.apps, 2)+\n",
        "                                              pow(c.accept - k.accept, 2)+\n",
        "                                              pow(c.enroll - k.enroll, 2)+\n",
        "                                              pow(c.top10perc - k.top10perc, 2)+\n",
        "                                              pow(c.top25perc - k.top25perc, 2)+\n",
        "                                              pow(c.f_undergrad - k.f_undergrad, 2)+\n",
        "                                              pow(c.p_undergrad - k.p_undergrad, 2)+\n",
        "                                              pow(c.outstate - k.outstate, 2)+\n",
        "                                              pow(c.room_board - k.room_board, 2)+\n",
        "                                              pow(c.personal - k.personal, 2)+\n",
        "                                              pow(c.phd - k.phd, 2)+\n",
        "                                              pow(c.terminal - k.terminal, 2)+\n",
        "                                              pow(c.s_f_ratio - k.s_f_ratio, 2)+\n",
        "                                              pow(c.perc_alumni - k.perc_alumni, 2)+\n",
        "                                              pow(c.expend - k.expend, 2)+\n",
        "                                              pow(c.grad_rate - k.grad_rate, 2)\n",
        "                ) as dist\n",
        "                FROM km_cluster_centroids c CROSS JOIN km_data k\n",
        "                \"\"\")\n",
        "    km_data_to_cluster_dist_sdf.createOrReplaceTempView('km_data_to_cluster_dist')\n",
        "\n",
        "    # SparkSQL is very limited in subqueries.  We want for each index,\n",
        "    # the cluster_id which minimizes the cluster distance.  We do two steps for\n",
        "    # each data item: (1) find the shortest distance to ANY cluster ID,\n",
        "    # (2) find the cluster ID whose distance equals the shortest distance.\n",
        "    km_data_to_cluster_best_dist_sdf = spark.sql('''\n",
        "              SELECT index,min(dist) as dist\n",
        "              FROM km_data_to_cluster_dist\n",
        "              GROUP BY index''')\n",
        "    km_data_to_cluster_best_dist_sdf.createOrReplaceTempView('km_data_to_cluster_best_dist')\n",
        "\n",
        "    km_data_to_cluster_sdf = spark.sql('''\n",
        "              SELECT kd.index, kd.cluster_id\n",
        "              FROM km_data_to_cluster_dist kd JOIN km_data_to_cluster_best_dist kb\n",
        "                ON kd.index = kb.index\n",
        "              WHERE kd.dist = kb.dist''')\n",
        "    km_data_to_cluster_sdf.createOrReplaceTempView('km_data_to_cluster')\n",
        "    km_data_to_cluster_sdf.show()\n",
        "    return km_data_to_cluster_sdf\n"
      ],
      "metadata": {
        "id": "Iyo7OSQz7dfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_centroids():\n",
        "  km_clusters_sdf = spark.sql('''\n",
        "            SELECT cluster_id,AVG(apps) AS apps,AVG(accept) AS accept,\n",
        "               AVG(enroll) AS enroll,AVG(top10perc) AS top10perc,\n",
        "               AVG(top25perc) AS top25perc,AVG(f_undergrad) AS f_undergrad,\n",
        "               AVG(p_undergrad) AS p_undergrad,AVG(outstate) AS outstate,\n",
        "               AVG(room_board) AS room_board,AVG(books) AS books,\n",
        "               AVG(personal) AS personal,AVG(phd) AS phd,\n",
        "               AVG(terminal) AS terminal,AVG(s_f_ratio) AS s_f_ratio,\n",
        "               AVG(perc_alumni) AS perc_alumni,AVG(expend) AS expend,\n",
        "               AVG(grad_rate) AS grad_rate\n",
        "            FROM km_data_to_cluster kc JOIN km_data kd ON kc.index = kd.index\n",
        "            GROUP BY kc.cluster_id\n",
        "            ''')\n",
        "  km_clusters_sdf.createOrReplaceTempView('km_cluster_centroids')\n",
        "  km_clusters_sdf.show()\n",
        "  return km_clusters_sdf"
      ],
      "metadata": {
        "id": "c1SjgcNa7n0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_iter = 1\n",
        "print('Initial data:')\n",
        "spark.sql('select * from km_data').show(5)\n",
        "print('Initial cluster centroids:')\n",
        "km_cluster_centroids_sdf = initialize()\n",
        "km_cluster_centroids_sdf.show()\n",
        "for i in range(max_iter):\n",
        "  km_data_to_cluster_sdf = assign_clusters()\n",
        "  # print ('Data-cluster assignments')\n",
        "  # spark.sql('select * from km_data_to_cluster order by index').show(5)\n",
        "  km_cluster_centroids_sdf = compute_centroids()\n",
        "  # print ('New cluster centroids')\n",
        "  # spark.sql('select * from km_cluster_centroids').show()\n",
        "\n",
        "km_cluster_centroids_sdf.explain()\n"
      ],
      "metadata": {
        "id": "ZCU7VbFr7phY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering in MLlib"
      ],
      "metadata": {
        "id": "XUALc3V47siC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "\n",
        "# Convert from dataframe features to a \"features\" column that is a vector\n",
        "vecAssembler = VectorAssembler(inputCols=['apps','accept','enroll',\\\n",
        "                                          'top10perc','top25perc','f_undergrad',\\\n",
        "                                          'p_undergrad','outstate','room_board',\\\n",
        "                                          'books','personal','phd','terminal',\\\n",
        "                                          's_f_ratio','perc_alumni','expend',\\\n",
        "                                          'grad_rate'], outputCol=\"features\")\n",
        "vecAssembler = VectorAssembler(inputCols=['apps','accept','enroll',\\\n",
        "                                          'top10perc','top25perc','f_undergrad',\\\n",
        "                                          'p_undergrad','outstate','room_board',\\\n",
        "                                          'books','personal','phd','terminal',\\\n",
        "                                          's_f_ratio','perc_alumni','expend',\\\n",
        "                                          'grad_rate'], outputCol=\"features\")\n",
        "df_kmeans = vecAssembler.transform(km_data_sdf).select('index', 'features')\n",
        "df_kmeans.show()\n",
        "\n",
        "# Trains a k-means model.\n",
        "kmeans = KMeans().setK(2).setSeed(5)\n",
        "\n",
        "model = kmeans.fit(df_kmeans)\n",
        "\n",
        "predictions = model.transform(df_kmeans)\n",
        "predictions.show()"
      ],
      "metadata": {
        "id": "tbW3fYPt7vLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Choosing the Right *k* for Clustering\n",
        "\n",
        "To pick the right value of *k* for our data, we will search the space of possible values -- looking at the smallest one that (roughly) minimizes the sum squared error (Euclidean distance), aka the distortion."
      ],
      "metadata": {
        "id": "f8-uXoO67yfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_embedded"
      ],
      "metadata": {
        "id": "e1OQbHi-uOw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distortions = []\n",
        "\n",
        "# Convert X_embedded to Spark DF\n",
        "num_features = X_embedded.shape[1]\n",
        "feature_cols = [f\"feature_{i}\" for i in range(num_features)]\n",
        "\n",
        "X_pdf = pd.DataFrame(X_embedded, columns=feature_cols)\n",
        "X_sdf = spark.createDataFrame(X_pdf)\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=feature_cols,\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "\n",
        "final_sdf = assembler.transform(X_sdf)\n",
        "\n",
        "max_k = 10\n",
        "for i in range(2,max_k+1):\n",
        "  km = KMeans(k=i,\n",
        "              maxIter=300,\n",
        "              seed=0)\n",
        "  model = km.fit(final_sdf)\n",
        "  # The distortion is called inertia in SciKit and trainingCost in MLLib\n",
        "  distortions.append(model.summary.trainingCost)\n",
        "\n",
        "plt.plot(range(2,max_k+1), distortions, marker='o')\n",
        "plt.xlabel('Cluster count (k)')\n",
        "plt.ylabel('Distortion')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9NBTiS9z7z8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise\n",
        "\n",
        "Let's switch back to SciKit rather than MLlib.\n",
        "\n",
        "Go back to the original glass data, re-run PCA *with no losses*, and find the optimal number of clusters.  Use the k-means++ algorithm (see [here](https://scikit-learn.org/1.5/modules/generated/sklearn.cluster.KMeans.html) for the parameters).  Search for up to 15 clusters.\n",
        "\n",
        "You should select the \"elbow\" as the point where the slope of the line changes from a steeper to a shallower point."
      ],
      "metadata": {
        "id": "sZCvjQKHtT7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "X_embedded = #TODO"
      ],
      "metadata": {
        "id": "Ov0KcazkjiVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distortions = []\n",
        "\n",
        "max_k = 15\n",
        "# TODO\n",
        "\n",
        "plt.plot(range(1,max_k+1), distortions, marker='o')\n",
        "plt.xlabel('Cluster count (k)')\n",
        "plt.ylabel('Distortion')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Xs0Lj054mpNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How many dimensions should we use, to get 95% explained variance ratio?\n",
        "clusters = # TODO for the number of clusters\n",
        "clusters"
      ],
      "metadata": {
        "id": "dmGxgY_RmTNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grader.grade('kmeans', clusters)"
      ],
      "metadata": {
        "id": "MICXh_o6HezG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CxSt80L-IeRB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}