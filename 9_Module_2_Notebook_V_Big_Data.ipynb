{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zackives/upenn-cis5450-hw/blob/main/9_Module_2_Notebook_V_Big_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cy6hpgnJY-Rp"
      },
      "source": [
        "# Big Data and Graph Data\n",
        "\n",
        "In this module, we'll take what we learned about indices and generalize!\n",
        "\n",
        "Apache Spark is a big data engine that runs on compute clusters, including on the cloud.  This notebook is set up assuming that (1) Spark is running on an AWS server that is public [this may **not** be true at the time you look at this!].\n",
        "\n",
        "You may need to look at this notebook without directly running it, until we give you specific instructions on launching your own Spark cluster.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "R5uxG6qfFwMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: fill this one in based on the host posted on Ed\n",
        "%set_env EMR_HOST=\n",
        "%set_env HW_ID=cis5450_25f_HW9"
      ],
      "metadata": {
        "id": "Y3x2xSfZdBCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import os\n",
        "\n",
        "# Start Spark Session by Specifying the Spark Cluster Address.\n",
        "spark = SparkSession.builder \\\n",
        "  .appName(\"CIS-5450\") \\\n",
        "  .remote(\"sc://{host}:15002\".format(host=os.getenv('EMR_HOST'))).getOrCreate()"
      ],
      "metadata": {
        "id": "JYz17hrdkoux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "id": "IXntNXYowJ0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stbF4ttIQBUG"
      },
      "source": [
        "The following line connects to Spark running remotely (note you'll need to start an Amazon AWS Elastic MapReduce instance)\n",
        ".  You will likely need to change the URL after the `-u` to connect to an active server."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autograder setup"
      ],
      "metadata": {
        "id": "E7pUp7HAwQp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW WHO\n",
        "#TO ASSIGN POINTS TO YOU IN OUR BACKEND\n",
        "STUDENT_ID = 99999999 # YOUR PENN-ID GOES HERE AS AN INTEGER##PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW WHO"
      ],
      "metadata": {
        "id": "WGM-vv4owbBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile notebook-config.yaml\n",
        "\n",
        "grader_api_url: 'https://23whrwph9h.execute-api.us-east-1.amazonaws.com/default/Grader23'\n",
        "grader_api_key: 'flfkE736fA6Z8GxMDJe2q8Kfk8UDqjsG3GVqOFOa'"
      ],
      "metadata": {
        "id": "OxKJ5J_awSOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install penngrader-client"
      ],
      "metadata": {
        "id": "S3n0LqhuwW2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from penngrader.grader import *\n",
        "\n",
        "grader = PennGrader('notebook-config.yaml', os.environ['HW_ID'], STUDENT_ID, STUDENT_ID)"
      ],
      "metadata": {
        "id": "dEHXUnRqwcP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u6rwG-wQIX2"
      },
      "source": [
        "## Example of Loading Sharded Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8O02TOXqQLgt"
      },
      "source": [
        "First let's do our preliminaries.  **Every** cell in this notebook will need `%%spark` at the start so it runs on the remote machine with Spark on it, instead of on the machine with Jupyter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El-Or-F-Qc5C"
      },
      "source": [
        "## Load into Spark\n",
        "\n",
        "Spark needs to know the structure of the data in its dataframes, i.e., their schemas.  Over the years it has gotten better at inferring schemas, but sometimes you'll want to set the schema yourself.\n",
        "\n",
        "There are some basic types:\n",
        "  * The table is a `StructType` with a list of fields (each row)\n",
        "  * Most fields, in our case, are `StringType`.\n",
        "  * We also have nested dictionary for the name, which is a `MapType` from `StringType` keys to `StringType` values.\n",
        "  * `skills` is an `ArrayType` since it's a list, and it contains `StringType`s.\n",
        "  * `also_view` is an array of structs.\n",
        "\n",
        "See Pyspark documentation on `StructType` and examples such as https://www.programcreek.com/python/example/104715/pyspark.sql.types.StructType.\n",
        "\n",
        "See below for a partial sketch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DN3NJaRuP8Tl"
      },
      "source": [
        "# Spark uses schemas to define the format for DataFrames. By default it will\n",
        "# try to infer, which has varying luck. Here is an example of part of a schema\n",
        "# for LinkedIn.\n",
        "from pyspark.sql.types import StringType, StructField, StructType, ArrayType, MapType\n",
        "schema = StructType([\n",
        "        StructField(\"_id\", StringType(), True),\n",
        "        StructField(\"name\", MapType(StringType(), StringType()), True),\n",
        "        StructField(\"locality\", StringType(), True),\n",
        "        StructField(\"skills\", ArrayType(StringType()), True),\n",
        "        StructField(\"industry\", StringType(), True),\n",
        "        StructField(\"summary\", StringType(), True),\n",
        "        StructField(\"url\", StringType(), True),\n",
        "        StructField(\"also_view\", ArrayType(\\\n",
        "                    StructType([\\\n",
        "                      StructField(\"url\", StringType(), True),\\\n",
        "                      StructField(\"id\", StringType(), True)])\\\n",
        "                    ), True)\\\n",
        "         ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now load a remote file.  To do this, we add the URL to the sparkContext, and then (in the next Cell) we will use `spark.read.json` to open and load the file."
      ],
      "metadata": {
        "id": "6nB1yjKEpvOC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmF8gC6nP3VW"
      },
      "source": [
        "# Read JSON Lines file\n",
        "linked_df = spark.read\\\n",
        "  .json(\"s3://penn-cis545-files/linkedin_anon.jsonl\")\\\n",
        "  .repartition('_id')\n",
        "\n",
        "linked_df.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see the full, inferred schema here:"
      ],
      "metadata": {
        "id": "6v3omsR8p3oQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linked_df.printSchema()"
      ],
      "metadata": {
        "id": "f4VuSIujjUnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try a simple select/project query!"
      ],
      "metadata": {
        "id": "6g74x035p-rz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoUo_4LfOiki"
      },
      "source": [
        "linked_df.filter(linked_df.locality == 'United States')[['_id', 'name', 'locality']].show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also in SQL-like syntax:"
      ],
      "metadata": {
        "id": "Pl3Y2yQjqBCY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oj1E3-AJ1G_y"
      },
      "source": [
        "linked_df.select(\"_id\", 'name', \"locality\").show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And real SQL:"
      ],
      "metadata": {
        "id": "xhpz-wUSqGXO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eD9SE_v-1v0C"
      },
      "source": [
        "linked_df.createOrReplaceTempView('linked_in')\n",
        "spark.sql('select * from linked_in').show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yur38PPmVGt-"
      },
      "source": [
        "spark.sql(\"select _id, name.given_name, name.family_name from linked_in\").show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This currently (Fall 2025) does not work between Colab and Apache Spark on Amazon Elastic Mapreduce, simply because Amazon EMR is 3 versions behind in Python (3.9 vs 3.12). When an updated EMR is available, it will work"
      ],
      "metadata": {
        "id": "Y-Y3apDpePsd"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcvsuNw9Tr6k"
      },
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "@udf(returnType=StringType(), useArrow=True)\n",
        "def acro(x: str):\n",
        "    return ''.join([n[0] for n in x.split()])\n",
        "\n",
        "# linked_df.select(\"_id\", acro(\"locality\").alias(\"acronym\")).show(5)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgMW4qFCQf4W"
      },
      "source": [
        "# Which industries are most popular?\n",
        "spark.sql('select count(_id), industry '+\\\n",
        "               'from linked_in '+\\\n",
        "               'group by industry '+\\\n",
        "               'order by count(_id) desc').\\\n",
        "    show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRrsSWsmI7pG"
      },
      "source": [
        "## Graphs\n",
        "\n",
        "For the next set of examples, we will look at graph-structured data.  It turns out our LinkedIn dataset has a list of nodes (by int ID, but associated with the user ID we used in the linked_in table) and a list of edges."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's consider edges to be bidirectional\n",
        "# from people and the organizations they work for\n",
        "temp_df = spark.sql(\"\"\"\n",
        "  CREATE TEMPORARY VIEW edges_nested AS\n",
        "  SELECT _id AS from, explode(experience) AS to\n",
        "  FROM linked_in\n",
        "\"\"\")\n",
        "\n",
        "# Create graph with edges in each direction\n",
        "edges_df = spark.sql('''\n",
        "  select from, to.org as to from edges_nested\n",
        "  union\n",
        "  select to.org as from, from as to from edges_nested\n",
        "  ''')\n",
        "\n",
        "edges_df.show(5)"
      ],
      "metadata": {
        "id": "cxLTnf2xjzeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edges_df.printSchema()"
      ],
      "metadata": {
        "id": "O4jX8fpUmWtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sV4WTTHohshi"
      },
      "source": [
        "edges_df.createOrReplaceTempView('edges')\n",
        "spark.sql('select from as id, count(to) as degree from edges group by from').show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbXkhbzOk1ZK"
      },
      "source": [
        "## Traversing the Graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAJ_BjL1kiQi"
      },
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Start with a subset of nodes, looking at everything\n",
        "# that could be considered a number under 1000\n",
        "start_nodes_df = edges_df[['from']].filter(edges_df['from'] < 1000).\\\n",
        "  select(col('from').alias('id')).drop_duplicates()\n",
        "\n",
        "print('{} start nodes'.format(start_nodes_df.count()))\n",
        "start_nodes_df.show(9)\n",
        "\n",
        "# The neighbors require us to join\n",
        "# and we'll use Spark DataFrames syntax here\n",
        "neighbor_nodes_df = start_nodes_df.\\\n",
        "  join(edges_df.alias('e'), start_nodes_df.id == col('e.from')).\\\n",
        "  select(col('to').alias('id'))\n",
        "\n",
        "neighbor_nodes_df.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpoPxuPQnmNQ"
      },
      "source": [
        "edges_df[['from']].orderBy('from').drop_duplicates().show()\n",
        "\n",
        "edges_df.filter(edges_df['from'] == '1').show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-pnH4lKqNiV"
      },
      "source": [
        "neighbor_neighbor_nodes_df = neighbor_nodes_df.\\\n",
        "  join(edges_df.alias('e'), neighbor_nodes_df.id == col('e.from')).\\\n",
        "  select(col('to').alias('id'))\n",
        "\n",
        "neighbor_neighbor_nodes_df.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's find a small subset of our graph that actually connects somewhere."
      ],
      "metadata": {
        "id": "-esxdcgYqsHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_nodes_df.createOrReplaceTempView('start_nodes')\n",
        "edges_df.createOrReplaceTempView('edges')\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "  select e1.from as from, e1.to as med, e2.to as to\n",
        "  from start_nodes s join edges e1 on s.id=e1.from join edges e2 on e1.to = e2.from\n",
        "\"\"\").show(5)\n",
        "\n",
        "# This will be the starting\n",
        "start_df = spark.sql(\"\"\"\n",
        "  select e1.from as from, e1.to as to\n",
        "  from start_nodes s join edges e1 on s.id=e1.from\n",
        "\"\"\")\n",
        "\n",
        "start_df.show(5)"
      ],
      "metadata": {
        "id": "rmIX4Pd4o1Gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvsJ2_LNsTbe"
      },
      "source": [
        "def iterate(df, edges, depth):\n",
        "  df.createOrReplaceTempView('base')\n",
        "  edges.createOrReplaceTempView('iter')\n",
        "\n",
        "  # Base case: direct connection\n",
        "  result = spark.sql('select from, to, 1 as depth from base')\n",
        "\n",
        "  for i in range(1, depth):\n",
        "    result.createOrReplaceTempView('result')\n",
        "    result = spark.sql(\"\"\"select r1.from as from, r2.to as to, r1.depth+1 as depth\n",
        "                            from result r1 join iter r2\n",
        "                            on r1.to=r2.from\n",
        "                            where r1.from <> r2.to\n",
        "                            \"\"\")\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjiE4BBMuwqv"
      },
      "source": [
        "iterate(start_df, edges_df, 1).orderBy('from','to').show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63RGiC1euw-y"
      },
      "source": [
        "iterate(start_df, edges_df, 2).orderBy('from','to').show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wG7vMrXuxFr"
      },
      "source": [
        "iterate(start_df, edges_df, 3).orderBy('from','to').show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akCba0mxkikw"
      },
      "source": [
        "## Joins in Spark, Beyond Graph Traversals\n",
        "\n",
        "\n",
        "What if we want to look at relationships between people -- say, co-working?  This involves looking at people and going *every 2 hops* because there are organizations in between."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecVwMCIspVPc"
      },
      "source": [
        "## Finding Coworkers, by ID\n",
        "\n",
        "Let's get our people first!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nodes_df = spark.sql(\"\"\"\n",
        "    select _id as nid, concat(name.given_name, ' ', name.family_name) as user, industry\n",
        "    from linked_in\n",
        "  \"\"\")"
      ],
      "metadata": {
        "id": "kAebehE8r9br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nodes_df.createOrReplaceTempView('nodes')\n",
        "\n",
        "# Let's limit coworkers to edges that start\n",
        "# from existing workers, and are 2 hops away (through a company)\n",
        "coworked_df = spark.sql(\"\"\"\n",
        "  select e1.from, e2.to as to\n",
        "  from edges e1 join edges e2 on e1.to = e2.from\n",
        "  where e1.from in (select nid from nodes)\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "LU-MNBMvsOf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18K1_i1jZ94B"
      },
      "source": [
        "nodes_df.createOrReplaceTempView('nodes')\n",
        "coworked_df.createOrReplaceTempView('edges')\n",
        "\n",
        "coworkers_df = spark.sql(\"\"\"SELECT n1.user, n2.user as coworker\n",
        "               FROM (nodes n1 join edges e on n1.nid = e.from) join nodes n2 on e.to = n2.nid\n",
        "               WHERE n1.user <> n2.user\n",
        "               \"\"\")\n",
        "\n",
        "coworkers_df.show(5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise\n",
        "\n",
        "Can you find the *company* with the most common coworker pairs?\n",
        "\n",
        "As a starting point, let's pull back the original edges table..."
      ],
      "metadata": {
        "id": "sZCvjQKHtT7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "edges_df.createOrReplaceTempView(\"edges\")\n"
      ],
      "metadata": {
        "id": "ga15yqBLtuXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Be sure to find the top-1 organization by count (you should return the `org` and the `count` in the schema).\n",
        "\n",
        "Recall that SQL has `ORDER BY` and `LIMIT` clauses."
      ],
      "metadata": {
        "id": "NRAC14bdSOHj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ISIzrRTcde05"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtAN7dmxgSgH"
      },
      "source": [
        "# TODO: create coworkers_company_sdf (Spark DataFrame).\n",
        "# You don't need to convert to Pandas\n",
        "\n",
        "# Make sure you only get one result from this!\n",
        "coworkers_company_sdf.show(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "coworkers_company_df = pd.DataFrame(coworkers_company_sdf.collect(),columns=coworkers_company_sdf.columns)\n",
        "\n",
        "if not isinstance(coworkers_company_df, pd.DataFrame) or not 'org' in coworkers_company_df.columns:\n",
        "  raise TypeError(\"Data should be in a DataFrame and organization should be a column\")\n",
        "\n",
        "grader.grade('top_coworkers', coworkers_company_df)"
      ],
      "metadata": {
        "id": "pztBTDs3uezL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oED_dhAqxJlV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}