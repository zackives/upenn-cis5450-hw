{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zackives/upenn-cis5450-hw/blob/main/8_Module_2_Part_IV_Parallelism.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49bwxLwZBJw7"
      },
      "source": [
        "# Lecture Module 2 Part 4: Parallelism\n",
        "\n",
        "## LinkedIn Social Analysis\n",
        "\n",
        "We now consider extensions of processing to multi-core and multi-machine models.  This includes:\n",
        "\n",
        "* Polars, a multi-core Pandas-style library that is seeing wide adoption due to its performance (and flexible programming model)\n",
        "* Sharded execution, a technique that is the basis of Spark's execution model.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -nc https://storage.googleapis.com/penn-cis5450/linkedin_anon.jsonl"
      ],
      "metadata": {
        "id": "XWdHUSWxS7un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PennGrader Setup"
      ],
      "metadata": {
        "id": "6NcotPae3MJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile notebook-config.yaml\n",
        "\n",
        "grader_api_url: 'https://23whrwph9h.execute-api.us-east-1.amazonaws.com/default/Grader23'\n",
        "grader_api_key: 'flfkE736fA6Z8GxMDJe2q8Kfk8UDqjsG3GVqOFOa'"
      ],
      "metadata": {
        "id": "lOYSYUus3PNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install penngrader-client"
      ],
      "metadata": {
        "id": "wrfZfYfl3S_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW WHO\n",
        "#TO ASSIGN POINTS TO YOU IN OUR BACKEND\n",
        "STUDENT_ID = 99999999 # YOUR PENN-ID GOES HERE AS AN INTEGER##PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW WHO"
      ],
      "metadata": {
        "id": "23jH64dy4FfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%set_env HW_ID=cis5450_25f_HW9"
      ],
      "metadata": {
        "id": "VN29CmOy3U1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from penngrader.grader import *\n",
        "\n",
        "grader = PennGrader('notebook-config.yaml', os.environ['HW_ID'], STUDENT_ID, STUDENT_ID)"
      ],
      "metadata": {
        "id": "wSTQUM9s3W3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCWrRHRXBJxq"
      },
      "source": [
        "# Loading our Data\n",
        "\n",
        "Now that we've seen how to do fairly complex queries over data in relations, we'll \"pop back\" to our big data example, which is the LinkedIn dataset.  Recall that we had a segment of the LinkedIn input file in our previous examples earlier in this module."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# JSON parsing\n",
        "import json\n",
        "\n",
        "# Time conversions\n",
        "import time"
      ],
      "metadata": {
        "id": "DZ64kVG7TCWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Simple code to pull out data from JSON and load into DuckDB.\n",
        "'''\n",
        "linked_in = open('linkedin_anon.jsonl')\n",
        "people_df = pd.read_json('linkedin_anon.jsonl', lines=True)"
      ],
      "metadata": {
        "id": "4pTCCGh8SluZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "people_df"
      ],
      "metadata": {
        "id": "GGJem6gxTW5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_nested_dict(rel, name):\n",
        "  # This evaluates the string that describes the dictionary, as a dictionary\n",
        "  # definition\n",
        "  ret = rel.copy()\n",
        "  # ret[name] = rel[name].map(lambda x: ast.literal_eval(x) if len(x) else np.NaN)\n",
        "  ret = ret.dropna()\n",
        "  # This joins rows on the index\n",
        "  return ret.drop(columns=name).join(pd.DataFrame(ret[name].tolist()))\n",
        "\n",
        "def get_nested_list(rel, name):\n",
        "  ret = rel.copy()\n",
        "  ret = ret.dropna().explode(name).dropna()\n",
        "  ret = ret.join(pd.DataFrame(ret[name].tolist())).drop(columns=name).drop_duplicates()\n",
        "  return ret.rename(columns={0: name})\n",
        "\n",
        "def get_nested_list_dict(rel, name):\n",
        "  ret = rel.copy()\n",
        "\n",
        "  ret = ret.dropna().explode(name)\n",
        "\n",
        "  exploded_pairs = pd.DataFrame(ret.apply(lambda x: {'_id': x['_id']} | x[name] if isinstance(x[name], dict) else {'_id': x['_id']}, axis=1).tolist())\n",
        "\n",
        "  return ret.merge(exploded_pairs, on='_id').drop(columns=name)\n",
        "\n",
        "# Take the lists, drop any blank strings\n",
        "specialties_df = people_df[['_id','specilities']].explode('specilities')\n",
        "specialties_df.dropna(inplace=True)\n",
        "interests_df = people_df[['_id','interests']].explode('interests')\n",
        "interests_df.dropna(inplace=True)\n",
        "\n",
        "names_df = get_nested_dict(people_df[['_id','name']], 'name')\n",
        "\n",
        "education_df = get_nested_list_dict(people_df[['_id','education']], 'education')\n",
        "experience_df = get_nested_list_dict(people_df[['_id','experience']], 'experience')\n",
        "skills_df = get_nested_list(people_df[['_id','skills']], 'skills')\n",
        "honors_df = get_nested_list(people_df[['_id','honors']], 'honors')\n",
        "events_df = get_nested_list_dict(people_df[['_id','events']], 'events')\n",
        "\n",
        "groups_df = get_nested_dict(people_df[['_id','group']], 'group')\n",
        "\n",
        "people_only_df = people_df.drop(columns=['name','education','group','skills','experience','honors','events','specilities','interests']).merge(names_df,on='_id')"
      ],
      "metadata": {
        "id": "LCJbwD1XSUMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Core Execution of Dataframes"
      ],
      "metadata": {
        "id": "A3VtOmdAsFIW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Polars\n",
        "\n",
        "Polars is a \"drop-in replacement\" for Pandas that supports more advanced processing, including multicore processing.\n",
        "\n",
        "Its syntax is not exactly the same as Pandas', but close.  (In fact, it has some similarities to Apache Spark DataFrames, which we'll be seeing shortly.)"
      ],
      "metadata": {
        "id": "bVzPtN8B76NS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Polars DataFrames\n",
        "\n",
        "To take advantage of Polars you want to convert your dataframes from Pandas to Polars. (If you have base files, you can also use Polars' read_csv etc as you would from Pandas.)"
      ],
      "metadata": {
        "id": "Qc2OMjxg9NVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "\n",
        "# Convert all Pandas dataframes to Polars Dataframes\n",
        "people_only_pdf = pl.from_pandas(people_only_df)\n",
        "education_pdf = pl.from_pandas(education_df)\n",
        "experience_pdf = pl.from_pandas(experience_df)\n",
        "skills_pdf = pl.from_pandas(skills_df)\n",
        "groups_pdf = pl.from_pandas(groups_df)\n",
        "names_pdf = pl.from_pandas(names_df)"
      ],
      "metadata": {
        "id": "2vz6aCzc4nh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PL.Col\n",
        "\n",
        "In Polars, when you are referring to a column in a DataFrame expression, this is represented by a *column object*.  For instance, you can express predicates on the values in a column when filtering, or you can request that certain columns be projected.\n",
        "\n",
        "You can, e.g., talk about `pl.col('_id')`.  SQL-style, you can also refer to `pl.col('*')` if you are asking for all columns."
      ],
      "metadata": {
        "id": "g9zq9eOi-CY_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selection and Projection in Polars\n",
        "\n",
        "Rather than using Pandas' `x[x[condition]]` notation, Polars has a *filter* function.   To project, you use a function called `select` (this is a bit confusing, but think SQL SELECT, not relational algebra select)."
      ],
      "metadata": {
        "id": "JF9zUBkA9Xa4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "people_only_pdf.\\\n",
        "  select(pl.col('_id','family_name','given_name')).\\\n",
        "  filter(pl.col('family_name') <= 'Smith')"
      ],
      "metadata": {
        "id": "XUXW4r8H9shC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Joins\n",
        "\n",
        "In many settings, multi-way joins are the most expensive query operations -- partly because they \"blow up the data\". You saw this with our discussions about join ordering.\n",
        "\n",
        "We should be able to see the benefits of Polars multicore processing with some join expressions. Note that we still need to keep everything in memory for Pandas/Polars, and in fact there are simple join queries (say, all people joined with experiences and education) that will cause us to run out of memory.\n",
        "\n",
        "Let's try a simpler query that joins people with some of their other info."
      ],
      "metadata": {
        "id": "hTVL89UUA2LE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pandas Version"
      ],
      "metadata": {
        "id": "DL6fJdp_3e7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "people_only_df.merge(skills_df, on='_id').merge(education_df, on='_id').merge(groups_df, on='_id')"
      ],
      "metadata": {
        "id": "OcbahVHr3d4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Polars Version"
      ],
      "metadata": {
        "id": "CV1RzgqGBn4w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice this finishes in roughly half the time as Pandas!"
      ],
      "metadata": {
        "id": "WUbsQ5wzAw7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "print(people_only_pdf.join(skills_pdf, on='_id').join(education_pdf, on='_id').join(groups_pdf, on='_id'))"
      ],
      "metadata": {
        "id": "KbEqxeDa8GtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Laziness"
      ],
      "metadata": {
        "id": "WlzPzykuDroC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "result_pdf = people_only_pdf.join(skills_pdf, on='_id').\\\n",
        "  join(education_pdf, on='_id').join(groups_pdf, on='_id')\n",
        "only_sf_pdf = result_pdf.filter(pl.col('locality') == 'San Francisco Bay Area')\n",
        "print (len(only_sf_pdf))"
      ],
      "metadata": {
        "id": "PpFzqm_UEGad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "result_pdf = people_only_pdf.lazy().join(skills_pdf.lazy(), on='_id').join(education_pdf.lazy(), on='_id').join(groups_pdf.lazy(), on='_id').lazy()\n",
        "only_sf_pdf = result_pdf.filter(pl.col('locality') == 'San Francisco Bay Area')\n",
        "print (len(only_sf_pdf.collect()))"
      ],
      "metadata": {
        "id": "_KnACNOL887n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = only_sf_pdf.explain()\n",
        "\n",
        "for line in results.split('\\n'):\n",
        "  print(line)"
      ],
      "metadata": {
        "id": "YHZkGESUDyqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding Sharded Execution\n",
        "\n",
        "Let's now look (in simulation) at how sharding works!  This is the basis of cluster-based processing."
      ],
      "metadata": {
        "id": "zMZowuHtsZYc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Data Graph in Relations\n",
        "\n",
        "To do this we'll adapt our LinkedIn data to a *graph* scenario, with people and companies."
      ],
      "metadata": {
        "id": "isob5iDlsj0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "persons_df = people_only_df[['_id','family_name','given_name']]\n",
        "\n",
        "persons_df"
      ],
      "metadata": {
        "id": "gjdLo72lsgS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experience_df"
      ],
      "metadata": {
        "id": "_fLHdqL8s_1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Companies are also nodes.  We'll auto-assign IDs for these."
      ],
      "metadata": {
        "id": "h3X9f761tM3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "companies_df = experience_df[['org']].drop_duplicates()\n",
        "companies_df['company_id'] = companies_df.index\n",
        "companies_df"
      ],
      "metadata": {
        "id": "fDHAjjmhD8wQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, here are some edges.  We preserve the relationships between people and their experiences with organizations."
      ],
      "metadata": {
        "id": "JnumH0M8sz4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "worked_for = experience_df[['_id', 'org', 'title']].copy().merge(companies_df.set_index('org'), on='org').drop_duplicates()\n",
        "worked_for = worked_for.drop(columns=['org'])\n",
        "worked_for"
      ],
      "metadata": {
        "id": "NmOgxh9gsucp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data"
      ],
      "metadata": {
        "id": "VadNpQrRuPpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('persons_df:')\n",
        "display(persons_df)\n",
        "print('companies_df:')\n",
        "display(companies_df)\n",
        "print('worked_df:')\n",
        "display(worked_for)"
      ],
      "metadata": {
        "id": "hqp9AM5Ru-fp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing Sharded Computations - Our Operators\n",
        "\n",
        "Rather than look at sharding in Spark, we'll instead simulate having 4 machines, each of which has access to a shard of each table.\n",
        "\n",
        "To illustrate the basic concepts we'll create our own basic implementations of Relational Algebra over sharded data -- as well as our own `repartition` etc."
      ],
      "metadata": {
        "id": "k9K2CfW4uWLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nodes = 4\n",
        "\n",
        "def get_hash(x):\n",
        "  '''\n",
        "  Hash code for data, used to compute a shard\n",
        "  '''\n",
        "  if isinstance(x, int):\n",
        "    return x\n",
        "  elif isinstance(x, str):\n",
        "    return x.__hash__()\n",
        "  elif pd.isna(x):\n",
        "    return 'null'.__hash__()\n",
        "  else:\n",
        "    raise RuntimeError('Cannot hash {}'.format(type(x)))\n",
        "\n",
        "def get_shard(sharded_df: list[pd.DataFrame], index: int):\n",
        "  '''\n",
        "  Retrieves the sub-dataframe corresponding to a shard\n",
        "  '''\n",
        "  return sharded_df[index]\n",
        "\n",
        "def repartition(df_or_shards, key: str, shards: int = nodes):\n",
        "  '''\n",
        "  Take a dataframe or a list of sharded sub-dataframes.\n",
        "  Repartition them on the key, based on the number of nodes.\n",
        "  Return the new list of sub-dataframes.\n",
        "  '''\n",
        "  data = df_or_shards\n",
        "  if isinstance(df_or_shards, list):\n",
        "    data = pd.concat(df_or_shards)\n",
        "\n",
        "  resharded_df = []\n",
        "  for shard in range(0, shards):\n",
        "    resharded_df.append(data[data[key].apply(lambda x: get_hash(x) % shards == shard)])\n",
        "\n",
        "  return resharded_df\n",
        "\n",
        "def rename_shards(sharded_df: list[pd.DataFrame], renamer: dict):\n",
        "  '''\n",
        "  Simple relational algebra operator:\n",
        "  Rename the columns of a list of sharded dataframes\n",
        "  '''\n",
        "  renamed_df = []\n",
        "  for shard in range(0, len(sharded_df)):\n",
        "    renamed_df.append(sharded_df[shard].rename(columns=renamer))\n",
        "\n",
        "  return renamed_df\n",
        "\n",
        "def filter_shards(sharded_df: list[pd.DataFrame], filter_fn):\n",
        "  '''\n",
        "  Simple relational algebra operator:\n",
        "  Filter a list of sharded dataframes with a predicate\n",
        "  '''\n",
        "  filtered_df = []\n",
        "  for shard in range(0, len(sharded_df)):\n",
        "    filtered_df.append(\n",
        "        sharded_df[shard][\n",
        "            sharded_df[shard].apply(filter_fn, axis=1)])\n",
        "\n",
        "  return filtered_df\n",
        "\n",
        "\n",
        "def join_shards(sharded_df1: list[pd.DataFrame], sharded_df2: list[pd.DataFrame],\n",
        "                left_key, right_key):\n",
        "  '''\n",
        "  Simple relational algebra operator:\n",
        "  Join two lists of sharded dataframes\n",
        "  '''\n",
        "  ret_result = []\n",
        "\n",
        "  for shard in range(0, len(sharded_df1)):\n",
        "    ret_result.append(sharded_df1[shard].merge(sharded_df2[shard], left_on=left_key, right_on=right_key))\n",
        "\n",
        "  return ret_result\n",
        "\n",
        "def join_shard_with_broadcast(sharded_df1: list[pd.DataFrame], sharded_df2: pd.DataFrame,\n",
        "                left_key, right_key):\n",
        "  '''\n",
        "  Simple relational algebra operator:\n",
        "  Join a list of sharded dataframes with a single dataframe\n",
        "  that conceptually gets \"broadcast\" to all of the shards\n",
        "  '''\n",
        "  ret_result = []\n",
        "\n",
        "  for shard in range(0, len(sharded_df1)):\n",
        "    ret_result.append(sharded_df1[shard].merge(sharded_df2, left_on=left_key, right_on=right_key))\n",
        "\n",
        "  return ret_result"
      ],
      "metadata": {
        "id": "lqNNV9eKti4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## An Example Query\n",
        "\n",
        "Suppose we simply want to ask this query:\n",
        "\n",
        "`(persons_df) -[worked_for]--> (companies_df)`"
      ],
      "metadata": {
        "id": "adnhtTc7vHoV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shard the Relations!\n",
        "\n",
        "We'll create versions of this:\n",
        "* `sharded_persons` (by _id)\n",
        "* `sharded_companies` (by company)\n",
        "* Two shards of `worked_for`:\n",
        "  - `sharded_by_user`\n",
        "  - `sharded_by_company`"
      ],
      "metadata": {
        "id": "MTC9biVTvMmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generally, for a table like persons, we would\n",
        "# shard by default on the key (_id).  We can\n",
        "# see each shard here.\n",
        "\n",
        "sharded_persons = repartition(persons_df, '_id')\n",
        "\n",
        "for p in sharded_persons:\n",
        "  display(p)"
      ],
      "metadata": {
        "id": "yloRnHWGuycR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Similarly with companies... we shard by company_id\n",
        "sharded_companies = repartition(companies_df, 'company_id')\n",
        "\n",
        "for c in sharded_companies:\n",
        "  display(c)"
      ],
      "metadata": {
        "id": "f7B_SDmSvOjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Edges are inherently trickier to shard: we can shard by the *source* or by the *destination* but can't simultaneously do so for a single table, by both.  We can, of course, create two dataframes - one sharded by each."
      ],
      "metadata": {
        "id": "xwRPjH3GyaCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Edge sharded by user ID\n",
        "sharded_by_user = repartition(worked_for, '_id')\n",
        "\n",
        "for edge in sharded_by_user:\n",
        "  display(edge)"
      ],
      "metadata": {
        "id": "kIE10eusvUxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's the other version...  Sharded by the company ID."
      ],
      "metadata": {
        "id": "eQN_mKPeymeZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sharded_by_companies = repartition(worked_for, 'company_id')\n",
        "\n",
        "for edge in sharded_by_companies:\n",
        "  display(edge)"
      ],
      "metadata": {
        "id": "ClM2gIeyvY6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's Do Some Computation!"
      ],
      "metadata": {
        "id": "Wlbf3LoVvx8o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Single Join: People -> works_for\n",
        "\n",
        "To do this, we need to be careful to join only between tables that are sharded by the join key (_id)."
      ],
      "metadata": {
        "id": "B7FPxJgdv0ug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sharded_persons is sharded by _id\n",
        "# sharded_by_user is worked_for sharded by _id\n",
        "for table in join_shards(sharded_persons, sharded_by_user, '_id', '_id'):\n",
        "  display(table)\n",
        "\n",
        "# Show the final results of the join\n",
        "display(pd.concat(join_shards(sharded_persons, sharded_by_user, '_id', '_id')))"
      ],
      "metadata": {
        "id": "SHw67Xw5vs8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, 256823 rows!\n",
        "\n",
        "We can prove the above is correct by looking at the standard, unsharded result."
      ],
      "metadata": {
        "id": "M9kqJWKnwG61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Traditional Pandas query, check the count and results\n",
        "persons_df.merge(worked_for, left_on='_id', right_on='_id')"
      ],
      "metadata": {
        "id": "5Bu7N6d1wFYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What If We Don't Watch Our Sharding?\n",
        "\n",
        "What if we don't align the shard columns and join keys? Let's see..."
      ],
      "metadata": {
        "id": "t7ob7TiXv9E6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sharded_persons is sharded by _id\n",
        "# sharded_by_companies is worked_for sharded by the company\n",
        "\n",
        "# Output the result of the join without proper sharding\n",
        "pd.concat(join_shards(sharded_persons, sharded_by_companies, '_id', '_id'))"
      ],
      "metadata": {
        "id": "F1XIOGC5v4t6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What happens if we *broadcast* one of the relations?\n",
        "\n",
        "We would generally only do this with a small table."
      ],
      "metadata": {
        "id": "m7TEN3VVwpL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.concat(join_shard_with_broadcast(sharded_persons, worked_for, '_id', '_id'))"
      ],
      "metadata": {
        "id": "vJudBVAhv-4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now Let's Do a Full Connection\n",
        "\n",
        "OK, we saw one join.  If we want to traverse our network from people to companies, we need two joins."
      ],
      "metadata": {
        "id": "l8PWbaNbwwpL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's the Pandas query we want to do in a sharded model..."
      ],
      "metadata": {
        "id": "fGIogwA2w2fq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "persons_df.merge(worked_for, left_on='_id', right_on='_id').merge(companies_df, left_on='company_id', right_on='company_id')"
      ],
      "metadata": {
        "id": "MsVKhE6ZwufB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To join with `companies_df` we need to repartition our intermediate result on the company."
      ],
      "metadata": {
        "id": "1Lh-qhLGw56C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the join from above\n",
        "persons_work_for = join_shards(sharded_persons, sharded_by_user, '_id', '_id')\n",
        "\n",
        "# persons_work_for is sharded by _id\n",
        "# sharded_companies is sharded by company\n",
        "# to join them, we need to find a common sharding key -- the company\n",
        "\n",
        "# SO: we repartition persons_work_for by company\n",
        "persons_companies = join_shards(repartition(persons_work_for, 'company_id'), sharded_companies, 'company_id', 'company_id')\n",
        "\n",
        "pd.concat(\n",
        "persons_companies\n",
        ")"
      ],
      "metadata": {
        "id": "xJHL8tFlwzZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, this matches our Pandas query."
      ],
      "metadata": {
        "id": "bLCyvcLVzPdY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Two Hops\n",
        "\n",
        "Can we find employees who have a common employer?\n",
        "\n",
        "Let's divide it into steps.\n",
        "\n",
        "First, let's do a building block, namely the set of people, companies they work for, and the interconnecting `works` relationships:\n",
        "\n",
        "```\n",
        "people --> works_for --> company <-- works_for\n",
        "[---- (persons_companies) -----]\n",
        "```"
      ],
      "metadata": {
        "id": "lJG3ZzolxBBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sharded_by_companies is the works_for edge, sharded by company\n",
        "second_works_for = rename_shards(sharded_by_companies, {'_id': '_id2', 'role': 'role2'})\n",
        "\n",
        "# We are currently sharded on company\n",
        "person_works_company_works = \\\n",
        "    filter_shards(join_shards(persons_companies, second_works_for, 'company_id', 'company_id'),\n",
        "           lambda row: row['_id'] != row['_id2']\n",
        "    )\n",
        "pd.concat(person_works_company_works)"
      ],
      "metadata": {
        "id": "3oGEN1zFw7qG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can join with another company of the"
      ],
      "metadata": {
        "id": "qa8c0z6a0NVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sharded_persons_2 = rename_shards(sharded_persons, {'_id': '_id2', 'given_name': 'given_name2', 'last_name': 'last_name2'})\n",
        "\n",
        "result = join_shards(repartition(person_works_company_works, '_id2'), sharded_persons_2, '_id2', '_id2')\n",
        "# Take the above and shard + join with sharded_persons\n",
        "for table in result:\n",
        "  display(table)\n",
        "\n",
        "display(pd.concat(result))"
      ],
      "metadata": {
        "id": "Vyt3rnX8xCuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise"
      ],
      "metadata": {
        "id": "xVC7i-rk00jw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's do this via a broadcast join."
      ],
      "metadata": {
        "id": "oZlhubllzpAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "result = join_shard_with_broadcast(# TODO\n",
        ")\n",
        "\n",
        "display(pd.concat(result))"
      ],
      "metadata": {
        "id": "UIualNpKxYxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grader.grade('sharded_net_result', len(pd.concat(result)))"
      ],
      "metadata": {
        "id": "SOAIf5iM095j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L0BcUmiB1KjE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}