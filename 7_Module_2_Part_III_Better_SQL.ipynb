{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPCMTDYWOyiICxt0/59Cg/R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zackives/upenn-cis5450-hw/blob/main/7_Module_2_Part_III_Better_SQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Better SQL - How to Understand, Write, and Debug\n",
        "\n",
        "It's easy to write SQL queries that become very challenging to debug.\n",
        "\n",
        "In this Notebook, we'll try to summarize some of the subtleties of different SQL constructs, how they relate, and how we might debug."
      ],
      "metadata": {
        "id": "YVoLIJgZ1042"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFkIJccr1Q7i",
        "outputId": "8722d908-30cc-4d71-debf-4199b7766529",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ‘linkedin_anon.jsonl’ already there; not retrieving.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -nc https://storage.googleapis.com/penn-cis5450/linkedin_anon.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install lxml\n",
        "!pip3 install duckdb"
      ],
      "metadata": {
        "id": "8HvsjaH61gud",
        "outputId": "4a39dd1e-733c-4de8-d165-3736c75938ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (5.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up a sample database\n",
        "\n",
        "All of this is to load up our LinkedIn data..."
      ],
      "metadata": {
        "id": "m4t6KF0y8EPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# JSON parsing\n",
        "import json\n",
        "\n",
        "# HTML parsing\n",
        "from lxml import etree\n",
        "import urllib\n",
        "\n",
        "# DuckDB RDBMS\n",
        "import duckdb\n",
        "\n",
        "# Polars big data package\n",
        "import polars\n",
        "\n",
        "# Time conversions\n",
        "import time"
      ],
      "metadata": {
        "id": "bcU0ug0x1jAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Simple code to pull out data from JSON and load into DuckDB.\n",
        "'''\n",
        "import ast\n",
        "\n",
        "linked_in = open('linkedin_anon.jsonl')\n",
        "people_df = pd.read_json('linkedin_anon.jsonl', lines=True)"
      ],
      "metadata": {
        "id": "N9YqVKck1kU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_nested_dict(rel, name):\n",
        "  # This evaluates the string that describes the dictionary, as a dictionary\n",
        "  # definition\n",
        "  ret = rel.copy()\n",
        "  # ret[name] = rel[name].map(lambda x: ast.literal_eval(x) if len(x) else np.NaN)\n",
        "  ret = ret.dropna()\n",
        "  # This joins rows on the index\n",
        "  return ret.drop(columns=name).join(pd.DataFrame(ret[name].tolist()))\n",
        "\n",
        "def get_nested_list(rel, name):\n",
        "  ret = rel.copy()\n",
        "  ret = ret.dropna().explode(name).dropna()\n",
        "  ret = ret.join(pd.DataFrame(ret[name].tolist())).drop(columns=name).drop_duplicates()\n",
        "  return ret.rename(columns={0: name})\n",
        "\n",
        "def get_nested_list_dict(rel, name):\n",
        "  ret = rel.copy()\n",
        "\n",
        "  ret = ret.dropna().explode(name)\n",
        "\n",
        "  exploded_pairs = pd.DataFrame(ret.apply(lambda x: {'_id': x['_id']} | x[name] if isinstance(x[name], dict) else {'_id': x['_id']}, axis=1).tolist())\n",
        "\n",
        "  return ret.merge(exploded_pairs, on='_id').drop(columns=name)\n",
        "  #pd.DataFrame(ret[name].tolist())).drop(columns=name).drop_duplicates()\n",
        "\n",
        "# Take the lists, drop any blank strings\n",
        "specialties_df = people_df[['_id','specilities']].explode('specilities').rename(columns={'_id': 'person'})\n",
        "specialties_df.dropna(inplace=True)\n",
        "interests_df = people_df[['_id','interests']].explode('interests').rename(columns={'_id': 'person'})\n",
        "interests_df.dropna(inplace=True)\n",
        "\n",
        "names_df = get_nested_dict(people_df[['_id','name']], 'name')\n",
        "\n",
        "education_df = get_nested_list_dict(people_df[['_id','education']], 'education')\n",
        "experience_df = get_nested_list_dict(people_df[['_id','experience']], 'experience')\n",
        "skills_df = get_nested_list(people_df[['_id','skills']], 'skills')\n",
        "honors_df = get_nested_list(people_df[['_id','honors']], 'honors')\n",
        "events_df = get_nested_list_dict(people_df[['_id','events']], 'events')\n",
        "\n",
        "groups_df = get_nested_dict(people_df[['_id','group']], 'group')\n",
        "\n",
        "people_only_df = people_df.drop(columns=['name','education','group','skills','experience','honors','events','specilities','interests']).\\\n",
        "  merge(names_df, on='_id')"
      ],
      "metadata": {
        "id": "WG9JU7sG1p5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## This is just to reset things so we don't have an index\n",
        "conn = duckdb.connect('linkedin.db')\n",
        "conn.execute('BEGIN TRANSACTION')\n",
        "conn.execute('DROP TABLE IF EXISTS people')\n",
        "conn.execute('DROP TABLE IF EXISTS education')\n",
        "conn.execute('DROP TABLE IF EXISTS experience')\n",
        "conn.execute('DROP TABLE IF EXISTS skills')\n",
        "conn.execute('DROP TABLE IF EXISTS honors')\n",
        "conn.execute('DROP TABLE IF EXISTS events')\n",
        "conn.execute('DROP TABLE IF EXISTS groups')\n",
        "conn.execute('DROP TABLE IF EXISTS specialties')\n",
        "conn.execute('DROP TABLE IF EXISTS interests')\n",
        "conn.execute('DROP INDEX IF EXISTS people_industry')\n",
        "conn.execute('CREATE TABLE people AS SELECT * FROM people_only_df')\n",
        "conn.execute('CREATE TABLE education AS SELECT * FROM education_df')\n",
        "conn.execute('CREATE TABLE experience AS SELECT * FROM experience_df')\n",
        "conn.execute('CREATE TABLE skills AS SELECT * FROM skills_df')\n",
        "conn.execute('CREATE TABLE honors AS SELECT * FROM honors_df')\n",
        "conn.execute('CREATE TABLE events AS SELECT * FROM events_df')\n",
        "conn.execute('CREATE TABLE groups AS SELECT * FROM groups_df')\n",
        "conn.execute('CREATE TABLE specialties AS SELECT * FROM specialties_df')\n",
        "conn.execute('CREATE TABLE interests AS SELECT * FROM interests_df')\n",
        "conn.execute('COMMIT')"
      ],
      "metadata": {
        "id": "02f78sKZ1wMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Penngrader setup"
      ],
      "metadata": {
        "id": "fwdLRGbDtkgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile notebook-config.yaml\n",
        "\n",
        "grader_api_url: 'https://23whrwph9h.execute-api.us-east-1.amazonaws.com/default/Grader23'\n",
        "grader_api_key: 'flfkE736fA6Z8GxMDJe2q8Kfk8UDqjsG3GVqOFOa'"
      ],
      "metadata": {
        "id": "WgFeS_i6tM-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install penngrader-client"
      ],
      "metadata": {
        "id": "57C9RJWptl9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW WHO\n",
        "#TO ASSIGN POINTS TO YOU IN OUR BACKEND\n",
        "STUDENT_ID = 99999999 # YOUR PENN-ID GOES HERE AS AN INTEGER##PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW WHO"
      ],
      "metadata": {
        "id": "epc7UPRJtoFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%set_env HW_ID=cis5450_25f_HW9"
      ],
      "metadata": {
        "id": "QaV98JxjtpZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from penngrader.grader import *\n",
        "\n",
        "grader = PennGrader('notebook-config.yaml', os.environ['HW_ID'], STUDENT_ID, STUDENT_ID)"
      ],
      "metadata": {
        "id": "O_FuROD3trts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validating the database setup"
      ],
      "metadata": {
        "id": "2b4wQx6WtLpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conn.sql(\"\"\"\n",
        "SELECT DISTINCT industry\n",
        "FROM people\n",
        "WHERE industry IS NOT NULL\n",
        "\"\"\").df()"
      ],
      "metadata": {
        "id": "IazWAst445_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conn.sql(\"\"\"\n",
        "SELECT DISTINCT skills\n",
        "FROM skills\n",
        "\"\"\").df()"
      ],
      "metadata": {
        "id": "UPm_PqCl5St4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Queries with Joins\n",
        "\n",
        "How many People have skills related to Biology and work in Tech?"
      ],
      "metadata": {
        "id": "iiao_dY_2Q2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conn.sql('select * from people').df()"
      ],
      "metadata": {
        "id": "40_Af85u3QRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding UNION, INTERSECTION, Cartesian Product, and JOIN\n",
        "\n",
        "Recall that relations are *sets of tuples* and that the Relational Algebra is a set of operations over sets of tuples.  SELECT filters tuples, PROJECT changes (projects out of) their schema, etc.\n",
        "\n",
        "Recall that sets have three common operators:\n",
        "\n",
        "1. UNION\n",
        "2. INTERSECTION\n",
        "3. CARTESIAN PRODUCT"
      ],
      "metadata": {
        "id": "WPDqTtu_6CUD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Recall that by default database tables allow duplicates.  We can always use `SELECT DISTINCT` to get true sets.)"
      ],
      "metadata": {
        "id": "HPi6mNUq7Lcs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UNION and WHERE c1() OR c2()\n",
        "\n",
        "In a way, UNION is the simplest operation, conceptually. We take two sets of tuples (with the same schema!) and put them together.\n",
        "\n",
        "We can do this, for instance, to collect sets of items that satisfy either of two different conditions."
      ],
      "metadata": {
        "id": "7VegW13j642C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(conn.sql(\"\"\"\n",
        "  SELECT DISTINCT _id, given_name, family_name\n",
        "  FROM people\n",
        "  WHERE lower(industry) LIKE '%bio%'\n",
        "  UNION\n",
        "  SELECT DISTINCT _id, given_name, family_name\n",
        "  FROM people\n",
        "  WHERE lower(industry) LIKE '%tech%'\n",
        "\"\"\").df())"
      ],
      "metadata": {
        "id": "UHjTbJQm64Zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because UNION combines sets, we can interchangeably go between different union \"branches\" and *disjunction* (`OR`) in our queries:\n",
        "\n",
        "```\n",
        " SELECT *\n",
        " FROM S\n",
        " WHERE c1(S)\n",
        "UNION\n",
        " SELECT *\n",
        " FROM S\n",
        " WHERE c2(S)\n",
        " ```\n",
        "\n",
        " vs\n",
        "\n",
        " ```\n",
        " SELECT *\n",
        " FROM S\n",
        " WHERE c1(S) OR c2(S)\n",
        "```"
      ],
      "metadata": {
        "id": "gOUFIRi47UHw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise\n",
        "\n",
        "Given the above: try writing the above query about bio and tech people as a single SELECT with an OR:"
      ],
      "metadata": {
        "id": "BiiMklb37vZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "result_df =  # TODO\n",
        "\n",
        "display(result_df)"
      ],
      "metadata": {
        "id": "0FquCORZ71bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not isinstance(result_df, pd.DataFrame):\n",
        "  raise TypeError(\"Value in results_df must be a pandas DataFrame\")\n",
        "grader.grade('sql_or', result_df)"
      ],
      "metadata": {
        "id": "2yLX_PcKvmfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multisets and UNION\n",
        "\n",
        "What if I want to *count* how many rows there are with people who have these skills? If so, I may want *multisets* or *bags*.  Here I don't use `DISTINCT`."
      ],
      "metadata": {
        "id": "4mrh_8ma8ye6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(conn.sql(\"\"\"\n",
        "  SELECT count(_id)\n",
        "  FROM people\n",
        "  WHERE lower(industry) LIKE '%bio%' OR lower(industry) LIKE '%tech%'\n",
        "\"\"\").df())"
      ],
      "metadata": {
        "id": "Ry-4NoT284BP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I can also assemble sub-results via UNION, but if I want \"bag union\" I need to say UNION ALL."
      ],
      "metadata": {
        "id": "egrgIhBx9I2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(conn.sql(\"\"\"\n",
        "  SELECT COUNT(*)\n",
        "  FROM (\n",
        "    SELECT _id, given_name, family_name\n",
        "    FROM people\n",
        "    WHERE lower(industry) LIKE '%bio%'\n",
        "    UNION ALL\n",
        "    SELECT _id, given_name, family_name\n",
        "    FROM people\n",
        "    WHERE lower(industry) LIKE '%tech%'\n",
        "  )\n",
        "\"\"\").df())"
      ],
      "metadata": {
        "id": "_tPnsLir9Paw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question for discussion\n",
        "\n",
        "*Why is this not the same?  What could we do to fix it?  You are welcome to discuss with your peers or on Ed Discussion.*"
      ],
      "metadata": {
        "id": "hwM7Oe329ZXH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## JOIN is a Cartesian Product"
      ],
      "metadata": {
        "id": "hcnNpQuS9exE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again connecting to set operations: JOIN is a special case of CARTESIAN PRODUCT, namely a CARTESIAN PRODUCT followed by SELECT (which is the join condition)."
      ],
      "metadata": {
        "id": "p0z0gMFz6Zy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(conn.sql(\"\"\"\n",
        "  SELECT DISTINCT _id, given_name, family_name\n",
        "  FROM people NATURAL JOIN skills\n",
        "\"\"\").df())\n",
        "\n",
        "display(conn.sql(\"\"\"\n",
        "  SELECT DISTINCT people._id, given_name, family_name\n",
        "  FROM people CROSS JOIN skills\n",
        "  WHERE people._id = skills._id\n",
        "\"\"\").df())"
      ],
      "metadata": {
        "id": "_tgHWFKm6hNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Join and Intersection\n",
        "\n",
        "Sometimes you are asked for sets of items that jointly satisfy conditions.  If you just want to return the basic items, this can be accomplished with an INTERSECTion:"
      ],
      "metadata": {
        "id": "C31MmRja-n-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conn.sql(\"\"\"\n",
        "  SELECT DISTINCT _id, given_name, family_name\n",
        "  FROM people NATURAL JOIN skills\n",
        "  WHERE lower(skills.skills) LIKE '%bio%'\n",
        "  INTERSECT\n",
        "  SELECT DISTINCT _id, given_name, family_name\n",
        "  FROM people NATURAL JOIN experience\n",
        "  WHERE lower(industry) LIKE '%technology%'\n",
        "\"\"\").df()"
      ],
      "metadata": {
        "id": "P9ur1s6A2nqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "But also with a join:"
      ],
      "metadata": {
        "id": "rHCX7JAA-xqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conn.sql(\"\"\"\n",
        "  SELECT DISTINCT _id, given_name, family_name\n",
        "  FROM people NATURAL JOIN skills NATURAL JOIN experience\n",
        "  WHERE lower(skills.skills) LIKE '%bio%' AND\n",
        "   lower(industry) LIKE '%technology%'\n",
        "\"\"\").df()"
      ],
      "metadata": {
        "id": "BluAlTW622Q2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conn.sql(\"\"\"\n",
        "  SELECT DISTINCT _id, given_name, family_name\n",
        "  FROM people NATURAL JOIN skills NATURAL JOIN experience\n",
        "  WHERE lower(skills.skills) LIKE '%bio%' AND\n",
        "   lower(industry) LIKE '%technology%'\n",
        "\"\"\").df()"
      ],
      "metadata": {
        "id": "TEPOtkWrDKXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So why would I use one vs the other?\n",
        "\n",
        "1. Remember that INTERSECTION only returns items from the set that satisfy the condition. You can't, for instance, include combinations of fields that match.\n",
        "2. If your checks-for-relationships span multiple tables, then that is *inherently* a join.\n",
        "3. Remember that JOIN will be default produce a multiset (you can SELECT DISTINCT to remove)."
      ],
      "metadata": {
        "id": "3w7pKkMt-2SJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conn.sql(\"\"\"\n",
        "  SELECT DISTINCT _id, given_name, family_name, skills, industry\n",
        "  FROM people NATURAL JOIN skills NATURAL JOIN experience\n",
        "  WHERE lower(skills.skills) LIKE '%bio%' AND\n",
        "   lower(industry) LIKE '%technology%'\n",
        "\"\"\").df()"
      ],
      "metadata": {
        "id": "8F6AAY9l5y3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7FoCfgLQ-4sp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Python Comprehensions vs SQL ... and Conditionals\n",
        "\n",
        "SQL is a bit like Python [list comprehensions](https://www.programiz.com/python-programming/list-comprehension).  In Python, we can create a new list from the members of another collection, using list-builder (square-bracket) notation, and the *for* keyword.\n",
        "\n",
        "Intuitively, the list comprehension is heavily inspired by set-builder notation in discrete mathematics. Perhaps you've seen mathematical expressions like this:\n",
        "\n",
        "$$\\{x | x \\in S \\wedge x < 5\\}$$\n",
        "\n",
        "Suppose $S$ were a list and not a set.  You could imagine extending to a list-builder notation like this:\n",
        "\n",
        "$$[x | x \\in S \\wedge x < 5]$$\n",
        "\n",
        "Indeed, that's roughly what Python does as a list comprehension:\n",
        "\n",
        "```\n",
        "[x for x in S]\n",
        "```\n",
        "\n",
        "Now let's connect this to DataFrames, which are really lists of tuples. We can, if we want to, iterate over the set of rows in a dataframe, and pull out the name from the content in a list:"
      ],
      "metadata": {
        "id": "8wVi_PZtItju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[x[1]['name'] for x in people_df.iterrows()]"
      ],
      "metadata": {
        "id": "EqHeo0UFJfBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is basically the same as, in DuckDB:"
      ],
      "metadata": {
        "id": "FrEL1ScFJ_b3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "duckdb.sql('select name from people_df')"
      ],
      "metadata": {
        "id": "SmfFVlyTKBAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also add conditionals on this.  Python has a bit of a weird syntax: each value of `x` in the collection needs something to be output, and we can output a different value depending on whether a condition is satisfied."
      ],
      "metadata": {
        "id": "HhNZsLqCJ92z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[x if not pd.isna(x) and x.find('-') < 6 else '(special)' for x in list(people_df['_id'])]"
      ],
      "metadata": {
        "id": "SQTPEyR6JEd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SQL also allows for conditions using a `CASE WHEN` syntax.  Note the `position` function in DuckDB SQL returns 1-based, as opposed to 0-based, positions."
      ],
      "metadata": {
        "id": "s3N080K0JRBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "duckdb.sql(\"\"\"\n",
        "  SELECT CASE WHEN _id IS NOT NULL AND position('-' IN _id) < 7 then _id else '(special)' end\n",
        "  FROM people_df\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "LNKIiCySKyCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise\n",
        "\n",
        "Write a SQL query that replaces all industries without \"tech\" as a substring with NULL.  Be sure you are case-agnostic in your query, but don't change the case in the result.  Make sure the column is called \"industry.\" Return the results as a dataframe."
      ],
      "metadata": {
        "id": "bSwQ6Dj0N-ol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = # TODO\n"
      ],
      "metadata": {
        "id": "yIYYqy9bON0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df.dropna()"
      ],
      "metadata": {
        "id": "47LbCruyO_7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not isinstance(results_df, pd.DataFrame):\n",
        "  raise TypeError(\"Value in results_df must be a pandas DataFrame\")\n",
        "elif len(results_df.dropna()) == len(results_df):\n",
        "  raise RuntimeError('We would expect some nulls!')\n",
        "grader.grade('sql_case', results_df)"
      ],
      "metadata": {
        "id": "S23D2aDrOX6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Uncorrelated* Subqueries / Table Expressions\n",
        "\n",
        "Generally, in SQL we can write a *table expression* anywhere we could use a table.  Maybe the simplest way is to consider an expression within the FROM clause."
      ],
      "metadata": {
        "id": "cHyhsWTaDVXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conn.sql(\n",
        "    \"\"\"\n",
        "  SELECT *\n",
        "  FROM (\n",
        "    SELECT _id, given_name, family_name\n",
        "    FROM people JOIN (SELECT _id FROM skills WHERE lower(skills) LIKE '%bio%') USING (_id)\n",
        "  )\n",
        "\"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "ZZgiDY71DXw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And SQL also allows for IN {table expression} or EXISTS({table expression}) within the WHERE clause."
      ],
      "metadata": {
        "id": "xwZR0hhqQpKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conn.sql(\n",
        "    \"\"\"\n",
        "  SELECT *\n",
        "  FROM (\n",
        "    SELECT _id, given_name, family_name\n",
        "    FROM people\n",
        "    WHERE _id IN (SELECT _id FROM skills WHERE lower(skills) LIKE '%bio%')\n",
        "  )\n",
        "\"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "L83NqhB5Q6Pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that both of the above queries compute the same thing!  In fact, generally we can accomplish the same thing as *either* of the above two uncorrelated query forms, as a single query with a JOIN.\n",
        "\n",
        "*Lesson here: think about whether you can simplify your nested queries into a single query that is easier to write / reason about!*"
      ],
      "metadata": {
        "id": "mYEn2YiiRBre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conn.sql(\n",
        "    \"\"\"\n",
        "  SELECT _id, given_name, family_name\n",
        "    FROM people JOIN skills USING (_id)\n",
        "    WHERE lower(skills) LIKE '%bio%'\n",
        "\"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "46NTMHKMRPZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Correlated Subqueries"
      ],
      "metadata": {
        "id": "1CdvvVPyDdRf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that one way of thinking about SQL queries is that the iterate over all of the tuples in each of the tables in the FROM clause. Each table iterator is given a variable name (if you don't specify one it will be the table's name):\n",
        "\n",
        "```\n",
        "SELECT *\n",
        "FROM people A, skills B\n",
        "```\n",
        "\n",
        "would iterate over all A and B tuples and consider their combinations. In fact you would get a Cartesian product as a result of this.\n",
        "\n",
        "We may want to write *subqueries* that test against the values in the iterators.\n"
      ],
      "metadata": {
        "id": "kO8KKbN5DZGd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EXISTS\n",
        "\n",
        "Perhaps the simplest subquery uses the `EXISTS` predicate in the `WHERE` clause. Within the predicate, we can compute any set-style expression, including a SQL query that returns results.  Naturally, `EXISTS` tests whether we have an empty set or not.\n",
        "\n",
        "Example: For each person in an industry, we can see if there exists at least one other person with the same first name, in the same industry."
      ],
      "metadata": {
        "id": "eyTZu9F6DhLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "conn.sql(\"\"\"\n",
        "  SELECT DISTINCT A.family_name, A.given_name, A.industry\n",
        "  FROM people A\n",
        "  WHERE EXISTS (\n",
        "    SELECT *\n",
        "    FROM people B\n",
        "    WHERE A._id != B._id AND\n",
        "      A.given_name = B.given_name AND\n",
        "      A.industry = B.industry\n",
        "  )\n",
        "\"\"\").df()"
      ],
      "metadata": {
        "id": "IkAsxzOASe6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IN\n",
        "\n",
        "One can also test whether a result exists within a (correlated) subquery.  This should mirror your intuitions for the logical $x \\in S$ test one would apply in logic.\n",
        "\n",
        "Here's a version of the previous query, looking for *all people who aren't the current person in the parent query but have a matching first name*.  Then we test if the industry matches.\n",
        "Observe that the results are the same but the query looks very different. What can you say about the *execution time*?"
      ],
      "metadata": {
        "id": "l40v1xskDi63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "conn.sql(\"\"\"\n",
        "  SELECT DISTINCT A.family_name, A.given_name, A.industry\n",
        "  FROM people A\n",
        "  WHERE A.industry IN (\n",
        "    SELECT industry\n",
        "    FROM people B\n",
        "    WHERE A._id != B._id\n",
        "    AND A.given_name = B.given_name\n",
        "  )\n",
        "\"\"\").df()"
      ],
      "metadata": {
        "id": "EduEYVV3TlJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question for discussion\n",
        "\n",
        "Can you think of a way to use a *join* to capture the same result above? Hint: you may need to also use `DISTINCT`.  Among the three options which is fastest?"
      ],
      "metadata": {
        "id": "EfOp-Q0YSpg4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test against ALL\n",
        "\n",
        "The `>= ALL()` predicate in the `WHERE` clause allows us to compare against the results of a set -- computed in a subquery.  (In most cases that subquery returns a collection of unary tuples, since we will be comparing a single scalar such as a string or int.)\n",
        "\n",
        "Example: Let's find, for each industry, the **person/people with the lexicographically greatest last name**. We can do this by seeing if the iterator's last name matches or exceeds *all* last names of people in the same industry."
      ],
      "metadata": {
        "id": "393npTQjRd7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conn.sql(\"\"\"\n",
        "   SELECT industry, _id, given_name, family_name\n",
        "   FROM people A\n",
        "   WHERE family_name >= ALL(\n",
        "    SELECT family_name\n",
        "    FROM people B\n",
        "    WHERE A.industry = B.industry)\n",
        "  ORDER BY A.industry\n",
        "\"\"\").df()"
      ],
      "metadata": {
        "id": "rD2CHRjJDfdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of course, if there is `>=ALL()`, you can imagine that there are *other* conditionals that can be tested against `ALL`."
      ],
      "metadata": {
        "id": "2ixsS89rS5Oh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grouping, HAVING vs WHERE\n",
        "\n",
        "Sometimes we need to do SQL *grouping* as well as *filtering*.  What's important is to understand whether we are filtering *before* the aggregation (e.g., we eliminate folks we don't want to count) or *after* the aggregation (e.g., we eliminate aggregate groups).\n",
        "\n",
        "For the former we use `WHERE` as per the usual.  But if we want to filter the *results* of a `GROUP BY` we need to either (1) feed the results into another query as a source in the `FROM` clause and filter, which is often painful; or (2) use the optional SQL HAVING clause.\n",
        "\n",
        "So, if we want to show in descending order all industries by popularity, as long as the industry is *not* Biotechnology and there are at least 10 people in the industry, we can do the following."
      ],
      "metadata": {
        "id": "MQ_oibW5Dwg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conn.sql(\"\"\"\n",
        "   SELECT industry, COUNT(_id) AS popularity\n",
        "   FROM people\n",
        "   WHERE industry <> 'Biotechnology'\n",
        "   GROUP BY industry\n",
        "   HAVING COUNT(_id) > 10\n",
        "   ORDER BY COUNT(_id) DESC\n",
        "\"\"\").df()"
      ],
      "metadata": {
        "id": "XJvVU1PkVYwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SQL Debugging"
      ],
      "metadata": {
        "id": "YhV51xMPE5dJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Debugging by Refactoring"
      ],
      "metadata": {
        "id": "zCvHYpiTDmAs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a complex query about transitive relationships -- say, people who have experience (3+ \"experience\" rows) and have a common company, but one is in tech and one is in marketing.  This is essentially a \"two-hop neighbor\" query on a kind of graph (person p1 --> company <-- p2 where both p1 and p2 have certain constraints)."
      ],
      "metadata": {
        "id": "sAenXXpMSG8J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we tried to write all of this out, maybe like this.  It has a lot of the right form, e.g., we know we are looking for people `p1` and `p2`, it looks for people with at least 3 experiences, etc.  But it has no results!"
      ],
      "metadata": {
        "id": "KY5Nu6hMYDvb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conn.sql(\"\"\"\n",
        "  SELECT DISTINCT p1._id, p1.family_name, p1.given_name, p2._id, p2.family_name, p2.given_name\n",
        "  FROM people p1 JOIN skills s ON p1._id = s._id JOIN experience e ON p1._id = e._id\n",
        "     JOIN people p2 ON e._id = p2._id JOIN skills s2 ON p2._id = s2._id\n",
        "  WHERE lower(s.skills) LIKE '%tech%' AND EXISTS (\n",
        "    SELECT _id FROM experience e\n",
        "    WHERE p1._id = e._id\n",
        "    GROUP BY e._id\n",
        "    HAVING COUNT(*) >= 3)\n",
        "  AND lower(s2.skills) = 'Marketing' AND EXISTS (\n",
        "    SELECT _id FROM experience e\n",
        "    WHERE p2._id = e._id\n",
        "    GROUP BY e._id\n",
        "    HAVING COUNT(*) >= 3\n",
        "  )\n",
        "\"\"\").df()"
      ],
      "metadata": {
        "id": "rMKoL3YwYG2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if we break into the two pieces first?"
      ],
      "metadata": {
        "id": "lYf1vCQrZOF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "part1_df = conn.sql(\"\"\"\n",
        "  SELECT DISTINCT _id, family_name, given_name, locality, skills, org\n",
        "  FROM people p JOIN skills s USING (_id) JOIN experience USING (_id)\n",
        "  WHERE lower(skills) LIKE '%tech%' AND EXISTS (\n",
        "    SELECT _id FROM experience e\n",
        "    WHERE p._id = e._id\n",
        "    GROUP BY _id\n",
        "    HAVING COUNT(*) >= 3)\n",
        "\"\"\").df()\n",
        "\n",
        "part1_df"
      ],
      "metadata": {
        "id": "yIkW1tcPXeLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "part2_df = conn.sql(\"\"\"\n",
        "  SELECT DISTINCT _id, family_name, given_name, skills, org\n",
        "  FROM people p JOIN skills s USING (_id) JOIN experience USING (_id)\n",
        "  WHERE skills = 'Marketing' AND EXISTS (\n",
        "    SELECT _id FROM experience e\n",
        "    WHERE p._id = e._id\n",
        "    GROUP BY _id\n",
        "    HAVING COUNT(*) >= 3)\n",
        "\"\"\").df()\n",
        "\n",
        "part2_df"
      ],
      "metadata": {
        "id": "0Fu8PjMVDn__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise\n",
        "\n",
        "Those look good.  How would I join them together to actually solve the problem?"
      ],
      "metadata": {
        "id": "vLjBbB6VZR-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = # TODO\n",
        "\n",
        "results_df"
      ],
      "metadata": {
        "id": "Nk0LZwpvZau1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not isinstance(results_df, pd.DataFrame):\n",
        "  raise TypeError(\"Value in results_df must be a pandas DataFrame\")\n",
        "elif len(results_df.dropna()) != len(results_df):\n",
        "  raise RuntimeError('We don\\'t expect nulls!')\n",
        "grader.grade('sql_int', results_df)"
      ],
      "metadata": {
        "id": "Z6G_tjUcpCjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Debugging over Samples"
      ],
      "metadata": {
        "id": "GAtuC4c2D483"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For really big datasets, you may find that it takes forever to run the query.  One approach is to *sample* results.\n",
        "\n",
        "*Caveat*: sampling independently from different tables that join is very risky -- each time we do this, the proportion of tuples in your query that join (\"selectivity\") goes down exponentially, because the real values are *correlated* but you are instead sampling *independently*.\n",
        "\n",
        "DuckDB addresses this by allowing you to *sample over a query result* instead of over the input tables."
      ],
      "metadata": {
        "id": "58pJAhEDZgFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conn.sql(\"\"\"\n",
        "  SELECT DISTINCT _id, family_name, given_name, skills, org\n",
        "  FROM people p\n",
        "  JOIN skills s USING (_id) JOIN experience USING (_id)\n",
        "  WHERE skills = 'Marketing' AND EXISTS (\n",
        "    SELECT _id FROM experience e\n",
        "    WHERE p._id = e._id\n",
        "    GROUP BY _id\n",
        "    HAVING COUNT(*) >= 3)\n",
        "   USING SAMPLE 10 PERCENT\n",
        "\"\"\").df()"
      ],
      "metadata": {
        "id": "O89PW5hwZ80F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gEolWXozaFCo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}