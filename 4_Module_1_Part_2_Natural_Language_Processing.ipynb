{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zackives/upenn-cis5450-hw/blob/main/4_Module_1_Part_2_Natural_Language_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing in the Transformers and LLMs Era\n",
        "\n",
        "As recently as 5 years ago, machine learning techniques for natural language and the Web were extremely brittle.  They still are not perfect, but they are often \"good enough\" to do real work -- thanks to large language models (LLMs) and transformers.  In this notebook we'll try some tools."
      ],
      "metadata": {
        "id": "xlrewAq2CKTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: use the Azure OpenAI key from Ed Discussion (not the OpenAI one!)\n",
        "%set_env AZURE_OPENAI_API_KEY=%TODO"
      ],
      "metadata": {
        "id": "qbXJcyJzBywC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "1zF2-lJyCPsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TF4RsJlXBmj_"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-core langchain-community langchain-openai\n",
        "!pip install chromadb transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Documents as Vectors\n",
        "\n",
        "Let's parse a paragraph and create a very simple document vector.  We'll use a parser from a package called `nltk`."
      ],
      "metadata": {
        "id": "ZR6mqxfK7hV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "ay1C7Ur95sbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "paragraph = '''A large language model (LLM) is a language model characterized by\n",
        "               its large size. Its size is enabled by AI accelerators, which are\n",
        "               able to process vast amounts of text data, mostly scraped from the Internet.'''\n",
        "\n",
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Accumulate all words, all sentences\n",
        "all_words = []\n",
        "for sent in sentences:\n",
        "  words = word_tokenize(sent)\n",
        "  all_words.extend([word.lower() for word in words if word.isalpha()])\n",
        "\n",
        "\n",
        "# Reorder the words in lexicographical order\n",
        "all_words.sort()\n",
        "print (all_words)"
      ],
      "metadata": {
        "id": "Ay2GW9jW5Sli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple function to create a dictionary of word / count\n",
        "def create_word_count_dict(sorted_list_of_words):\n",
        "  word_count_dict = {}\n",
        "  current_word = None\n",
        "  current_count = 0\n",
        "  for word in sorted_list_of_words:\n",
        "    if word != current_word:\n",
        "      if current_word is not None:\n",
        "        word_count_dict[current_word] = current_count\n",
        "      current_word = word\n",
        "      current_count = 1\n",
        "    else:\n",
        "      current_count += 1\n",
        "  if current_word is not None:\n",
        "    word_count_dict[current_word] = current_count\n",
        "  return word_count_dict\n",
        "\n",
        "print (create_word_count_dict(all_words))"
      ],
      "metadata": {
        "id": "huQma2HI5nAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK allows us to do a lot more, especially based on linguistic cues.  However, let's now switch to some tools that use embeddings and transformers to do our tasks."
      ],
      "metadata": {
        "id": "5QmTQEF6bumv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Analysis from a Model on HuggingFace\n",
        "\n",
        "To do sentiment analysis, we'll use a transformer model called *distilbert*. Distilbert, \"fine-tuned\" on a sentiment analysis task, does a fairly good job of capturing sentiment of words and sentences. Note we will be loading the model onto our Colab machine from a model hosting site called HuggingFace."
      ],
      "metadata": {
        "id": "hx4-pWSNYZD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "n-UCnSc9l1EZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\")"
      ],
      "metadata": {
        "id": "mHXj4aQOpyeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Beware Biases on Words from Training on Text\n",
        "\n",
        "Beware that seemingly neutral statements may end up showing sentiment, because the terms themselves were used in positive or negative comments.  It's now known that, e.g., young people view iPhones in a much more favorable light than Android phones. Perhaps that's why we see this?"
      ],
      "metadata": {
        "id": "UMf_e3B-wxMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_pipeline('They bought an Android phone')"
      ],
      "metadata": {
        "id": "yr40t_EXt3s3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_pipeline('They bought an iPhone')"
      ],
      "metadata": {
        "id": "OIigRf2yun2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nonetheless, for the most part transformer-based sentiment analysis works quite well.  Let's see it over product reviews.  Note this is quite expensive computationally!"
      ],
      "metadata": {
        "id": "pD0deQnUvTW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentiment for a DB of Product Reviews"
      ],
      "metadata": {
        "id": "MVgqlv3kw2ut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_df = pd.read_csv('https://storage.googleapis.com/penn-cis5450/GrammarandProductReviews.csv')"
      ],
      "metadata": {
        "id": "65aP1lgyoqNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snacks_df = reviews_df[reviews_df['categories'].apply(lambda x: 'Snacks,' in x)]\n",
        "\n",
        "snacks_df"
      ],
      "metadata": {
        "id": "xlgkBBcSoyG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_text_df = snacks_df[['manufacturer','manufacturerNumber','name','reviews.text']].copy()\n",
        "\n",
        "reviews_text_df"
      ],
      "metadata": {
        "id": "tz41C2l7o1Sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_text_df.dtypes"
      ],
      "metadata": {
        "id": "Z8D0gS3Yo4g9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_text_df['sentiment'] = reviews_text_df['reviews.text'].apply(sentiment_pipeline)\n",
        "\n",
        "reviews_text_df"
      ],
      "metadata": {
        "id": "OR7ft2CHqUWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_text_df['label'] = reviews_text_df['sentiment'].apply(lambda x:x[0]['label'])\n",
        "reviews_text_df['score'] = reviews_text_df.apply(lambda x:x['sentiment'][0]['score'] if x['label'] == 'POSITIVE' else -x['sentiment'][0]['score'], axis=1)\n"
      ],
      "metadata": {
        "id": "Ip5INH8x9Lvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_text_df"
      ],
      "metadata": {
        "id": "O5poZqUoxbNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_text_df[['manufacturer','manufacturerNumber','name','score']].groupby(\n",
        "    by=['manufacturer','name','manufacturerNumber']).mean().sort_values(by='score')"
      ],
      "metadata": {
        "id": "pZUI4RvjxN02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_text_df.describe()"
      ],
      "metadata": {
        "id": "F1IDaeTe_Dnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Named Entity Recognition from a Model on HuggingFace\n",
        "\n",
        "What is a sentence or paragraph talking about?  Knowing the nouns may allow us to understand what's going on, or learn about entitities.\n",
        "\n",
        "For this task, a popular model is called *spaCy*. Again, we can install it on our host machine. It will probably require you to restart your kernel. You can execute from this cell onwards."
      ],
      "metadata": {
        "id": "jbVBsqcNoOfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy[transformers]\n",
        "!pip install -U spacy-experimental\n",
        "!pip install -U spacy-transformers"
      ],
      "metadata": {
        "id": "63ipcB5xBL_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "id": "mF37ToXtBb2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy"
      ],
      "metadata": {
        "id": "o41uY7UOBR3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_lg')"
      ],
      "metadata": {
        "id": "iWpPfWa6Bsl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = '''\n",
        "After standing down from a first attempt Thursday night, SpaceX teams at Cape\n",
        "Canaveral Space Force Station are now on track to launch a Falcon 9 rocket\n",
        "carrying 22 Starlink internet satellites at 11:38 p.m. EDT from Launch Complex 40.\n",
        "\n",
        "An additional launch opportunity for the Starlink 6-16 mission is set for 12:07\n",
        "a.m. EDT. Saturday. Otherwise, two backup opportunities are available Saturday night,\n",
        "at 11:13 p.m. and 11:38 p.m. EDT.'''\n",
        "\n",
        "displacy.render(nlp(text), style='ent', jupyter=True)"
      ],
      "metadata": {
        "id": "TW1rgCujBwHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "displacy.render(nlp(text), style='dep', jupyter=True, options={'compact': True, 'space': 70})"
      ],
      "metadata": {
        "id": "Z2etUpDmXuY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the different types of words in SpaCy (from https://towardsdatascience.com/explorations-in-named-entity-recognition-and-was-eleanor-roosevelt-right-671271117218):\n",
        "\n",
        "```\n",
        "PERSON:      People, including fictional.\n",
        "NORP:        Nationalities or religious or political groups.\n",
        "FAC:         Buildings, airports, highways, bridges, etc.\n",
        "ORG:         Companies, agencies, institutions, etc.\n",
        "GPE:         Countries, cities, states.\n",
        "LOC:         Non-GPE locations, mountain ranges, bodies of water.\n",
        "PRODUCT:     Objects, vehicles, foods, etc. (Not services.)\n",
        "EVENT:       Named hurricanes, battles, wars, sports events, etc.\n",
        "WORK_OF_ART: Titles of books, songs, etc.\n",
        "LAW:         Named documents made into laws.\n",
        "LANGUAGE:    Any named language.\n",
        "DATE:        Absolute or relative dates or periods.\n",
        "TIME:        Times smaller than a day.\n",
        "PERCENT:     Percentage, including ”%“.\n",
        "MONEY:       Monetary values, including unit.\n",
        "QUANTITY:    Measurements, as of weight or distance.\n",
        "ORDINAL:     “first”, “second”, etc.\n",
        "CARDINAL:    Numerals that do not fall under another type.\n",
        "```"
      ],
      "metadata": {
        "id": "6ncz9pV1tC_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "words = []\n",
        "for word in nlp(text).ents:\n",
        "  words.append({'word': word.text, 'type': word.label_})\n",
        "\n",
        "pd.DataFrame(words)"
      ],
      "metadata": {
        "id": "B0q0Kd2kEGE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Named Entity Recognition\n",
        "\n",
        "Let's see how we do, focusing only on \"people, places, and things\"..."
      ],
      "metadata": {
        "id": "iZaiOooFsv-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for ent in nlp(text).ents:\n",
        "  if ent.label_ in ['ORG', 'PERSON', 'PRODUCT', 'NORP', 'FAC', 'GPE']:\n",
        "    print(ent.text, ent.label_)\n"
      ],
      "metadata": {
        "id": "OCc6zoIFsy1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "... Actually it's not *that* great when you look at the labels.  \"Cape Canaveral Space Force Station\" should be a FAC, SpaceX should be an ORG, Falcon should be a PRODUCT, etc."
      ],
      "metadata": {
        "id": "0OchixUa0iJJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zero-Shot Learning\n",
        "\n",
        "Here we'll use a package called `langchain` to send a question to the GPT Large Language Model.  \"Zero shot learning\" simply asks the LLM a question based on what it knows, without giving it any examples of what you expect."
      ],
      "metadata": {
        "id": "f77oCgWobEja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import AzureChatOpenAI\n",
        "from langchain import PromptTemplate\n",
        "import os"
      ],
      "metadata": {
        "id": "4SiENAyB7dJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ],
      "metadata": {
        "id": "ZV7K4RTwkjgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "endpoint = \"https://zives-cis5450-openai.openai.azure.com/\"\n",
        "model_name = \"gpt-4.1-mini\"\n",
        "deployment = \"gpt-4.1-mini\"\n",
        "\n",
        "subscription_key = str(os.getenv('AZURE_OPENAI_API_KEY'))\n",
        "api_version = \"2024-12-01-preview\"\n",
        "\n",
        "llm = AzureChatOpenAI(\n",
        "    api_version=api_version,\n",
        "    deployment_name=deployment,\n",
        "    azure_endpoint=endpoint,\n",
        "    api_key=subscription_key\n",
        ")\n",
        "llm_chain = prompt | llm"
      ],
      "metadata": {
        "id": "8tSa15pjky3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What are the main topics of a big data course?\"\n",
        "\n",
        "response = llm_chain.invoke({\"question\": question})\n",
        "\n",
        "for sentence in response.content.split('\\n'):\n",
        "  print (sentence)"
      ],
      "metadata": {
        "id": "Mi2V50n9lH1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Relation Extraction via Azure OpenAI\n",
        "\n",
        "Relation extraction involves taking text and trying to populate a schema.  Sometimes one must do this via \"few-shot\" learning (provide a few examples) but for simpler cases zero-shot learning (with the schema) may be adequate.\n",
        "\n",
        "Here's an example from the text copied from an Internet Movie Database poll.\n"
      ],
      "metadata": {
        "id": "cGUXeMdKbVY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input from IMDB poll on best movie characters, https://www.imdb.com/poll/gBcmBMHGh4k/results?ref_=po_sr"
      ],
      "metadata": {
        "id": "DpTEnrtA4SrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/penn-cis5450/imdb-poll.html"
      ],
      "metadata": {
        "id": "ly63D4oB1TMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're going to slightly simplify the document, so it costs less to have GPT process it. This step isn't strictly necessary if you have infinite money."
      ],
      "metadata": {
        "id": "5yozGljwZqly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def remove_javascript_from_html(html_content):\n",
        "    \"\"\"\n",
        "    Parses an HTML document and removes all <script> tags and their content.\n",
        "\n",
        "    Args:\n",
        "        html_content (str): The HTML document as a string.\n",
        "\n",
        "    Returns:\n",
        "        str: The HTML document with all JavaScript removed.\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "    # Find all <script> tags and remove them\n",
        "    for script_tag in soup.find_all('script'):\n",
        "        script_tag.decompose()\n",
        "\n",
        "    return str(soup)\n",
        "\n",
        "try:\n",
        "    with open(\"imdb-poll.html\", \"r\") as f_in:\n",
        "        html_from_file = f_in.read()\n",
        "\n",
        "    html_without_js_from_file = remove_javascript_from_html(html_from_file)\n",
        "\n",
        "    with open(\"poll.html\", \"w\") as f_out:\n",
        "        f_out.write(html_without_js_from_file)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: imdb-poll.html not found.\")"
      ],
      "metadata": {
        "id": "pOgaP1d2ZMq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional\n",
        "from pydantic import Field, BaseModel\n",
        "\n",
        "class Movie(BaseModel):\n",
        "    ranked: int = Field(description=\"The rank of the character\")\n",
        "    actor: str = Field(description=\"The actor in the movie\")\n",
        "    character: str = Field(description=\"The character in the movie\")\n",
        "    votes: int = Field(description=\"The number of votes\")\n",
        "    movie: str = Field(description=\"The name of the movie\")\n",
        "\n",
        "\n",
        "class Document(BaseModel):\n",
        "    actors: List[Movie] = Field(..., description=\"List of movie actors and characters\")\n",
        "\n",
        "# Input from IMDB poll on best movie characters, https://www.imdb.com/poll/gBcmBMHGh4k/results?ref_=po_sr\n",
        "with open('poll.html','rt') as inp:\n",
        "  input_data = inp.read()\n",
        "\n",
        "structured_llm = llm.with_structured_output(Document)\n",
        "results = structured_llm.invoke(\"You are an extraction algorithm. Please extract every possible instance of quotation information.\\n\\n\" + input_data)\n",
        "\n",
        "print(results)"
      ],
      "metadata": {
        "id": "259OseTw3GZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = pd.DataFrame([character.dict() for character in results.actors])\n",
        "results_df"
      ],
      "metadata": {
        "id": "34IdeKAD7IAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise\n",
        "\n",
        "Take the list of Penn CIS courses and extract the information into a DataFrame!"
      ],
      "metadata": {
        "id": "3-YdWa0RtyRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/penn-cis5450/cis-catalog.html"
      ],
      "metadata": {
        "id": "ifH-m5b557At"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a class specifying the schema to extract. It should include the fields `course`, `name`, `prerequisites`, `units`, `description`, and `frequency`."
      ],
      "metadata": {
        "id": "X_oTFEwE8G5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: use \"structured output\" to map the text\n",
        "# to a series of nested objects (by defining classes with properties).\n",
        "# These are:\n",
        "#  A Document has a list of Courses\n",
        "#  A Course has a course of type CourseNumber, as well as the fields above.\n",
        "#    Prerequisites should be a list of CourseNumber as well.\n",
        "#  A CourseNumber has a degree program and a number.\n",
        "from typing import List, Optional\n",
        "from pydantic import Field, BaseModel\n",
        "\n",
        "class CourseNumber(BaseModel):\n",
        "  # TODO\n",
        "\n",
        "class Course(BaseModel):\n",
        "  course: CourseNumber = Field(description=\"The course number\")\n",
        "  # TODO\n",
        "\n",
        "class Document(BaseModel):\n",
        "    courses: # TODO\n",
        "\n",
        "with open('cis-catalog.html','rt') as inp:\n",
        "  input_data = inp.read()\n",
        "\n",
        "structured_llm = llm.with_structured_output(Document)\n",
        "results = structured_llm.invoke(\"You are an extraction algorithm. Please extract every possible instance of course information.\\n\\n\" + input_data)\n",
        "\n",
        "print(results)"
      ],
      "metadata": {
        "id": "B6GTkgqfurVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = pd.DataFrame([course.model_dump() for course in results.courses])\n",
        "results_df\n"
      ],
      "metadata": {
        "id": "yyb7Ry0T5uOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is just to catch simple mistakes\n",
        "\n",
        "if 'name' not in results_df.columns or 'units' not in results_df.columns:\n",
        "  print('Please revise your schema according to the spec')"
      ],
      "metadata": {
        "id": "nLc5FcsA8R74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile notebook-config.yaml\n",
        "\n",
        "grader_api_url: 'https://23whrwph9h.execute-api.us-east-1.amazonaws.com/default/Grader23'\n",
        "grader_api_key: 'flfkE736fA6Z8GxMDJe2q8Kfk8UDqjsG3GVqOFOa'"
      ],
      "metadata": {
        "id": "yUlTyBOY8lj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install penngrader-client"
      ],
      "metadata": {
        "id": "6Ds_lWZm8sK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW WHO\n",
        "#TO ASSIGN POINTS TO YOU IN OUR BACKEND\n",
        "STUDENT_ID = 99999999 # YOUR PENN-ID GOES HERE AS AN INTEGER##PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW WHO"
      ],
      "metadata": {
        "id": "Ky9P3kGv8w0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%set_env HW_ID=cis5450_25f_HW9"
      ],
      "metadata": {
        "id": "gDjm-_O88xzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from penngrader.grader import *\n",
        "\n",
        "grader = PennGrader('notebook-config.yaml', os.environ['HW_ID'], STUDENT_ID, STUDENT_ID)"
      ],
      "metadata": {
        "id": "YdYKpTCx80VU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grader.grade('extracted_courses', results_df)"
      ],
      "metadata": {
        "id": "M5NieNzy8aPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lK9BOlme910y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}