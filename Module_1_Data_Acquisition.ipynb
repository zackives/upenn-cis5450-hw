{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zackives/upenn-cis5450-hw/blob/main/Module_1_Data_Acquisition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_jsTACdAaZe"
      },
      "source": [
        "# Overview of Part I of Big Data Analytics\n",
        "\n",
        "As we start our journey into Big Data Analytics, the first thing we need to do is **get the data** in the form we need for analysis!  We'll start with an overview of how to acquire and *wrangle* data.\n",
        "\n",
        "This notebook will be built incrementally to consider several tasks:\n",
        "\n",
        "* Acquiring data from files and remote sources\n",
        "* Information extraction over HTML content\n",
        "* A basic \"vocabulary\" of operators over tables (the relational algebra)\n",
        "* Basic manipulation using SQL in DuckDB\n",
        "\n",
        "* \"Data wrangling\" or integration:\n",
        "  * Cleaning and filtering data, using rules and based operations\n",
        "  * Linking data across dataframes or relations\n",
        "  * The need for approximate match and record linking\n",
        "  * Different techniques\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N01ql5zclw7M"
      },
      "source": [
        "## The Motivating Question\n",
        "To illustrate the principles, we focus on the question of **how old company CEOs and founders** (in general, leaders) are.  The question was in part motivated by the following New York Times article:\n",
        "\n",
        "* Founders of Successful Tech Companies Are Mostly Middle-Aged: https://www.nytimes.com/2019/08/29/business/tech-start-up-founders-nest.html?searchResultPosition=2\n",
        "\n",
        "So let's test this hypothesis!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial Libraries\n",
        "\n",
        "We'll be using [DuckDB](https://duckdb.org/) as a means of managing our tables.  DuckDB works like a Python library, but manages a full SQL database (in files).  It also integrates very nicely with Pandas, so we'll use it in this course."
      ],
      "metadata": {
        "id": "v7P39UsKpdoK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dy4ltbGCt6MZ"
      },
      "outputs": [],
      "source": [
        "!pip3 install duckdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YT1J84O3wCju"
      },
      "outputs": [],
      "source": [
        "!pip3 install lxml"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile notebook-config.yaml\n",
        "\n",
        "grader_api_url: 'https://23whrwph9h.execute-api.us-east-1.amazonaws.com/default/Grader23'\n",
        "grader_api_key: 'flfkE736fA6Z8GxMDJe2q8Kfk8UDqjsG3GVqOFOa'"
      ],
      "metadata": {
        "id": "9TZsWfgY41De"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install penngrader-client"
      ],
      "metadata": {
        "id": "hZY7k8Z14lWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For quiz credit you'll need to update your student ID here!"
      ],
      "metadata": {
        "id": "HkENA-6r4vYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW WHO\n",
        "#TO ASSIGN POINTS TO YOU IN OUR BACKEND\n",
        "STUDENT_ID = 99999999 # YOUR PENN-ID GOES HERE AS AN INTEGER##PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW WHO"
      ],
      "metadata": {
        "id": "L3_sSMr84uCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quizzes will cumulatively count as HW9... Don't edit this..."
      ],
      "metadata": {
        "id": "oGAFhhVF5LZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%set_env HW_ID=cis5450_25f_HW9"
      ],
      "metadata": {
        "id": "93y6gp6l5DJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from penngrader.grader import *\n",
        "\n",
        "grader = PennGrader('notebook-config.yaml', os.environ['HW_ID'], STUDENT_ID, STUDENT_ID)"
      ],
      "metadata": {
        "id": "zt58w5q_43eQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Xah24fnRRGZ"
      },
      "outputs": [],
      "source": [
        "# Imports we'll use through the notebook, collected here for simplicity\n",
        "\n",
        "# For parsing dates and being able to compare\n",
        "import datetime\n",
        "\n",
        "# For fetching remote data\n",
        "import urllib\n",
        "import urllib.request\n",
        "\n",
        "# Pandas dataframes and operations\n",
        "import pandas as pd\n",
        "\n",
        "# Numpy matrix and array operations\n",
        "import numpy as np\n",
        "\n",
        "# Sqlite is a simplistic database\n",
        "import duckdb\n",
        "\n",
        "# Data visualization\n",
        "import matplotlib\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWOeu1wHZDqz"
      },
      "source": [
        "# 1. Acquiring External Data\n",
        "\n",
        "To test our hypothesis, we might want:\n",
        "\n",
        "1. A list of companies (and, for futher details, perhaps their lines of business)\n",
        "2. A list of company CEOs\n",
        "3. Ages of the CEOs\n",
        "\n",
        "We'll go through each of these using real data from the web.\n",
        "\n",
        "### Reading Structured Data Sources\n",
        "\n",
        "Let's start by looking up data about companies.  We are using a dataset from: https://www.kaggle.com/datasets/peopledatalabssf/free-7-million-company-dataset?resource=download\n",
        "\n",
        "but we have a copy of it at an alternate site for convenience of downloading."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1. External CSV Data\n",
        "\n",
        "Comma-separated values are generally easy to read. The main questions are column headings (which are in an optional row that isn't always provided) and datatypes (which might default to the wrong thing)."
      ],
      "metadata": {
        "id": "QFy0S5W4IrG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -nc https://storage.googleapis.com/penn-cis5450/companies_sorted.csv"
      ],
      "metadata": {
        "id": "AOoXr_uamjnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1P7Ry8RPXFRp"
      },
      "outputs": [],
      "source": [
        "# This reads remotely. To avoid multiple fetches, we'll instead..\n",
        "\n",
        "# data = urllib.request.urlopen(\\\n",
        "#        'https://storage.googleapis.com/penn-cis5450/companies_sorted.csv')\n",
        "# company_data_df = pd.read_csv(data)\n",
        "\n",
        "## ... instead copy to a local file and read there...\n",
        "\n",
        "company_data_df = pd.read_csv('companies_sorted.csv')\n",
        "\n",
        "company_data_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's use DuckDB to work with the dataframe."
      ],
      "metadata": {
        "id": "NQAxbHoWqFQJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARqMkBYGEMjp"
      },
      "outputs": [],
      "source": [
        "# We can ask for the contents of a Pandas Dataframe through DuckDB, in the SQL language.\n",
        "duckdb.sql(\"\"\"SELECT *\n",
        "              FROM company_data_df\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2. Storing Data Locally and Re-Loading it\n",
        "\n",
        "DuckDB nicely integrates with Pandas and Python. If you create a connection to a file, this results in the creation of a database stored within that file.\n",
        "\n",
        "Normally we need to `CREATE TABLE` with the table name and columns. But we can actually create the table to match the *schema* of the DataFrame, as follows."
      ],
      "metadata": {
        "id": "zcsowIrxnk6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "con = duckdb.connect('local.db')\n",
        "con.sql(\"\"\"CREATE TABLE IF NOT EXISTS company_data AS\n",
        "           SELECT *\n",
        "           FROM company_data_df\"\"\")\n",
        "\n",
        "# query the table\n",
        "con.table(\"company_data\").show()"
      ],
      "metadata": {
        "id": "QhG3KQZinP-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "company_data_df = con.table(\"company_data\").df()\n",
        "\n",
        "company_data_df"
      ],
      "metadata": {
        "id": "mwFiYO1YqTVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcP3q-yREiZJ"
      },
      "source": [
        "## 1.3. Companies' CEOs: a Web Table\n",
        "\n",
        "Now we need to figure out who the CEOs are for corporations.  One place to look is Wikipedia, which has an HTML table describing the CEOs.\n",
        "\n",
        "https://en.wikipedia.org/wiki/List_of_chief_executive_officers#List_of_CEOs\n",
        "\n",
        "Pandas actually makes it easy to read HTML tables..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtMlN0XAATIA"
      },
      "outputs": [],
      "source": [
        "# Now let's read an HTML table!\n",
        "\n",
        "company_ceos_df = pd.read_html('https://en.wikipedia.org/wiki/List_of_chief_executive_officers#List_of_CEOs')[1]\n",
        "\n",
        "company_ceos_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "con.sql('create table if not exists company_ceos as select * from company_ceos_df')\n",
        "\n",
        "con.table('company_ceos').show()"
      ],
      "metadata": {
        "id": "G6nItgjSqiZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_ssZuLYZfkT"
      },
      "source": [
        "## 1.4. The Problem Gets Harder... Extracting Fields from Tagged Data on the Web\n",
        "\n",
        "So far we have companies and CEOs.  But we don't have information on how old the CEOs are!\n",
        "\n",
        "For a solution, we're going to go back to Wikipedia -- this time looking at the web pages for the CEOs!\n",
        "\n",
        "This involves \"crawling\" the CEO pages, and \"scraping\" the relevant content.  In other words we have to do *information extraction*.  For this particular problem, we will do extraction over very regular parts of Wikipedia.\n",
        "\n",
        "We'll start by constructing a list of CEO web pages, from the Company CEO dataframe above.  For this, we need to take the names and do a bit of tweaking, for example adding underscores instead of spaces.\n",
        "\n",
        "(Later we'll see how to do this over more free-form text.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbnYN696wmPo"
      },
      "outputs": [],
      "source": [
        "def get_ceo_urls(company_ceos_df):\n",
        "  crawl_list = []\n",
        "\n",
        "  for executive in company_ceos_df['Executive']:\n",
        "    crawl_list.append('https://en.wikipedia.org/wiki/' + executive.replace(' ', '_'))\n",
        "  return crawl_list\n",
        "\n",
        "crawl_list = get_ceo_urls(company_ceos_df)\n",
        "\n",
        "crawl_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Zc3jtg5UAcHB"
      },
      "outputs": [],
      "source": [
        "# Use urllib.urlopen to crawl all pages in crawl_list, and store the response of the page\n",
        "# in list pages\n",
        "\n",
        "pages = []\n",
        "\n",
        "for url in crawl_list:\n",
        "    page = url.split(\"/\")[-1] #extract the person name at the end of the url\n",
        "\n",
        "    # An issue: some of the accent characters won't work.  We need to convert them\n",
        "    # into an HTML URL.  We'll split the URL, then use \"parse.quote\" to change\n",
        "    # the structure, then re-form the URL\n",
        "    url_list = list(urllib.parse.urlsplit(url))\n",
        "    url_list[2] = urllib.parse.quote(url_list[2])\n",
        "    url_ascii = urllib.parse.urlunsplit(url_list)\n",
        "    try:\n",
        "      response = urllib.request.urlopen((url_ascii))\n",
        "      #Save page and url for later use.\n",
        "      pages.append(response)\n",
        "    except urllib.error.URLError as e:\n",
        "      print(e.reason)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUlaNKlqZwHo"
      },
      "source": [
        "## 1.5. Crawling: Populating the Table with Executives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lz0zHTxdw92L"
      },
      "outputs": [],
      "source": [
        "# Use lxml.etree.HTML(...) on the HTML content of each page to get a DOM tree that\n",
        "# can be processed via XPath to extract the bday information.  Store the CEO name,\n",
        "# webpage, and the birthdate (born) in exec_df.\n",
        "\n",
        "# We first check that the HTML content has a table of type `vcard`,\n",
        "# and then extract the `bday` information.  If there is no birthdate, the datetime\n",
        "# value is NaT (not a type).\n",
        "\n",
        "from lxml import etree\n",
        "\n",
        "rows = []\n",
        "for page in pages:\n",
        "  url = page.geturl()\n",
        "  print (url)\n",
        "  content = page.read().decode(\"utf-8\")\n",
        "  tree = etree.HTML(content)  #create a DOM tree of the page\n",
        "  bday = tree.xpath('//table[contains(@class,\"vcard\")]//span[@class=\"bday\"]/text()')\n",
        "  if len(bday) > 0:\n",
        "      name = url[url.rfind('/')+1:] # The part of the URL after the last /\n",
        "      rows.append({'name': name, 'page': url,\n",
        "                  'born': datetime.datetime.strptime(bday[0], '%Y-%m-%d')})\n",
        "  else:\n",
        "          rows.append({'name': url[url.rfind('/')+1:], 'page': url\n",
        "                                    , 'born': np.datetime64('NaT')})\n",
        "\n",
        "exec_df = pd.DataFrame(rows)\n",
        "exec_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "con.sql('create table if not exists executives as select * from exec_df')\n",
        "\n",
        "con.table('executives').show()"
      ],
      "metadata": {
        "id": "kxrXD90AoxBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 1 Exercises\n",
        "\n",
        "Extract, as a dataframe, the list of cities in Pennsylvania (see https://en.wikipedia.org/wiki/List_of_cities_in_Pennsylvania). Store these in the dataframe `pa_cities_df`."
      ],
      "metadata": {
        "id": "2ZO4ysEGJZr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pa_cities_df = # Do something here!\n",
        "\n",
        "pa_cities_df"
      ],
      "metadata": {
        "id": "wP4XaBiwJ-u4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check (and record) your answer...  You can retry until you get things right!"
      ],
      "metadata": {
        "id": "bCM6qfUHKVGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grader.grade('cities_quiz', answer=pa_cities_df)"
      ],
      "metadata": {
        "id": "wiJHsGu7KGL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Here is a helper function to visualize A DOM tree\n",
        "\n",
        "It uses the GraphViz library.  Run the next couple of cells to define `show_dom` which takes a string."
      ],
      "metadata": {
        "id": "3sE1nGPNsSVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install graphviz"
      ],
      "metadata": {
        "id": "fmluiEiZsSrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import graphviz\n",
        "from bs4 import BeautifulSoup\n",
        "from bs4.element import Tag, NavigableString\n",
        "import bs4\n",
        "from IPython.display import Image\n",
        "\n",
        "def visualize_dom_tree(html_content):\n",
        "    \"\"\"Visualizes a DOM tree from HTML content using Graphviz.\n",
        "\n",
        "    Args:\n",
        "        html_content (str): The HTML content to parse.\n",
        "\n",
        "    Returns:\n",
        "        graphviz.Digraph: The generated Graphviz graph object.\n",
        "    \"\"\"\n",
        "\n",
        "    # Parse the HTML content using BeautifulSoup\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "    # Create a Graphviz graph object\n",
        "    graph = graphviz.Digraph(comment='DOM Tree Visualization')\n",
        "\n",
        "    # Create a recursive function to traverse the DOM tree and add nodes and edges\n",
        "    def add_node_and_edges(tag, parent_node):\n",
        "        # Create a node for the tag\n",
        "        node_id = f\"node_{id(tag)}\"\n",
        "        if isinstance(tag, bs4.element.Tag):\n",
        "          graph.node(node_id, label=tag.name)\n",
        "        else:\n",
        "          graph.node(node_id, label=tag.string)\n",
        "\n",
        "        # Add an edge from the parent node to this node\n",
        "        if parent_node:\n",
        "            graph.edge(parent_node, node_id)\n",
        "\n",
        "        # Recursively add child nodes\n",
        "        if isinstance(tag, bs4.element.Tag):\n",
        "          for child in tag.children:\n",
        "              add_node_and_edges(child, node_id)\n",
        "\n",
        "    # Start the recursion with the root element\n",
        "    add_node_and_edges(soup, None)\n",
        "\n",
        "    return graph\n",
        "\n",
        "def show_dom(content):\n",
        "  graph = visualize_dom_tree(content.strip())\n",
        "  graph.render('dom_tree.gv', view=True, format='png')\n",
        "  return Image('dom_tree.gv.png')"
      ],
      "metadata": {
        "id": "t7QqpsVAsUsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a simple example using the results from the executives..."
      ],
      "metadata": {
        "id": "iRJh8zWPsdfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from lxml import etree\n",
        "\n",
        "## Simple example\n",
        "\n",
        "tree = etree.HTML(content)  #create a DOM tree of the page\n",
        "example = tree.xpath('//table[contains(@class,\"vcard\")]')\n",
        "\n",
        "# example is a list of nodes\n",
        "# let's take the first element of the list (in our case there should only be one)\n",
        "# and let's convert it back to a Unicode string using etree.tostring(...)\n",
        "\n",
        "show_dom(etree.tostring(example[0], encoding='unicode'))"
      ],
      "metadata": {
        "id": "E4mz23b5sX3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's actually do our task! You can refer back to Section 1.4 for example code.\n",
        "\n",
        "* Use `urllib.request.urlopen` to fetch https://en.wikipedia.org/wiki/List_of_women_CEOs_of_Fortune_500_companies.\n",
        "\n",
        "* Read, decode, and parse the page into a DOM tree called `tree`.\n",
        "\n"
      ],
      "metadata": {
        "id": "pVWVXCoasfv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "page = urllib.request.urlopen( # TODO\n",
        "\n",
        "tree = # TODO\n"
      ],
      "metadata": {
        "id": "UpnmyQxlshy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now Compute a list called `rows` with all rows (`tr`) within the single `table` in the DOM tree. Remember to use `//` and `/` appropriately. Beware that the table has a `tbody` between the `table` and `tr`."
      ],
      "metadata": {
        "id": "5p7m6yJnsumg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rows = tree.xpath(# TODO\n",
        "\n",
        "show_dom(etree.tostring(rows[1], encoding='unicode'))"
      ],
      "metadata": {
        "id": "penD3fjcslcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using XPath, compute, in the variable `oracle_ceo`, the DOM element for the *name* of the CEO of `Oracle` corporation.  You might want to look at https://en.wikipedia.org/wiki/List_of_women_CEOs_of_Fortune_500_companies in your browser and View source."
      ],
      "metadata": {
        "id": "iTY9V2Oks15Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "oracle_ceo = tree.xpath(# TODO\n",
        "\n",
        "\n",
        "# This will help you see what you got. A list of elements, a Unicode\n",
        "# string, or a DOM tree\n",
        "if len(oracle_ceo) > 1:\n",
        "  print ('Multiple answers: {}'.format(oracle_ceo))\n",
        "elif isinstance(oracle_ceo[0], etree._ElementUnicodeResult):\n",
        "  print(str(oracle_ceo[0]))\n",
        "else:\n",
        "  show_dom(etree.tostring(oracle_ceo[0], encoding='unicode'))"
      ],
      "metadata": {
        "id": "XscBMVxms2WK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Once you actually get a person's name that you think is right, submit here.\n",
        "grader.grade('ceo', answer=str(oracle_ceo[0]))"
      ],
      "metadata": {
        "id": "_fFGTvxRtBqI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "dataset_inspector": {
      "cols": {
        "lenName": 24,
        "lenType": 16,
        "lenVar": 40
      },
      "dataset_inspector_section_display": "none",
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "oldHeight": 88.66133400000001,
      "position": {
        "height": "40px",
        "left": "0px",
        "right": "20px",
        "top": "0px",
        "width": "800px"
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}