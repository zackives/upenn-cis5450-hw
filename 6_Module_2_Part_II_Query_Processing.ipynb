{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zackives/upenn-cis5450-hw/blob/main/6_Module_2_Part_II_Query_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49bwxLwZBJw7"
      },
      "source": [
        "# Lecture Module 2.2: Making Choices about Data Processing\n",
        "\n",
        "## LinkedIn Social Analysis\n",
        "\n",
        "Our next module explores concepts in:\n",
        "\n",
        "* Algorithmic implications of design choices\n",
        "* Techniques for indexing, parallelism, and sequence\n",
        "\n",
        "It sets the stage for Module 3, which focuses on cloud/cluster-compute data processing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -nc https://storage.googleapis.com/penn-cis5450/linkedin_anon.jsonl"
      ],
      "metadata": {
        "id": "XWdHUSWxS7un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Access Patterns\n",
        "\n",
        "Let's create two data structures, an integer list and a dictionary (hash map).  Each will have the same data."
      ],
      "metadata": {
        "id": "DartjPIt6Y7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "intlist = []\n",
        "for i in range(0,5000000):\n",
        "  intlist.append((i+1,'a value'))\n",
        "\n",
        "intdict = {}\n",
        "for i in range(0,5000000):\n",
        "  intdict[i] = ((i+1,'a value'))"
      ],
      "metadata": {
        "id": "tV7iGScA6bgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "count = 0\n",
        "for i in range(0,len(intlist)):\n",
        "  count += intlist[i][0]"
      ],
      "metadata": {
        "id": "TIND_ce561-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "count = 0\n",
        "for i in range(0,len(intdict)):\n",
        "  count += intdict[i][0]"
      ],
      "metadata": {
        "id": "eTP2Zp6F673J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# All 50,000+ records from linkedin\n",
        "linked_in = open('linkedin_anon.jsonl')\n",
        "\n",
        "copied_data = open('linkedin_anon_copy.jsonl','w', buffering=1)\n",
        "\n",
        "count = 0\n",
        "for repeat in range(0,10):\n",
        "  linked_in.seek(0)\n",
        "  for line in linked_in:\n",
        "    count += 1\n",
        "    copied_data.write(line)\n",
        "\n",
        "print (f\"Copied {count} records\")"
      ],
      "metadata": {
        "id": "ERhe6Ewg8mZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# All 50,000+ records from linkedin\n",
        "linked_in = open('linkedin_anon.jsonl')\n",
        "\n",
        "copied_data = open('linkedin_anon_copy.jsonl','w', buffering=4096)\n",
        "\n",
        "count = 0\n",
        "for repeat in range(0,10):\n",
        "  linked_in.seek(0)\n",
        "  for line in linked_in:\n",
        "    count += 1\n",
        "    copied_data.write(line)\n",
        "\n",
        "print (f\"Copied {count} records\")"
      ],
      "metadata": {
        "id": "qDwlpNLS91ER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCWrRHRXBJxq"
      },
      "source": [
        "# Big Data Takes a Long Time to Process\n",
        "\n",
        "Now that we've seen how to do fairly complex queries over data in relations, we'll \"pop back\" to our big data example, which is the LinkedIn dataset.  Recall that we had a segment of the LinkedIn input file in our previous examples earlier in this module."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install lxml\n",
        "!pip3 install duckdb"
      ],
      "metadata": {
        "id": "V648_1nFTAjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# JSON parsing\n",
        "import json\n",
        "\n",
        "# HTML parsing\n",
        "from lxml import etree\n",
        "import urllib\n",
        "\n",
        "# DuckDB RDBMS\n",
        "import duckdb\n",
        "\n",
        "# Time conversions\n",
        "import time"
      ],
      "metadata": {
        "id": "DZ64kVG7TCWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "Wygsp1wyBJxr"
      },
      "source": [
        "%%time\n",
        "# 50,000 records from linkedin\n",
        "linked_in = open('linkedin_anon.jsonl')\n",
        "\n",
        "people = []\n",
        "\n",
        "for line in linked_in:\n",
        "    person = json.loads(line)\n",
        "    people.append(person)\n",
        "\n",
        "people_df = pd.DataFrame(people)\n",
        "people_df[people_df['industry'] == 'Medical Devices']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eDL3xNtBJxs"
      },
      "source": [
        "%%time\n",
        "# 500,000 records from linkedin\n",
        "linked_in = open('linkedin_anon.jsonl')\n",
        "\n",
        "people = []\n",
        "\n",
        "for line in linked_in:\n",
        "    person = json.loads(line)\n",
        "    if 'industry' in person and person['industry'] == 'Medical Devices':\n",
        "        people.append(person)\n",
        "\n",
        "people_df = pd.DataFrame(people)\n",
        "people_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cl_KntTIBJxt"
      },
      "source": [
        "## SQL query without an index\n",
        "\n",
        "SQL databases will automatically \"push down\" selection and projection where feasible.  They also don't need to parse.\n",
        "\n",
        "Let's load people_df into tables as per our prior notebook."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Simple code to pull out data from JSON and load into DuckDB.\n",
        "'''\n",
        "import ast\n",
        "\n",
        "linked_in = open('linkedin_anon.jsonl')\n",
        "\n",
        "START = 0\n",
        "LIMIT = 50000\n",
        "\n",
        "def get_df(rel):\n",
        "    ret = pd.DataFrame(rel)\n",
        "    return ret\n",
        "\n",
        "lines = []\n",
        "i = 1\n",
        "for line in linked_in:\n",
        "    if i > START + LIMIT:\n",
        "        break\n",
        "    elif i >= START:\n",
        "        person = json.loads(line)\n",
        "\n",
        "        lines.append(person)\n",
        "    i = i + 1\n",
        "\n",
        "people_df = get_df(pd.DataFrame(lines))\n",
        "\n"
      ],
      "metadata": {
        "id": "4pTCCGh8SluZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "people_df"
      ],
      "metadata": {
        "id": "GGJem6gxTW5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_nested_dict(rel, name):\n",
        "  # This evaluates the string that describes the dictionary, as a dictionary\n",
        "  # definition\n",
        "  ret = rel.copy()\n",
        "  # ret[name] = rel[name].map(lambda x: ast.literal_eval(x) if len(x) else np.NaN)\n",
        "  ret = ret.dropna()\n",
        "  # This joins rows on the index\n",
        "  return ret.drop(columns=name).join(pd.DataFrame(ret[name].tolist()))\n",
        "\n",
        "def get_nested_list(rel, name):\n",
        "  ret = rel.copy()\n",
        "  ret = ret.dropna().explode(name).dropna()\n",
        "  ret = ret.join(pd.DataFrame(ret[name].tolist())).drop(columns=name).drop_duplicates()\n",
        "  return ret.rename(columns={0: name})\n",
        "\n",
        "def get_nested_list_dict(rel, name):\n",
        "  ret = rel.copy()\n",
        "\n",
        "  ret = ret.dropna().explode(name)\n",
        "\n",
        "  exploded_pairs = pd.DataFrame(ret.apply(lambda x: {'_id': x['_id']} | x[name] if isinstance(x[name], dict) else {'_id': x['_id']}, axis=1).tolist())\n",
        "\n",
        "  return ret.merge(exploded_pairs, on='_id').drop(columns=name)\n",
        "  #pd.DataFrame(ret[name].tolist())).drop(columns=name).drop_duplicates()\n",
        "\n",
        "# Take the lists, drop any blank strings\n",
        "specialties_df = people_df[['_id','specilities']].explode('specilities').rename(columns={'_id': 'person'})\n",
        "specialties_df.dropna(inplace=True)\n",
        "interests_df = people_df[['_id','interests']].explode('interests').rename(columns={'_id': 'person'})\n",
        "interests_df.dropna(inplace=True)\n",
        "\n",
        "names_df = get_nested_dict(people_df[['_id','name']], 'name')\n",
        "\n",
        "education_df = get_nested_list_dict(people_df[['_id','education']], 'education')\n",
        "experience_df = get_nested_list_dict(people_df[['_id','experience']], 'experience')\n",
        "skills_df = get_nested_list(people_df[['_id','skills']], 'skills')\n",
        "honors_df = get_nested_list(people_df[['_id','honors']], 'honors')\n",
        "events_df = get_nested_list_dict(people_df[['_id','events']], 'events')\n",
        "\n",
        "groups_df = get_nested_dict(people_df[['_id','group']], 'group')\n",
        "\n",
        "people_only_df = people_df.drop(columns=['name','education','group','skills','experience','honors','events','specilities','interests'])"
      ],
      "metadata": {
        "id": "LCJbwD1XSUMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2wxLzk_BJxt"
      },
      "source": [
        "## This is just to reset things so we don't have an index\n",
        "conn = duckdb.connect('linkedin.db')\n",
        "conn.execute('BEGIN TRANSACTION')\n",
        "conn.execute('DROP TABLE IF EXISTS people')\n",
        "conn.execute('DROP INDEX IF EXISTS people_industry')\n",
        "conn.execute('CREATE TABLE people AS SELECT * FROM people_df')\n",
        "conn.execute('CREATE TABLE education AS SELECT * FROM education_df')\n",
        "conn.execute('CREATE TABLE experience AS SELECT * FROM experience_df')\n",
        "conn.execute('CREATE TABLE skills AS SELECT * FROM skills_df')\n",
        "conn.execute('CREATE TABLE honors AS SELECT * FROM honors_df')\n",
        "conn.execute('CREATE TABLE events AS SELECT * FROM events_df')\n",
        "conn.execute('CREATE TABLE groups AS SELECT * FROM groups_df')\n",
        "conn.execute('CREATE TABLE specialties AS SELECT * FROM specialties_df')\n",
        "conn.execute('CREATE TABLE interests AS SELECT * FROM interests_df')\n",
        "conn.execute('COMMIT')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PG1zu17DBJxv"
      },
      "source": [
        "%%time\n",
        "\n",
        "conn.sql(\"\"\"\n",
        "  SELECT *\n",
        "  FROM people JOIN experience ON people._id = experience._id\n",
        "  WHERE industry='Medical Devices'\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VMjQn9fBJxw"
      },
      "source": [
        "## Let's build an index now...\n",
        "\n",
        "Our data is very small, so the index probably won't speed anything up at this scale. But it can be created and the database will use it *transparently*!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZn3LlQHBJxx"
      },
      "source": [
        "conn.execute('BEGIN TRANSACTION')\n",
        "conn.execute('DROP INDEX IF EXISTS people_industry')\n",
        "conn.execute(\"CREATE INDEX people_industry ON people(industry)\")\n",
        "conn.execute('COMMIT')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbZn2ZKABJxy"
      },
      "source": [
        "%%time\n",
        "# Treat the view as a table, see what's there\n",
        "conn.sql(\"\"\"\n",
        " CREATE VIEW people_medicine AS\n",
        "  SELECT *\n",
        "  FROM people JOIN experience ON people._id = experience._id\n",
        "  WHERE industry='Medical Devices'\"\"\")\n",
        "\n",
        "conn.sql(\"\"\"\n",
        "  SELECT *\n",
        "  FROM people_medicine\"\"\")\n",
        "\n",
        "# In our tests, this was 5x faster!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "conn.sql(\"\"\"\n",
        "  SELECT name.given_name, name.family_name\n",
        "  FROM people\n",
        "  WHERE name.given_name='Jeeves'\"\"\")"
      ],
      "metadata": {
        "id": "kV6z8n82T64t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11EtG9P6BJx0"
      },
      "source": [
        "people_df2 = conn.sql('select * from people limit 500').df()\n",
        "experience_df2 = conn.sql('select * from experience limit 5000').df()\n",
        "skills_df2 = conn.sql('select * from skills limit 8000').df()\n",
        "\n",
        "print (\"%d people\"%len(people_df2))\n",
        "print (\"%d experiences\"%len(experience_df2))\n",
        "print (\"%d skills\"%len(skills_df2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IquppboSBJx2"
      },
      "source": [
        "def merge(S,T,l_on,r_on):\n",
        "    ret = []\n",
        "    count = 0\n",
        "    s_pos = S.columns.get_loc(l_on)\n",
        "    t_pos = T.columns.get_loc(r_on)\n",
        "    for s_index in range(0, len(S)):\n",
        "        for t_index in range(0, len(T)):\n",
        "            count = count + 1\n",
        "            if S.iat[s_index, s_pos] == T.iat[t_index, t_pos]:\n",
        "              ret.append(S.iloc[s_index].to_dict() | T.iloc[t_index].to_dict())\n",
        "\n",
        "    print('Merge compared %d tuples'%count)\n",
        "    return pd.DataFrame(ret)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jb7rckPbBJx3"
      },
      "source": [
        "%%time\n",
        "# Here's a test join, with people and their experiences.  We can see how many\n",
        "# comparisons are made\n",
        "\n",
        "merge(people_df2, experience_df2, '_id', '_id')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9kKmlIcBJx4"
      },
      "source": [
        "# Let's find all people (by ID) who have Marketing as a skill\n",
        "\n",
        "mbio_df = skills_df2[skills_df2['skills'] == 'Molecular Biology'].reset_index()[['_id']]\n",
        "mbio_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5TFuDW3BJx7"
      },
      "source": [
        "%%time\n",
        "merge(merge(people_df2, experience_df2, '_id', '_id'), mbio_df, '_id', '_id')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NsBlaGPBJx8"
      },
      "source": [
        "%%time\n",
        "merge(merge(people_df2, mbio_df, '_id', '_id'), experience_df2, '_id', '_id')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcVvt7GLBJx_"
      },
      "source": [
        "%%time\n",
        "\n",
        "conn.sql(\"\"\"select distinct s._id,s.skills from people p join skills s on p._id=s._id join\n",
        "                  experience ex on s._id=ex._id and s.skills='Molecular Biology'\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "conn.sql(\"\"\"select distinct s._id,s.skills from skills s join\n",
        "                  experience ex on s._id=ex._id join people p on p._id=s._id where s.skills='Molecular Biology'\"\"\")"
      ],
      "metadata": {
        "id": "VlrvqEatDHG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conn.sql(\"select count(distinct _id) from skills where skills='Molecular Biology'\")"
      ],
      "metadata": {
        "id": "qOclu3AfCtQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AudktkCTBJyA"
      },
      "source": [
        "# Join using a *hash map*\n",
        "# from keys to (single) values\n",
        "def merge_map(S,T,l_on,r_on):\n",
        "    ret = []\n",
        "    T_map = {}\n",
        "    count = 0\n",
        "    # Take each value in the r_on field, and\n",
        "    # make a map entry for it\n",
        "    t_pos = T.columns.get_loc(r_on)\n",
        "    for t_index in range(0, len(T)):\n",
        "        # Make sure we aren't overwriting an entry!\n",
        "        if (T.iat[t_index,t_pos] not in T_map):\n",
        "          T_map[T.iat[t_index,t_pos]] = [T.loc[t_index]]\n",
        "        else:\n",
        "          T_map[T.iat[t_index,t_pos]].append(T.loc[t_index])\n",
        "        count = count + 1\n",
        "\n",
        "    # Now find matches\n",
        "    S2 = S.reset_index().drop(columns=['index'])\n",
        "    for s_index in range(0, len(S2)):\n",
        "        count = count + 1\n",
        "        if S2.loc[s_index, l_on] in T_map:\n",
        "          for item in T_map[S2.loc[s_index, l_on]]:\n",
        "            ret.append(S2.loc[s_index].to_dict() | item.drop(labels=r_on).to_dict())\n",
        "\n",
        "    print('Merge compared %d tuples'%count)\n",
        "    return pd.DataFrame(ret)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPpJWiUkBJyB"
      },
      "source": [
        "%%time\n",
        "\n",
        "# Here's a test join, with people and their experiences.  We can see how many\n",
        "# comparisons are made\n",
        "merge_map(experience_df2, people_df2, '_id', '_id')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise"
      ],
      "metadata": {
        "id": "xV60GWdffxmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile notebook-config.yaml\n",
        "\n",
        "grader_api_url: 'https://23whrwph9h.execute-api.us-east-1.amazonaws.com/default/Grader23'\n",
        "grader_api_key: 'flfkE736fA6Z8GxMDJe2q8Kfk8UDqjsG3GVqOFOa'"
      ],
      "metadata": {
        "id": "JXIfKikIf5DV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install penngrader-client"
      ],
      "metadata": {
        "id": "bB_k4Mhhf63P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW WHO\n",
        "#TO ASSIGN POINTS TO YOU IN OUR BACKEND\n",
        "STUDENT_ID = 99999999 # YOUR PENN-ID GOES HERE AS AN INTEGER##PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW WHO"
      ],
      "metadata": {
        "id": "GNrmFa8Nf8lX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%set_env HW_ID=cis5450_25f_HW9"
      ],
      "metadata": {
        "id": "tqCE2bPwf_4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from penngrader.grader import *\n",
        "\n",
        "grader = PennGrader('notebook-config.yaml', os.environ['HW_ID'], STUDENT_ID, STUDENT_ID)"
      ],
      "metadata": {
        "id": "C83mtjk7gBkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take the following query and use the `merge` or `merge_map` functions to execute it.  You can use Pandas to pre-apply or post-apply any filter conditions (selections) on dataframes.\n",
        "\n",
        "```\n",
        "SELECT _id, industry, skills\n",
        "FROM people_df2 p JOIN skills_df2 s ON p._id = s._id\n",
        "WHERE industry = 'Pharmaceuticals'\n",
        "```"
      ],
      "metadata": {
        "id": "g_bFRnnkfKW8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jxr020UZBJyH"
      },
      "source": [
        "# TODO: compute results_df as per the above\n",
        "results_df = # TODO\n",
        "\n",
        "results_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grader.grade('pharma', results_df)"
      ],
      "metadata": {
        "id": "71ke0HOxgID6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k9qH5JT-g3Gp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}