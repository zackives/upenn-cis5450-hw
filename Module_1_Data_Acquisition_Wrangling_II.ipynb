{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zackives/upenn-cis5450-hw/blob/main/Module_1_Data_Acquisition_Wrangling_II.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_jsTACdAaZe"
      },
      "source": [
        "# Part I of Big Data Analytics - Notebook 2\n",
        "\n",
        "## Recall\n",
        "\n",
        "As we start our journey into Big Data Analytics, the first thing we need to do is **get the data** in the form we need for analysis!  We'll start with an overview of how to acquire and *wrangle* data.\n",
        "\n",
        "This notebook will be built incrementally to consider several tasks:\n",
        "\n",
        "* Acquiring data from files and remote sources\n",
        "* Information extraction over HTML content\n",
        "* A basic \"vocabulary\" of operators over tables (the relational algebra)\n",
        "* Basic manipulation using SQL in DuckDB\n",
        "\n",
        "* \"Data wrangling\" or integration:\n",
        "  * Cleaning and filtering data, using rules and based operations\n",
        "  * Linking data across dataframes or relations\n",
        "  * The need for approximate match and record linking\n",
        "  * Different techniques\n",
        "\n",
        "## Before you start this notebook\n",
        "\n",
        "Make sure you did the exercises in [Notebook 1](https://colab.research.google.com/github/zackives/cis5450-hw/blob/main/Module_1_Data_Acquisition.ipynb)!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N01ql5zclw7M"
      },
      "source": [
        "## The Motivating Question\n",
        "To illustrate the principles, we focus on the question of **how old company CEOs and founders** (in general, leaders) are.  The question was in part motivated by the following New York Times article:\n",
        "\n",
        "* Founders of Successful Tech Companies Are Mostly Middle-Aged: https://www.nytimes.com/2019/08/29/business/tech-start-up-founders-nest.html?searchResultPosition=2\n",
        "\n",
        "So let's test this hypothesis!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial Libraries\n",
        "\n",
        "We'll be using [DuckDB](https://duckdb.org/) as a means of managing our tables.  DuckDB works like a Python library, but manages a full SQL database (in files).  It also integrates very nicely with Pandas, so we'll use it in this course."
      ],
      "metadata": {
        "id": "v7P39UsKpdoK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dy4ltbGCt6MZ"
      },
      "outputs": [],
      "source": [
        "!pip3 install duckdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YT1J84O3wCju"
      },
      "outputs": [],
      "source": [
        "!pip3 install lxml"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile notebook-config.yaml\n",
        "\n",
        "grader_api_url: 'https://23whrwph9h.execute-api.us-east-1.amazonaws.com/default/Grader23'\n",
        "grader_api_key: 'flfkE736fA6Z8GxMDJe2q8Kfk8UDqjsG3GVqOFOa'"
      ],
      "metadata": {
        "id": "9TZsWfgY41De"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install penngrader-client"
      ],
      "metadata": {
        "id": "hZY7k8Z14lWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For quiz credit you'll need to update your student ID here!"
      ],
      "metadata": {
        "id": "HkENA-6r4vYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW WHO\n",
        "#TO ASSIGN POINTS TO YOU IN OUR BACKEND\n",
        "STUDENT_ID = 99999999 # YOUR PENN-ID GOES HERE AS AN INTEGER##PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW WHO"
      ],
      "metadata": {
        "id": "L3_sSMr84uCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quizzes will cumulatively count as HW9... Don't edit this..."
      ],
      "metadata": {
        "id": "oGAFhhVF5LZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%set_env HW_ID=cis5450_25f_HW9"
      ],
      "metadata": {
        "id": "93y6gp6l5DJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from penngrader.grader import *\n",
        "\n",
        "grader = PennGrader('notebook-config.yaml', os.environ['HW_ID'], STUDENT_ID, STUDENT_ID)"
      ],
      "metadata": {
        "id": "zt58w5q_43eQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Xah24fnRRGZ"
      },
      "outputs": [],
      "source": [
        "# Imports we'll use through the notebook, collected here for simplicity\n",
        "\n",
        "# For parsing dates and being able to compare\n",
        "import datetime\n",
        "\n",
        "# For fetching remote data\n",
        "import urllib\n",
        "import urllib.request\n",
        "\n",
        "# Pandas dataframes and operations\n",
        "import pandas as pd\n",
        "\n",
        "# Numpy matrix and array operations\n",
        "import numpy as np\n",
        "\n",
        "# Sqlite is a simplistic database\n",
        "import duckdb\n",
        "\n",
        "# Data visualization\n",
        "import matplotlib\n",
        "\n",
        "from lxml import etree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWOeu1wHZDqz"
      },
      "source": [
        "# 1. Reload from HW1\n",
        "\n",
        "Here is a quick reload of the data from Homework 1.\n",
        "\n",
        "For simplicity, we reload all of the following:\n",
        "* `company_ceos_df` -- Wikipedia list of companies and their CEOs\n",
        "* `exec_df` -- crawled executive names and birthdays\n",
        "* `company_data_df` -- 7M entries about companies\n",
        "* `company_info_df` -- CSV about companies and lines of business"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -nc https://storage.googleapis.com/penn-cis5450/companies_sorted.csv"
      ],
      "metadata": {
        "id": "AOoXr_uamjnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a helper for importing HTML"
      ],
      "metadata": {
        "id": "GbwY0jyGpfo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from io import StringIO\n",
        "\n",
        "headers = {\n",
        "  'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "}\n",
        "\n",
        "def import_html(url):\n",
        "  # Let's read an HTML table!\n",
        "  headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "  }\n",
        "  page = requests.get(url, headers=headers).text\n",
        "  return page\n"
      ],
      "metadata": {
        "id": "7uEfhvg-pcH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1P7Ry8RPXFRp"
      },
      "outputs": [],
      "source": [
        "from io import StringIO\n",
        "\n",
        "company_ceos_df = pd.read_html(StringIO(import_html('https://en.wikipedia.org/wiki/List_of_chief_executive_officers#List_of_CEOs')))[1]\n",
        "\n",
        "pages = []\n",
        "page_urls = []\n",
        "\n",
        "for i, executive in enumerate(company_ceos_df['Executive']):\n",
        "  url = 'https://en.wikipedia.org/wiki/' + executive.replace(' ', '_')\n",
        "\n",
        "  page = url.split(\"/\")[-1] #extract the person name at the end of the url\n",
        "\n",
        "  # An issue: some of the accent characters won't work.  We need to convert them\n",
        "  # into an HTML URL.  We'll split the URL, then use \"parse.quote\" to change\n",
        "  # the structure, then re-form the URL\n",
        "  url_list = list(urllib.parse.urlsplit(url))\n",
        "  url_list[2] = urllib.parse.quote(url_list[2])\n",
        "  url_ascii = urllib.parse.urlunsplit(url_list)\n",
        "  try:\n",
        "    response = import_html(url_ascii)\n",
        "    pages.append(response)\n",
        "    page_urls.append(url)\n",
        "  except requests.exceptions.RequestException as e:\n",
        "    print(e)\n",
        "\n",
        "\n",
        "# Use lxml.etree.HTML(...) on the HTML content of each page to get a DOM tree that\n",
        "# can be processed via XPath to extract the bday information.  Store the CEO name,\n",
        "# webpage, and the birthdate (born) in exec_df.\n",
        "\n",
        "# We first check that the HTML content has a table of type `vcard`,\n",
        "# and then extract the `bday` information.  If there is no birthdate, the datetime\n",
        "# value is NaT (not a type).\n",
        "\n",
        "rows = []\n",
        "for i, page in enumerate(pages):\n",
        "  url = page_urls[i]\n",
        "\n",
        "  tree = etree.HTML(page)  #create a DOM tree of the page\n",
        "  bday = tree.xpath('//table[contains(@class,\"vcard\")]//span[@class=\"bday\"]/text()')\n",
        "  if len(bday) > 0:\n",
        "      name = url[url.rfind('/')+1:] # The part of the URL after the last /\n",
        "      rows.append({'name': name, 'page': url,\n",
        "                  'born': datetime.datetime.strptime(bday[0], '%Y-%m-%d')})\n",
        "  else:\n",
        "          rows.append({'name': url[url.rfind('/')+1:], 'page': url\n",
        "                                    , 'born': np.datetime64('NaT')})\n",
        "\n",
        "exec_df = pd.DataFrame(rows)\n",
        "exec_df['clean_name'] = exec_df['name'].apply(lambda x: x.replace('_', ' '))\n",
        "company_data_df = pd.read_csv('companies_sorted.csv')\n",
        "countries_df = pd.read_csv(\"https://raw.githubusercontent.com/lukes/ISO-3166-Countries-with-Regional-Codes/master/all/all.csv\")\n",
        "\n",
        "data = urllib.request.urlopen(\\\n",
        "       'https://gist.github.com/jvilledieu/c3afe5bc21da28880a30/raw/a344034b82a11433ba6f149afa47e57567d4a18f/Companies.csv')\n",
        "\n",
        "company_info_df = pd.read_csv(data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's use DuckDB to allow working with on-disk and in memory data."
      ],
      "metadata": {
        "id": "NQAxbHoWqFQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "con = duckdb.connect('local.db')\n",
        "con.sql(\"\"\"CREATE TABLE IF NOT EXISTS company_data AS\n",
        "           SELECT *\n",
        "           FROM company_data_df\"\"\")\n",
        "con.sql(\"\"\"create table if not exists company_ceos as select * from company_ceos_df\"\"\")\n",
        "con.sql(\"\"\"create table if not exists executives as select * from exec_df\"\"\")\n",
        "\n",
        "# query the table\n",
        "con.table(\"company_data\").show()"
      ],
      "metadata": {
        "id": "JA3y1E0WYuit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laSVfrfHZDrC"
      },
      "source": [
        "# 2.0 Data Transformation and Querying\n",
        "\n",
        "Looking at our data to clean via *projection*...\n",
        "\n",
        "Generally, we can extract one \"narrower\" table form another by using **double brackets**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rZQSnnMZDrC"
      },
      "outputs": [],
      "source": [
        "# Let's take a look at the data.  Here's a way of PROJECTING the exec_df dataframe into\n",
        "# a smaller table\n",
        "\n",
        "exec_df[['name', 'born']]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In SQL it's SELECT with the fields FROM the table\n",
        "con.sql('select name, born from executives')"
      ],
      "metadata": {
        "id": "-7KO_iStrRVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAY737VoMnN1"
      },
      "outputs": [],
      "source": [
        "# If I use single brackets, I can extract a single column as a Series.\n",
        "exec_df['name']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can use SQL over the dataframe OR here\n",
        "duckdb.sql('SELECT name FROM exec_df')"
      ],
      "metadata": {
        "id": "W4PHQxamrfZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDrhz4vqZDrE"
      },
      "outputs": [],
      "source": [
        "# Notice anything awry?\n",
        "\n",
        "for person in exec_df['name']:\n",
        "    print (person)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cg3ywxVnNvxa"
      },
      "outputs": [],
      "source": [
        "def to_space(x):\n",
        "  return x.replace('_', ' ')\n",
        "\n",
        "# Let's use *apply* to call a function over each element, returning a new Series\n",
        "exec_df['name'].apply(to_space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5axXYticOpZi"
      },
      "outputs": [],
      "source": [
        "# Let's use *apply* to call a function over each element, returning a new Series\n",
        "exec_df['name'].apply(lambda x: x.replace('_', ' '))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtUoKeWjN4RG"
      },
      "outputs": [],
      "source": [
        "# I can also use *apply* to call a function over the rows of a dataframe\n",
        "exec_df.apply(lambda x: x['name'].replace('_', ' '), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LiWISL5DZDrG"
      },
      "outputs": [],
      "source": [
        "# Let's clean the name by removing underscores...\n",
        "exec_df['clean_name'] = exec_df['name'].apply(lambda x: x.replace('_', ' '))\n",
        "\n",
        "exec_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnjD3tMgWgE9"
      },
      "outputs": [],
      "source": [
        "exec_df.rename(columns={'name': 'old_name'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSl6EtTIZDrH"
      },
      "outputs": [],
      "source": [
        "# We can do the same via SQL.\n",
        "\n",
        "duckdb.sql(\"\"\"SELECT name, replace(name, '_', ' ') AS clean_name\n",
        "              FROM exec_df\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7hgQyB0aV7_"
      },
      "source": [
        "## 2.1. Selecting a subset of the rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmdeXLmkTVKv"
      },
      "outputs": [],
      "source": [
        "# Here's a column\n",
        "\n",
        "exec_df['clean_name']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dorz_n5QTubF"
      },
      "outputs": [],
      "source": [
        "# We can apply a test (predicate) to each column, returning a Series of boolean true/false values\n",
        "\n",
        "exec_df['clean_name'] == 'Julie Sweet'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUKLfohVUEUp"
      },
      "outputs": [],
      "source": [
        "# If we compose these, we'll get only those rows where the boolean condition was True\n",
        "\n",
        "exec_df[exec_df['clean_name'] == 'Julie Sweet']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SQL lets us use any case, but convention is to capitalize the SQL keywords such as `SELECT`, `FROM`, `WHERE` to aid in readability.  Also, we should use single-quotes for SQL strings, so we'll typically pass the SQL command in with double-quotes."
      ],
      "metadata": {
        "id": "ni-Ap7qaLUUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "duckdb.sql(\"SELECT * FROM exec_df WHERE clean_name='Julie Sweet'\")"
      ],
      "metadata": {
        "id": "DvGAml-asto8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKGCNE7-6Qm8"
      },
      "outputs": [],
      "source": [
        "exec_df[exec_df['clean_name'] == 'Julie Sweet'][['page']]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Here we'll use the triple-quote syntax for Python strings, which allows us to pass a multi-line string to SQL...\n"
      ],
      "metadata": {
        "id": "A_nqpoIfLSgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "duckdb.sql(\"\"\"SELECT clean_name\n",
        "            FROM exec_df\n",
        "            WHERE clean_name='Julie Sweet'\"\"\")"
      ],
      "metadata": {
        "id": "rKnl3EGns5sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_u-rpXFQ8-1o"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "exec_df.dropna(subset=['born'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKFBClXADPxo"
      },
      "outputs": [],
      "source": [
        "duckdb.sql(\"\"\"SELECT *\n",
        "            FROM exec_df\n",
        "            WHERE born IS NOT NULL\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRs62sJHeOq1"
      },
      "source": [
        "## 2.2. Joining Data\n",
        "\n",
        "We start with a simple join between company_ceos_df and exec_df and persist it to the database.  We then check how many companies did not have a match on CEO name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wU7fznW28nhI"
      },
      "outputs": [],
      "source": [
        "exec_df[['clean_name', 'born']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ox8A8ucqTBcY"
      },
      "outputs": [],
      "source": [
        "# Remove any duplicate executive entries\n",
        "\n",
        "exec_df = exec_df.drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rT-IUtwu8qtR"
      },
      "outputs": [],
      "source": [
        "company_ceos_df[['Executive', 'Company']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwz5j4rVeOCN"
      },
      "outputs": [],
      "source": [
        "company_ceos_df[['Executive', 'Company']].merge(exec_df[['clean_name', 'born']],\n",
        "                                                left_on=['Executive'],\n",
        "                                                right_on=['clean_name'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can `JOIN ON` in the `FROM` clause."
      ],
      "metadata": {
        "id": "ZB-oyhzGLvmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "duckdb.sql(\"\"\"\n",
        "            SELECT Executive, Company, born\n",
        "            FROM company_ceos_df JOIN exec_df ON Executive=clean_name\n",
        "          \"\"\")"
      ],
      "metadata": {
        "id": "HpRTkYQqtLu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note there is another way you'll sometimes see, in older versions of SQL... Which is to put the join as a `WHERE` condition:"
      ],
      "metadata": {
        "id": "vfq29ztPL1FS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "duckdb.sql(\"\"\"\n",
        "            SELECT Executive, Company, born\n",
        "            FROM company_ceos_df, exec_df\n",
        "            WHERE Executive=clean_name\n",
        "          \"\"\")"
      ],
      "metadata": {
        "id": "r7lOgXgFL7CO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OK, let's drop the cases where we don't have a CEO's birthday: these aren't useful!"
      ],
      "metadata": {
        "id": "d4IuAS4hMCW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shall we skip the cases where we don't have the birthday?\n",
        "duckdb.sql(\"\"\"\n",
        "            SELECT Executive, Company, born\n",
        "            FROM company_ceos_df JOIN exec_df ON Executive=clean_name\n",
        "            WHERE born is not null\n",
        "          \"\"\")"
      ],
      "metadata": {
        "id": "szqv_prrtkYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "company_ceos_df[['Executive', 'Company']]"
      ],
      "metadata": {
        "id": "aYMDM3PtZ17f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exec_df[['clean_name', 'born']]"
      ],
      "metadata": {
        "id": "LXlq2hmdZ238"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJJJJfZ9UFsu"
      },
      "source": [
        "## 2.4. Finding the misses in the join with OUTER JOINs.\n",
        "\n",
        "Note that the join above resulted in 174 rows.  However, there are more rows in company_ceos_df so we are missing some companies.  We can see which are missed using a LEFT OUTERJOIN (aka LEFT JOIN); setting \"indicator= True\" allows us to see which tuples in company_ceos_df failed to find a match (left_only, e.g. row 24 and 172)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6w6FtOBKjV5N"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_rows', 200)\n",
        "display(company_ceos_df[['Executive', 'Company']].merge(exec_df[['clean_name', 'born']],\n",
        "                                                left_on=['Executive'],\n",
        "                                                right_on=['clean_name'], how=\"left\", indicator=True))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dET0V4epJ8nI"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_rows', 50)\n",
        "result_df = company_ceos_df[['Executive', 'Company']].merge(exec_df[['clean_name', 'born']],\n",
        "                                                left_on=['Executive'],\n",
        "                                                right_on=['clean_name'], how=\"outer\", indicator=True)\n",
        "\n",
        "result_df[result_df['_merge'] != 'both']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also do this in SQL (there is no indicator but we can test for NULL):"
      ],
      "metadata": {
        "id": "JlvAd3UuMSi2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "duckdb.sql(\"\"\"\n",
        "            SELECT Executive, Company, clean_name, born\n",
        "            FROM company_ceos_df FULL JOIN exec_df ON Executive=clean_name\n",
        "            WHERE clean_name is null or Company is null\n",
        "          \"\"\")"
      ],
      "metadata": {
        "id": "hGc6Gv3AtzEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Yc7zzRmGfKI"
      },
      "source": [
        "## 2.3. Composing Joins\n",
        "\n",
        "Of course, we can join the results of a join with another table -- representing a *composition*!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's join with company data!"
      ],
      "metadata": {
        "id": "t3UzuuBFwHDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "duckdb.sql(\"\"\"SELECT Executive, Company, born\n",
        "            FROM company_ceos_df\n",
        "            JOIN exec_df ON Executive=replace(name, '_', ' ')\n",
        "            JOIN company_data_df cd ON Company=cd.name\n",
        "            WHERE born is not null\"\"\")"
      ],
      "metadata": {
        "id": "8C48kbphwEGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hmm, what is wrong here?\n",
        "\n",
        "Let's switch to the tables saved in DuckDB for these."
      ],
      "metadata": {
        "id": "2HYhm5arwtT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "con.sql('SELECT * from company_data')"
      ],
      "metadata": {
        "id": "WSnLIBf-wu8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice the case for `name`?"
      ],
      "metadata": {
        "id": "rzztqGxow0ls"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qrYniMCFE89"
      },
      "outputs": [],
      "source": [
        "con.sql(\"\"\"SELECT Executive, Company, born\n",
        "            FROM company_ceos\n",
        "            JOIN executives ON Executive=replace(name, '_', ' ')\n",
        "            JOIN company_data cd ON lower(Company)=lower(cd.name)\n",
        "            WHERE born is not null\n",
        "            ORDER BY Company\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hmm, there are duplicates!  This is because of fields in the `company_data` table that we don't care about. We can remove the duplicates via `SELECT DISTINCT`."
      ],
      "metadata": {
        "id": "iRfcYEfuw-eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "con.sql(\"\"\"SELECT DISTINCT Executive, Company, born\n",
        "            FROM company_ceos\n",
        "            JOIN executives ON Executive=replace(name, '_', ' ')\n",
        "            JOIN company_data cd ON lower(Company)=lower(cd.name)\n",
        "            WHERE born is not null\n",
        "            ORDER BY Company\"\"\")"
      ],
      "metadata": {
        "id": "UtBX_ZElxAGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can we do all of this in Pandas? Of course!\n",
        "\n",
        "First, we need to lowercase the company names."
      ],
      "metadata": {
        "id": "rKOQAcT3MirQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "company_ceos_df['company_lc'] = company_ceos_df['Company'].apply(lambda x: x.lower())"
      ],
      "metadata": {
        "id": "ELazjTKRxJZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice this is slower than DuckDB?"
      ],
      "metadata": {
        "id": "iWUzk-DOyM_3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84mXJ0beHdqo"
      },
      "outputs": [],
      "source": [
        "company_ceos_df.merge(exec_df.dropna(),\n",
        "                      left_on=['Executive'],\n",
        "                      right_on=['clean_name']).\\\n",
        "                      merge(company_data_df,\n",
        "                            left_on='company_lc',\n",
        "                            right_on='name')[['Executive','Company','born']].drop_duplicates().sort_values('Company')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2 Exercises"
      ],
      "metadata": {
        "id": "iT4KKkAzMp3g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hmm, there seem to be quite a few Roberts and Bobs. (Maybe also Roberta?)\n",
        "\n",
        "Using either Pandas operations like `merge` and the various bracket notations (but **not** the `loc`/`iloc` commands if you know these): write a query that takes the `company_ceos_df` and returns all companies that are overseen by someone with names `Bob`, `Robert`, or `Roberta`."
      ],
      "metadata": {
        "id": "kETWufcGNlHt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, define a function that *robustly* does this:"
      ],
      "metadata": {
        "id": "FdsuEstNOINW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bob(name: str) -> bool:\n",
        "  first_name = name # TODO: change this to get first name!\n",
        "\n",
        "  return False # TODO: change this!"
      ],
      "metadata": {
        "id": "no37-a8OOKNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dill.source import getsource\n",
        "grader.grade('bob_test', getsource(bob))"
      ],
      "metadata": {
        "id": "furAfRzsO7zG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now *select a subset of the rows* that have Bob/etc. using the function `bob` and the `apply` function."
      ],
      "metadata": {
        "id": "wluCOUaVbkvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bob_co_df = None# TODO\n",
        "bob_co_df"
      ],
      "metadata": {
        "id": "_2_PgPDcMsBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grader.grade(test_case_id='bobs_your_uncle_or_aunt', answer=bob_co_df)"
      ],
      "metadata": {
        "id": "1YbrQxhuPjFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NtmWgUFVg-yD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "dataset_inspector": {
      "cols": {
        "lenName": 24,
        "lenType": 16,
        "lenVar": 40
      },
      "dataset_inspector_section_display": "none",
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "oldHeight": 88.66133400000001,
      "position": {
        "height": "40px",
        "left": "0px",
        "right": "20px",
        "top": "0px",
        "width": "800px"
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}