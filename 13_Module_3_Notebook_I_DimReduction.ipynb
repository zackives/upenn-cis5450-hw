{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zackives/upenn-cis5450-hw/blob/main/13_Module_3_Notebook_I_DimReduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statistical Tests\n",
        "\n",
        "We saw in the previous notebook how the t-test works. Here let's look at comparing distributions of two categorical attributes."
      ],
      "metadata": {
        "id": "KILOEuwYF6wW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ztsy-EhwGBsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "voters_df = pd.DataFrame([{\"gender\": \"male\", \"republican\": 120, \"democrat\": 90, \"independent\": 40}, \\\n",
        "                          {\"gender\": \"female\", \"republican\": 110, \"democrat\": 95, \"independent\": 45}])\n",
        "\n",
        "voters_df.set_index(\"gender\", inplace=True)\n",
        "\n",
        "voters_df\n"
      ],
      "metadata": {
        "id": "jVAiBmsBGiHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "\n",
        "# The \"contingency table\" shows\n",
        "result = stats.chi2_contingency(voters_df)\n",
        "\n",
        "print(f\"Statistic: {result[0]}\")\n",
        "print(f\"p-value: {result[1]}\")\n",
        "\n",
        "if result[1] < 0.05:\n",
        "  print(\"Statistically significant\")\n",
        "else:\n",
        "  print(\"Not statistically significant\")"
      ],
      "metadata": {
        "id": "oMN-KReAGmCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cy6hpgnJY-Rp"
      },
      "source": [
        "# Reducing Dimensionality\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autograder setup"
      ],
      "metadata": {
        "id": "E7pUp7HAwQp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW WHO\n",
        "#TO ASSIGN POINTS TO YOU IN OUR BACKEND\n",
        "STUDENT_ID = 99999999 # YOUR PENN-ID GOES HERE AS AN INTEGER##PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW WHO"
      ],
      "metadata": {
        "id": "WGM-vv4owbBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile notebook-config.yaml\n",
        "\n",
        "grader_api_url: 'https://23whrwph9h.execute-api.us-east-1.amazonaws.com/default/Grader23'\n",
        "grader_api_key: 'flfkE736fA6Z8GxMDJe2q8Kfk8UDqjsG3GVqOFOa'"
      ],
      "metadata": {
        "id": "OxKJ5J_awSOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%set_env HW_ID=cis5450_25f_HW9"
      ],
      "metadata": {
        "id": "RfX8o5jjwe0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install penngrader-client"
      ],
      "metadata": {
        "id": "S3n0LqhuwW2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from penngrader.grader import *\n",
        "\n",
        "grader = PennGrader('notebook-config.yaml', os.environ['HW_ID'], STUDENT_ID, STUDENT_ID)"
      ],
      "metadata": {
        "id": "dEHXUnRqwcP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Looking at Glass"
      ],
      "metadata": {
        "id": "obNjyVNtCzvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Glass data from https://archive.ics.uci.edu/ml/machine-learning-databases/glass/"
      ],
      "metadata": {
        "id": "Qr_7sk7SC1yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://archive.ics.uci.edu/static/public/42/glass+identification.zip"
      ],
      "metadata": {
        "id": "ZUoaLM-FDHAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip glass+identification.zip"
      ],
      "metadata": {
        "id": "tBHrw9Z1EAjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load into a dataframe, with the header in row 0\n",
        "import pandas as pd\n",
        "\n",
        "glass_df = pd.read_csv('glass.data',header=None,names=['ID','RefractiveIndex','Na','Mg','Al','Si','K','Ca','Ba','Fe','Label'])\n",
        "\n",
        "glass_df"
      ],
      "metadata": {
        "id": "dLvTxx84DJ4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploratory Data Analysis\n",
        "\n",
        "Let's do some \"EDA\" - exploratory data analysis.  Typically that involves getting a sense of the fields, distirbutions, missing values, correlations, and more."
      ],
      "metadata": {
        "id": "4F7lbYUCDPv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "glass_df.info()"
      ],
      "metadata": {
        "id": "MzV2Udz3DUNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What are the data distributions within each column?\n",
        "glass_df.describe()"
      ],
      "metadata": {
        "id": "qitJJs24DS6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observe the really wide differences between means, value ranges, and more across the different elements and the refractive index."
      ],
      "metadata": {
        "id": "pA7I_s6pEzGh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Any missing values?"
      ],
      "metadata": {
        "id": "DakUaySzE5Yn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "glass_df.isnull().sum()"
      ],
      "metadata": {
        "id": "XLhugzNgDVY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No -- that's good!\n",
        "\n",
        "Now let's look at the value distributions relative to each other..."
      ],
      "metadata": {
        "id": "SlKuXQbbE6vD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "glass_df.set_index('ID')\n",
        "glass_types_df = glass_df[['Label']]\n",
        "\n",
        "# We don't really need these\n",
        "glass_df = glass_df.drop(columns=['ID', 'Label'])\n",
        "\n",
        "display(glass_df)\n",
        "display(glass_types_df)"
      ],
      "metadata": {
        "id": "zbNgDiFuFIE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see the popularity of each label...\n",
        "\n",
        "glass_types_df.value_counts().plot(kind='bar')"
      ],
      "metadata": {
        "id": "FL9IXyYqFiIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observe that some values, e.g., 4, don't have any instances."
      ],
      "metadata": {
        "id": "5mr7EJHqF86V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A Pairplot of every item vs every other item\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.pairplot(glass_df)"
      ],
      "metadata": {
        "id": "INjbGIq5DLm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr_matrix = glass_df.corr()\n",
        "\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')"
      ],
      "metadata": {
        "id": "WB0Qf0nSDfp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observe that some of the features are fairly (anti-)correlated, e.g., Ba and Mg."
      ],
      "metadata": {
        "id": "r26BGe9-HVVK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unsupervised Machine Learning!\n",
        "\n",
        "We typically set up machine learning problems as follows.\n",
        "\n",
        "1. Convert from categorical and other values into numeric values\n",
        "2. Convert from dataframes to arrays\n",
        "3. Separate out any classes / labels (like glass type)\n",
        "\n",
        "We will call the *input data*  $X$ and the *labels* $y$."
      ],
      "metadata": {
        "id": "DZ778RCnHsku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the problem\n",
        "\n",
        "X = glass_df.to_numpy()\n",
        "y = glass_types_df.to_numpy()"
      ],
      "metadata": {
        "id": "u-iBcIhnHonu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "PR9nE_w7H9wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# This is the covariance matrix\n",
        "np.cov(X)"
      ],
      "metadata": {
        "id": "pmqTTRSpIGO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at a couple of dimensions within the data..."
      ],
      "metadata": {
        "id": "FlXYYlVRIavg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig1 = glass_df.plot.scatter(x='RefractiveIndex',y='Na')"
      ],
      "metadata": {
        "id": "5iuxMLIpIdBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applying PCA\n",
        "\n",
        "Let's first scale the data (by removing the mean and scaling by unit variance)."
      ],
      "metadata": {
        "id": "5YphncL7IiFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Standardizing the features based on unit variance\n",
        "X = StandardScaler().fit_transform(X)\n",
        "\n",
        "print (X.shape)\n",
        "print(X)"
      ],
      "metadata": {
        "id": "gLe3xwdzItXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll re-plot our two dimensions"
      ],
      "metadata": {
        "id": "JaDnawEwIxAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-plotting now with the mean at the center!\n",
        "plt.scatter(X[:,0], X[:,1])"
      ],
      "metadata": {
        "id": "WtlH5PPwIyJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actually Running PCA\n",
        "\n",
        "We'll use, for the first time, a standard sckikit-learn 'flow': create a model, `fit` it, and `transform` the data."
      ],
      "metadata": {
        "id": "4lnupjGiI1ZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA()\n",
        "X2 = pca.fit_transform(X)"
      ],
      "metadata": {
        "id": "6_vgEtYoI_8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see the components. There are p of them, each with p elements\n",
        "pca.components_"
      ],
      "metadata": {
        "id": "qsdjUYqVJCaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`X2` is the transformed matrix, in a new subspace -- we called this `X'`.  `pca.components_` is what we called `W`."
      ],
      "metadata": {
        "id": "M_sYNfZyJFfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print (X2.shape)\n",
        "print (X2)"
      ],
      "metadata": {
        "id": "eqFhHAOONI23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get this, we can directly compute the original data times the transformation matrix (transposed)."
      ],
      "metadata": {
        "id": "I3HGUCW_fkn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X @ pca.components_.T"
      ],
      "metadata": {
        "id": "wJFZmA0yNEGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, we can invert the process..."
      ],
      "metadata": {
        "id": "GsLPu8IMf02f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca.inverse_transform(X2)"
      ],
      "metadata": {
        "id": "sstwvOrIf3iV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X2 @ pca.components_"
      ],
      "metadata": {
        "id": "kaWvFU5HUIzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "3ECeJAV-Jki7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing the first 2 components (eigenvectors)"
      ],
      "metadata": {
        "id": "cDQXKeRTgb8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization code based on\n",
        "# https://stackoverflow.com/questions/18299523/basic-example-for-pca-with-matplotlib\n",
        "import numpy as np\n",
        "\n",
        "# Let's take our first two dimensions, as before\n",
        "data = X[:, 0:2]\n",
        "\n",
        "mu = data.mean(axis=0)\n",
        "data = (data - mu)/data.std(axis=0)\n",
        "eigenvectors, eigenvalues, V = np.linalg.svd(data.T, full_matrices=False)\n",
        "projected_data = np.dot(data, eigenvectors)\n",
        "sigma = projected_data.std(axis=0).mean()\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(X[:,0], X[:,1])\n",
        "for axis in eigenvectors:\n",
        "    start, end = mu, mu + sigma * axis\n",
        "    ax.annotate(\n",
        "        '', xy=end, xycoords='data',\n",
        "        xytext=start, textcoords='data',\n",
        "        arrowprops=dict(facecolor='red', width=2.0))\n",
        "ax.set_aspect('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eNt5KiMoghdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's re-plot by rotating along the first two dimensions"
      ],
      "metadata": {
        "id": "lxiFmRMzglE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here is the transformed data along the first 2 components\n",
        "plt.scatter(X2[:,0], X2[:,1])"
      ],
      "metadata": {
        "id": "pbN6DWZLgnte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How Many Components? Principal Components vs Explained Variance\n",
        "\n",
        "How much does each component explain the variance?  We can look at the `explained_variance_ratio_` to tell..."
      ],
      "metadata": {
        "id": "_62lj3UpgrJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.set_printoptions(suppress=True)\n",
        "pca.explained_variance_ratio_"
      ],
      "metadata": {
        "id": "Ohah4eAdgsoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See how much is contributed by the first few terms\n",
        "pc_vs_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "pc_vs_variance\n",
        "plt.plot(pc_vs_variance)"
      ],
      "metadata": {
        "id": "iwTQaxT6guoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "... So, the first 6 components (0 through 5, of 9) give 90% explained variance.  Not too bad!"
      ],
      "metadata": {
        "id": "jlgWoFL-gyqw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA and Learning a Predictor (Classifier)\n",
        "\n",
        "From the above, we saw how to do PCA on the overall dataset.  But let's do it more methodically as part of machine learning.  We'll start with separate training and test data.\n",
        "\n"
      ],
      "metadata": {
        "id": "1KXmsbrTg6Kw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import linear_model\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\\\n",
        "  X, y, test_size=0.20, random_state=42)\n",
        "\n",
        "# Fit the PCA on the training data\n",
        "pca = PCA(n_components=6)\n",
        "pca.fit(X_train)\n",
        "# and transform it\n",
        "X_train_2 = pca.transform(X_train)\n",
        "\n",
        "# Then train a simple linear regression classifier\n",
        "# (tries to find the best weighted linear combination to\n",
        "# match the output)\n",
        "regr = linear_model.LinearRegression()\n",
        "regr.fit(X_train_2, y_train)\n",
        "\n",
        "X_train_2"
      ],
      "metadata": {
        "id": "FrQs-jt6g3dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_2 = pca.transform(X_test)\n",
        "\n",
        "regr.predict(X_test_2)\n",
        "\n",
        "regr.score(X_test_2, y_test)"
      ],
      "metadata": {
        "id": "9MqeLdbNhHql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, 87.4% predictive accuracy on the test set.\n",
        "\n",
        "How does that compare with working directly on the real data?"
      ],
      "metadata": {
        "id": "v_jKf47whKZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate over non-dimensionality-reduced data\n",
        "regr_full_data = linear_model.LinearRegression()\n",
        "regr_full_data.fit(X_train, y_train)\n",
        "\n",
        "regr_full_data.predict(X_test)\n",
        "regr_full_data.score(X_test, y_test)"
      ],
      "metadata": {
        "id": "-pOvVZechLVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Actually better!  How can that be? We not only reduced dimensions, but we removed correlation. The `LinearRegression` classifier assumes uncorrelated features."
      ],
      "metadata": {
        "id": "TWsWRLRLhNS-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA on Spark\n",
        "\n",
        "Thus far we've seen PCA using Scikit-Learn, which is fantastic for mid-sized data sets.\n",
        "\n",
        "What if we have a really big Spark dataframe with our dataset?\n"
      ],
      "metadata": {
        "id": "D5kKxnOjhgUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%set_env SPARK_VERSION=3.5.6"
      ],
      "metadata": {
        "id": "KJlwZFXGVs9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Let's install Apache Spark on Colab\n",
        "\n",
        "!wget -nc https://downloads.apache.org/spark/spark-$SPARK_VERSION/spark-$SPARK_VERSION-bin-hadoop3.tgz\n",
        "!tar xf spark-$SPARK_VERSION-bin-hadoop3.tgz\n",
        "!pip install findspark\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-\" + os.environ['SPARK_VERSION'] + \"-bin-hadoop3\""
      ],
      "metadata": {
        "id": "iANMz5pqhu3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "spark = SparkSession.builder.appName('Clustering').getOrCreate()"
      ],
      "metadata": {
        "id": "DgwDwu_jVxgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pyspark import SparkFiles\n",
        "\n",
        "from pyspark.sql.types import StringType, IntegerType, DoubleType, StructField, StructType, ArrayType, MapType\n",
        "\n",
        "# ID,RefractiveIndex,Na,Mg,Al,Si,K,Ca,Ba,Fe,Type\n",
        "schema = StructType([\n",
        "        StructField(\"ID\", IntegerType(), True),\n",
        "        StructField(\"RefractiveIndex\", DoubleType(), True),\n",
        "        StructField(\"Na\", DoubleType(), True),\n",
        "        StructField(\"Mg\", DoubleType(), True),\n",
        "        StructField(\"Al\", DoubleType(), True),\n",
        "        StructField(\"Si\", DoubleType(), True),\n",
        "        StructField(\"K\", DoubleType(), True),\n",
        "        StructField(\"Ca\", DoubleType(), True),\n",
        "        StructField(\"Ba\", DoubleType(), True),\n",
        "        StructField(\"Fe\", DoubleType(), True),\n",
        "        StructField(\"Type\", IntegerType(), True),\n",
        "         ])\n",
        "\n",
        "glass_sdf = spark.createDataFrame(\\\n",
        "                                  pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data'), \\\n",
        "                                  schema=schema)\n",
        "\n",
        "glass_sdf.show(5)"
      ],
      "metadata": {
        "id": "-hWEvSsPhy-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From Spark, we need to compute  a matrix (specifically, a Row Matrix) for MLLib's linear algebra operators to work on.\n",
        "Then we can call `computePrincipalComponents`."
      ],
      "metadata": {
        "id": "Md117CaaioN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.mllib.linalg import Vectors\n",
        "from pyspark.mllib.linalg.distributed import RowMatrix\n",
        "\n",
        "M = RowMatrix(glass_sdf.select('RefractiveIndex','Na','Mg','Al','Si','K','Ca','Ba','Fe').rdd.map(\\\n",
        "  lambda row: Vectors.dense(list(row.asDict().values()))))\n",
        "\n",
        "pc = M.computePrincipalComponents(6)\n",
        "\n",
        "projected = M.multiply(pc)\n",
        "\n",
        "projected.rows.collect()"
      ],
      "metadata": {
        "id": "7CU6GFOah17H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## t-SNE\n",
        "\n",
        "For high-dimensional data, we often use t-Distributed Stochastic Neighbor Embedding (t-SNE) to reduce dimensionality.  This is a stochastic method so it doesn't always produce the same output.\n",
        "\n",
        "t-SNE isn't supported directly in Apache Spark (there is a 3rd party extension) but it's built into SciKit-Learn."
      ],
      "metadata": {
        "id": "1TpaMtPlh7gq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "X_embedded = TSNE(n_components=2).fit_transform(X)\n",
        "plt.scatter(X_embedded[:,0],X_embedded[:,1])"
      ],
      "metadata": {
        "id": "k_mHoh_Ph-Jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise\n",
        "\n",
        "Let's take another popular dataset, of housing prices in Boston."
      ],
      "metadata": {
        "id": "sZCvjQKHtT7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
        "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
        "\n",
        "X = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
        "y = raw_df.values[1::2, 2]"
      ],
      "metadata": {
        "id": "Ov0KcazkjiVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scaling\n",
        "\n",
        "Don't forget to rescale your data"
      ],
      "metadata": {
        "id": "ecukbmzznlGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# TODO"
      ],
      "metadata": {
        "id": "Xs0Lj054mpNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot a heatmap to see if there is a lot of correlation."
      ],
      "metadata": {
        "id": "hFJxRpE9nsHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: compute correlation matrix, plot heatmap"
      ],
      "metadata": {
        "id": "O9dYlQdLjoSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll fit a PCA model without reducing any\n",
        "# dimensions here\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA()\n",
        "pca.fit(X)"
      ],
      "metadata": {
        "id": "KpLz2_ZnmFLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now plot the explained variance and find the total number of dimensions that will get us to 95% or higher. Recall that this will count Dimension 0 so your count will be 1 more than the last component."
      ],
      "metadata": {
        "id": "nERH9YgJnzoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: explained variance ratio curve"
      ],
      "metadata": {
        "id": "pbrc1L2wmMvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How many dimensions should we use, to get 95% explained variance ratio?\n",
        "dimensions = # TODO for the number of component dimensions\n",
        "dimensions"
      ],
      "metadata": {
        "id": "dmGxgY_RmTNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grader.grade('pca', dimensions)"
      ],
      "metadata": {
        "id": "MICXh_o6HezG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CxSt80L-IeRB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}