{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zackives/upenn-cis5450-hw/blob/main/5_Module_2_Data_Modeling_and_Decomposition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49bwxLwZBJw7"
      },
      "source": [
        "# Lecture Module 2: Logical Design: Conceptual Data Representation\n",
        "\n",
        "## LinkedIn Social Analysis\n",
        "\n",
        "Our second module explores concepts in:\n",
        "\n",
        "* Designing data representations to capture important relationships\n",
        "* Reasoning over graphs\n",
        "* Exploring and traversing graphs\n",
        "\n",
        "\n",
        "Subsequently, in the next module, we'll look at how *physical design* (indexing, data layout) and *algorithms* can affect performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5MKpqZGmq1n"
      },
      "source": [
        "## Generality of Data Models\n",
        "\n",
        "We have claimed that data can be represented as a tree, as tables, or as graphs -- and all are equivalent. We'll see this in action here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsLl7bF1m3We"
      },
      "source": [
        "## Hierarchical data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnUGkvJmBJw8"
      },
      "source": [
        "### Preliminaries\n",
        "\n",
        "We'll use MongoDB on the cloud as a sample NoSQL database.\n",
        "\n",
        "We'll first collect Colab's host IP address, which you might need if you aren't able to connect to the database.  If you get an authorization error in connecting to MongoDB, you'll need to post this IP address to Ed Discussion so we can add permissions to make a request."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: fill in from Ed\n",
        "%env PASSWORD=#TODO"
      ],
      "metadata": {
        "id": "hdWPhkCZdthM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1bV-OVjn9Rw"
      },
      "outputs": [],
      "source": [
        "!curl ipecho.net/plain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTPLNZvgBJw8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!pip3 install pymongo\n",
        "!pip3 install lxml\n",
        "!pip3 install duckdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkbP2ZFpBJw-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# JSON parsing\n",
        "import json\n",
        "\n",
        "# HTML parsing\n",
        "from lxml import etree\n",
        "import urllib\n",
        "\n",
        "# DuckDB RDBMS\n",
        "import duckdb\n",
        "\n",
        "# Time conversions\n",
        "import time\n",
        "\n",
        "# NoSQL DB\n",
        "from pymongo import MongoClient\n",
        "from pymongo.errors import DuplicateKeyError, OperationFailure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y72TT7SnBJxA"
      },
      "source": [
        "## Our Example Dataset\n",
        "\n",
        "A crawl of LinkedIn, stored as a sequence of JSON objects (one per line).  Here's a scan through the sample dataset, taken from Kaggle (https://www.kaggle.com/linkedindata/linkedin-crawled-profiles-dataset).  We have subsequently removed all names of individuals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEYVnsAm0K0p"
      },
      "outputs": [],
      "source": [
        "!wget -nc https://storage.googleapis.com/penn-cis5450/linkedin_anon.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08GyADFtBJxA"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# 50K records from linkedin\n",
        "linked_in = open('linkedin_anon.jsonl')\n",
        "\n",
        "people = []\n",
        "\n",
        "for line in linked_in:\n",
        "    person = json.loads(line)\n",
        "    people.append(person)\n",
        "\n",
        "people_df = pd.DataFrame(people)\n",
        "print (\"%d records\"%len(people_df))\n",
        "\n",
        "people_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftYrnpfDBJxC"
      },
      "source": [
        "## NoSQL storage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_35KqvZBJxC"
      },
      "source": [
        "For this part we will give you read-only access to our copy of MongoDB.\n",
        "\n",
        "We may need to tell MongoDB to add your Colab IP address (so you can talk to the machine)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mInGhVcRBJxF",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Store in MongoDB and in an in-memory list\n",
        "\n",
        "START = 0\n",
        "# We already have the data loaded into MongoDB, so we won't actually\n",
        "# read all 50000 records.  We'll test by reading + writing the first\n",
        "# 3700 though!\n",
        "LIMIT = 37#00\n",
        "\n",
        "from pymongo.mongo_client import MongoClient\n",
        "from pymongo.server_api import ServerApi\n",
        "import pymongo.client_options\n",
        "import pymongo\n",
        "password = os.getenv('PASSWORD')\n",
        "\n",
        "if password is None:\n",
        "    raise Exception(\"You must set the PASSWORD environment variable\")\n",
        "\n",
        "uri = \"mongodb+srv://cis5450:\" + password + \"@test2450.3emsbl6.mongodb.net/?retryWrites=true&w=majority&appName=Test2450\"\n",
        "# Create a new client and connect to the server\n",
        "client = MongoClient(uri, server_api=ServerApi('1'))\n",
        "\n",
        "linkedin_db = client['linkedin']\n",
        "linked_in = open('linkedin_anon.jsonl')\n",
        "\n",
        "print('MongoDB has the following databases' + str(client.list_database_names()))\n",
        "\n",
        "people = 0\n",
        "for line in linked_in:\n",
        "    person = json.loads(line)\n",
        "    if people >= START:\n",
        "        try:\n",
        "            linkedin_db.posts.insert_one(person)\n",
        "        except DuplicateKeyError as e:\n",
        "            print (e)\n",
        "            pass\n",
        "        except OperationFailure as e:\n",
        "            # If the above still uses our cluster, you'll get this error in\n",
        "            # attempting to write to our MongoDB client because we haven't\n",
        "            # given you write access\n",
        "            if (\"user is not allowed to do action [insert]\" not in str(e)):\n",
        "              print (e)\n",
        "            pass\n",
        "    people = people + 1\n",
        "    if people >= LIMIT:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m02hh0WMEYld"
      },
      "outputs": [],
      "source": [
        "# Build a list of the JSON elements\n",
        "list_for_comparison = []\n",
        "\n",
        "people = 0\n",
        "for line in linked_in:\n",
        "    person = json.loads(line)\n",
        "    if people >= START:\n",
        "        try:\n",
        "            list_for_comparison.append(person)\n",
        "        except DuplicateKeyError:\n",
        "            pass\n",
        "        except OperationFailure:\n",
        "            # If the above still uses our cluster, you'll get this error in\n",
        "            # attempting to write to our MongoDB client\n",
        "            pass\n",
        "    people = people + 1\n",
        "    if people >= LIMIT:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slseAYgaOmp6"
      },
      "outputs": [],
      "source": [
        "list_for_comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwnOME1ABJxH"
      },
      "outputs": [],
      "source": [
        "# Two ways of looking up skills, one based on an in-memory\n",
        "# list, one based on MongoDB queries\n",
        "\n",
        "def find_skills_in_list(skill):\n",
        "    for post in list_for_comparison:\n",
        "        if 'skills' in post:\n",
        "            skills = post['skills']\n",
        "            if skills is not None:\n",
        "              for this_skill in skills:\n",
        "                  if this_skill == skill:\n",
        "                      return post\n",
        "    return None\n",
        "\n",
        "def find_skills_in_mongodb(skill):\n",
        "    return linkedin_db.posts.find_one({'skills': skill})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joIj39ItBJxI"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "find_skills_in_list('Marketing')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suDlZXyBBJxK"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "find_skills_in_mongodb('Marketing')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJTEefYTBJxL"
      },
      "source": [
        "## Designing a relational schema from hierarchical data\n",
        "\n",
        "Given that we already have a predefined set of fields / attributes / features, we don't need to spend a lot of time defining our table *schemas*, except that we need to unnest data.\n",
        "\n",
        "* Nested relationships can be captured by creating a second table, which has a **foreign key** pointing to the identifier (key) for the main (parent) table.\n",
        "* Ordered lists can be captured by encoding an index number or row number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHp7C_-CBJxL"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Simple code to pull out data from JSON and load into DuckDB.\n",
        "'''\n",
        "import ast\n",
        "\n",
        "linked_in = open('linkedin_anon.jsonl')\n",
        "\n",
        "START = 0\n",
        "LIMIT = 10000\n",
        "\n",
        "def get_df(rel):\n",
        "    ret = pd.DataFrame(rel)\n",
        "    return ret\n",
        "\n",
        "lines = []\n",
        "i = 1\n",
        "for line in linked_in:\n",
        "    if i > START + LIMIT:\n",
        "        break\n",
        "    elif i >= START:\n",
        "        person = json.loads(line)\n",
        "\n",
        "        lines.append(person)\n",
        "    i = i + 1\n",
        "\n",
        "people_df = get_df(pd.DataFrame(lines))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uljK5WbPgG9v"
      },
      "outputs": [],
      "source": [
        "people_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYo3um4AgUTu"
      },
      "outputs": [],
      "source": [
        "def get_nested_dict(rel, name):\n",
        "  # This evaluates the string that describes the dictionary, as a dictionary\n",
        "  # definition\n",
        "  ret = rel.copy()\n",
        "  # ret[name] = rel[name].map(lambda x: ast.literal_eval(x) if len(x) else np.NaN)\n",
        "  ret = ret.dropna()\n",
        "  # This joins rows on the index\n",
        "  return ret.drop(columns=name).join(pd.DataFrame(ret[name].tolist()))\n",
        "\n",
        "def get_nested_list(rel, name):\n",
        "  ret = rel.copy()\n",
        "  ret = ret.dropna().explode(name).dropna()\n",
        "  ret = ret.join(pd.DataFrame(ret[name].tolist())).drop(columns=name).drop_duplicates()\n",
        "  return ret.rename(columns={0: name})\n",
        "\n",
        "def get_nested_list_dict(rel, name):\n",
        "  ret = rel.copy()\n",
        "\n",
        "  ret = ret.dropna().explode(name)\n",
        "\n",
        "  exploded_pairs = pd.DataFrame(ret.apply(lambda x: {'_id': x['_id']} | x[name] if isinstance(x[name], dict) else {'_id': x['_id']}, axis=1).tolist())\n",
        "\n",
        "  return ret.merge(exploded_pairs, on='_id').drop(columns=name)\n",
        "  #pd.DataFrame(ret[name].tolist())).drop(columns=name).drop_duplicates()\n",
        "\n",
        "# Take the lists, drop any blank strings\n",
        "specialties_df = people_df[['_id','specilities']].explode('specilities').rename(columns={'_id': 'person'})\n",
        "specialties_df.dropna(inplace=True)\n",
        "interests_df = people_df[['_id','interests']].explode('interests').rename(columns={'_id': 'person'})\n",
        "interests_df.dropna(inplace=True)\n",
        "\n",
        "names_df = get_nested_dict(people_df[['_id','name']], 'name')\n",
        "\n",
        "education_df = get_nested_list_dict(people_df[['_id','education']], 'education')\n",
        "experience_df = get_nested_list_dict(people_df[['_id','experience']], 'experience')\n",
        "skills_df = get_nested_list(people_df[['_id','skills']], 'skills')\n",
        "honors_df = get_nested_list(people_df[['_id','honors']], 'honors')\n",
        "events_df = get_nested_list_dict(people_df[['_id','events']], 'events')\n",
        "\n",
        "groups_df = get_nested_dict(people_df[['_id','group']], 'group')\n",
        "\n",
        "people_df = people_df.drop(columns=['name','education','group','skills','experience','honors','events','specilities','interests'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqX245SNpNFx"
      },
      "outputs": [],
      "source": [
        "events_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxxQbW7xgmTp"
      },
      "outputs": [],
      "source": [
        "interests_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nC5VjmIgxVX"
      },
      "outputs": [],
      "source": [
        "specialties_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEk6beI3WMiF"
      },
      "outputs": [],
      "source": [
        "names_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSFWYc9f0jRH"
      },
      "outputs": [],
      "source": [
        "education_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXVf3M_IjgX0"
      },
      "outputs": [],
      "source": [
        "experience_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrJTEoDBzthw"
      },
      "outputs": [],
      "source": [
        "groups_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gccNilaEYVOM"
      },
      "outputs": [],
      "source": [
        "conn = duckdb.connect('linkedin.db')\n",
        "\n",
        "conn.sql('drop table if exists people')\n",
        "conn.sql('drop table if exists names')\n",
        "conn.sql('drop table if exists education')\n",
        "conn.sql('drop table if exists groups')\n",
        "conn.sql('drop table if exists skills')\n",
        "conn.sql('drop table if exists experience')\n",
        "conn.sql('drop table if exists honors')\n",
        "conn.sql('drop table if exists events')\n",
        "conn.sql('drop table if exists specialties')\n",
        "conn.sql('drop table if exists interests')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdvHKTGZBJxN"
      },
      "outputs": [],
      "source": [
        "# Save these to the SQLite database\n",
        "\n",
        "conn.sql(\"\"\"\n",
        "  CREATE TABLE IF NOT EXISTS people AS\n",
        "   SELECT * FROM people_df\n",
        "\"\"\")\n",
        "\n",
        "conn.sql(\"\"\"\n",
        "  CREATE TABLE IF NOT EXISTS names AS\n",
        "   SELECT * FROM names_df\n",
        "\"\"\")\n",
        "\n",
        "conn.sql(\"\"\"\n",
        "  CREATE TABLE IF NOT EXISTS education AS\n",
        "   SELECT * FROM education_df\n",
        "\"\"\")\n",
        "\n",
        "conn.sql(\"\"\"\n",
        "  CREATE TABLE IF NOT EXISTS groups AS\n",
        "   SELECT * FROM groups_df\n",
        "\"\"\")\n",
        "\n",
        "conn.sql(\"\"\"\n",
        "  CREATE TABLE IF NOT EXISTS skills AS\n",
        "   SELECT * FROM skills_df\n",
        "\"\"\")\n",
        "\n",
        "conn.sql(\"\"\"\n",
        "  CREATE TABLE IF NOT EXISTS experience AS\n",
        "   SELECT * FROM experience_df\n",
        "\"\"\")\n",
        "\n",
        "conn.sql(\"\"\"\n",
        "  CREATE TABLE IF NOT EXISTS honors AS\n",
        "   SELECT * FROM honors_df\n",
        "\"\"\")\n",
        "\n",
        "conn.sql(\"\"\"\n",
        "  CREATE TABLE IF NOT EXISTS events AS\n",
        "   SELECT * FROM events_df\n",
        "\"\"\")\n",
        "\n",
        "conn.sql(\"\"\"\n",
        "  CREATE TABLE IF NOT EXISTS specialties AS\n",
        "   SELECT * FROM specialties_df\n",
        "\"\"\")\n",
        "\n",
        "conn.sql(\"\"\"\n",
        "  CREATE TABLE IF NOT EXISTS interests AS\n",
        "   SELECT * FROM interests_df\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTuIrRu4BJxR"
      },
      "outputs": [],
      "source": [
        "conn.sql(\"\"\"\n",
        "  SELECT experience._id, org\n",
        "  FROM people\n",
        "  JOIN experience ON people._id=experience._id\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDTvxq7PBJxT"
      },
      "outputs": [],
      "source": [
        "conn.sql(\"\"\"\n",
        "  SELECT experience._id, group_concat(org) AS experience\n",
        "  FROM people\n",
        "  LEFT JOIN experience ON people._id=experience._id\n",
        "  GROUP BY experience._id\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIvIcS3xBJxU"
      },
      "source": [
        "## Views\n",
        "\n",
        "The following code starts a transaction (we can either `commit` or `rollback` at the end), removes an existing view, and creates a new one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYPXhgQ_BJxV",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "conn.sql('BEGIN TRANSACTION')\n",
        "conn.sql('DROP VIEW IF EXISTS people_experience')\n",
        "conn.execute(\"\"\"\n",
        "  CREATE VIEW IF NOT EXISTS people_experience AS\n",
        "    SELECT experience._id, group_concat(org) AS experience\n",
        "    FROM people\n",
        "    LEFT JOIN experience ON people._id=experience._id\n",
        "    GROUP BY experience._id\"\"\")\n",
        "conn.execute('COMMIT')\n",
        "\n",
        "# Treat the view as a table, see what's there\n",
        "conn.sql('SELECT * FROM people_experience')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O19yVoRKnvl"
      },
      "source": [
        "## Deep Dive: Converting a Complex Tree to Relations\n",
        "\n",
        "Now that we've seen the basics of taking hierarchical data and turning it into relations, let's put the LinkedIn data on the stack for a brief time, and try a more difficult exercise representing (and querying) tree-structured data.\n",
        "\n",
        "We'll take the HTML data from Wikipedia pages, seen in the Lecture 1 Notebook, and \"shred\" the HTML into tables.\n",
        "\n",
        "Briefly, if we think of the HTML as a tree of nodes, e.g.:\n",
        "\n",
        "```\n",
        "   <html>\n",
        "   |   |\n",
        "<head> <body>\n",
        "   |    |   |\n",
        "<title> <h1> <p>\n",
        "   |     |    \\\n",
        " ABC    ABC    DEF\n",
        "```\n",
        "\n",
        "Then we can give a **node ID** to each node in the tree; an a **position** (0, 1, ...) to each sibling at a level in the tree.  We will \"slice\" the tree into segments, each of which becomes a row in a table.  The row will include the node ID, the node label or type (\"h1\" or \"text()\"), the node value if the type is text(), and the position."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kC8brD1t8GAp"
      },
      "outputs": [],
      "source": [
        "import urllib\n",
        "from lxml import etree\n",
        "import pandas as pd\n",
        "import requests\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXVwvRF596zm"
      },
      "source": [
        "## HTML as edges\n",
        "\n",
        "Each time we parse an HTML node, we can give it a new ID.  If we record the ID of its parent, we essentially get an _edge_ going back to the parent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xo0mIN0YBJxW"
      },
      "outputs": [],
      "source": [
        "def import_html(url: str):\n",
        "  # Now let's read an HTML table!\n",
        "  headers = {\n",
        "      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "  }\n",
        "\n",
        "  return requests.get(url, headers=headers).text\n",
        "\n",
        "\n",
        "# Recursively crawl the node and add rows to the html_tree table\n",
        "def traverse_html(node, parent, pos, nodes_list) -> list:\n",
        "    if node.text and parent > -1 and len(str(node.text).strip()):\n",
        "        text_id = len(nodes_list)\n",
        "        entry = {'node_id': text_id, 'parent_node_id': parent, 'type_or_label': 'text()', 'pos': pos, 'value': str(node.text).strip()}\n",
        "        print (str(entry))\n",
        "        nodes_list.append(entry)\n",
        "\n",
        "    if node.tag:\n",
        "        node_id = len(nodes_list)\n",
        "        entry = {'node_id': node_id, 'parent_node_id': parent, 'type_or_label': node.tag, 'pos': pos, 'value': ''}\n",
        "        nodes_list.append(entry)\n",
        "        print (str(entry))\n",
        "        index = 0\n",
        "        for child in list(node):\n",
        "            (child_id, nodes_list) = traverse_html(child, node_id, index, nodes_list)\n",
        "            index = index + 1\n",
        "\n",
        "    if node.tail:\n",
        "        text_id = len(nodes_list)\n",
        "        entry = {'node_id': text_id, 'parent_node_id': parent, 'type_or_label': 'text()', 'pos': pos, 'value': node.tail}\n",
        "        print (str(entry))\n",
        "        nodes_list.append(entry)\n",
        "    return (node_id, nodes_list)\n",
        "\n",
        "pages_list = []\n",
        "nodes_list = []\n",
        "\n",
        "\n",
        "# Crawl these pages\n",
        "page_list = ['https://en.wikipedia.org/wiki/Tim_Cook',\n",
        "             'https://en.wikipedia.org/wiki/Chan_Zuckerberg_Initiative']\n",
        "for page in page_list:\n",
        "    page_content = import_html(page)\n",
        "    page_tree = etree.HTML(page_content)\n",
        "    (root_node,nodes_list) = traverse_html(page_tree, -1, 0, nodes_list)\n",
        "    pages_list.append({'url': page, 'root_id': root_node})\n",
        "\n",
        "pages_df = pd.DataFrame(pages_list)\n",
        "pages_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTR7QEOY-H3X"
      },
      "source": [
        "Let's see the nodes and the edges to their parents.  A parent of `-1` is a root node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcTlgUCfBJxY"
      },
      "outputs": [],
      "source": [
        "node_df = pd.DataFrame(nodes_list)\n",
        "node_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaZ-jC40-WCT"
      },
      "source": [
        "Let's look at the pages and the IDs of their root nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBbQcZ4jBJxa"
      },
      "outputs": [],
      "source": [
        "pages_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CC0Rkn_Q-h9y"
      },
      "source": [
        "From the pages, we can join the root nodes and see what the tags are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgW2ehz1BJxb"
      },
      "outputs": [],
      "source": [
        "# Find all document roots\n",
        "pages_df.merge(node_df,left_on=['root_id'],right_on=['node_id'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAIKmweT_Ks-"
      },
      "source": [
        "Now let's consider an XPath query to find all text within paragraphs.\n",
        "\n",
        "This would be `//p/text()`. We can evaluate this easily by just looking for `p` elements whose children are text. This can be done by joining between nodes with `p`s and nodes with text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMu-0w-_BJxd"
      },
      "outputs": [],
      "source": [
        "# Return the contents of all text() nodes inside of <p> tags\n",
        "\n",
        "node_df[node_df['type_or_label']=='p'][['node_id']].\\\n",
        "    merge(node_df[node_df['type_or_label']=='text()'], \\\n",
        "          left_on=['node_id'], right_on=['parent_node_id'])[['value']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-5UFXcVAITE"
      },
      "source": [
        "It's potentially more informative to see a bit of context.  Let's show (1) the node ID of the parent of the `p` tag, (2) the node ID of the `p` tag, (3) the node ID of the text node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiaUlP2rBJxg"
      },
      "outputs": [],
      "source": [
        "p_text_nodes = node_df[node_df['type_or_label']=='p'][['parent_node_id','node_id']].\\\n",
        "    merge(node_df[node_df['type_or_label']=='text()'][['parent_node_id','node_id']], \\\n",
        "          left_on=['node_id'], right_on=['parent_node_id']).\\\n",
        "    rename(columns={'parent_node_id_x': 'p_parent_node_id', 'node_id_y': 'text_node_id'}).\\\n",
        "    drop(columns='node_id_x').rename(columns={'parent_node_id_y': 'p_node_id'})\n",
        "\n",
        "p_text_nodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhdZ-Mh8BKAQ"
      },
      "source": [
        "What can we say about the types of the parent's of the `p`-nodes?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qowiqu8HBJxk"
      },
      "outputs": [],
      "source": [
        "current_items_df = p_text_nodes.rename(columns={'p_parent_node_id': 'ancestor_node_id'})\n",
        "\n",
        "parents_df = current_items_df[['ancestor_node_id','text_node_id']].\\\n",
        "    merge(node_df,\\\n",
        "    left_on=['ancestor_node_id'],right_on=['node_id'])\\\n",
        "    [['parent_node_id','text_node_id','type_or_label']].\\\n",
        "rename(columns={'parent_node_id': 'ancestor_node_id'})\n",
        "\n",
        "parents_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H45TnQ9gBXth"
      },
      "source": [
        "And we can even traverse once more, to the parents of the parents!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fbd3mmY4BJxl"
      },
      "outputs": [],
      "source": [
        "current_items_df = parents_df\n",
        "\n",
        "grandparents_df = current_items_df[['ancestor_node_id','text_node_id']].drop_duplicates().\\\n",
        "    merge(node_df,\\\n",
        "    left_on=['ancestor_node_id'],right_on=['node_id'])\\\n",
        "    [['parent_node_id','text_node_id','type_or_label']].\\\n",
        "rename(columns={'parent_node_id': 'ancestor_node_id'}).drop_duplicates()\n",
        "\n",
        "grandparents_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlYD2vNOBldX"
      },
      "source": [
        "## Recursively find all ancestors!\n",
        "\n",
        "We can start with the text nodes, then find their parents, then find their parents, then ...\n",
        "\n",
        "This is a recursive process that stops when there aren't any more parents, and is called a *transitive closure* because it includes the full set of all transitively related nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAag8eq4BJxn"
      },
      "outputs": [],
      "source": [
        "def find_ancestor_nodes(node_df, current_items_df):\n",
        "    if len(current_items_df) == 0:\n",
        "        return current_items_df\n",
        "    else:\n",
        "        parents_df = current_items_df[['ancestor_node_id','text_node_id']].drop_duplicates().\\\n",
        "            merge(node_df,\\\n",
        "            left_on=['ancestor_node_id'],right_on=['node_id'])\\\n",
        "            [['parent_node_id','text_node_id','type_or_label']].\\\n",
        "        rename(columns={'parent_node_id': 'ancestor_node_id'}).drop_duplicates()\n",
        "\n",
        "        return pd.concat([parents_df,find_ancestor_nodes(node_df, parents_df)]).drop_duplicates()\n",
        "\n",
        "nodes_ancestors = find_ancestor_nodes(node_df, p_text_nodes.rename(columns={'p_parent_node_id': 'ancestor_node_id'}))\n",
        "\n",
        "nodes_ancestors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kheEpBriBJxo"
      },
      "outputs": [],
      "source": [
        "# Can we find ONLY text from the Tim Cook (0th) document?\n",
        "\n",
        "nodes_ancestors[nodes_ancestors['ancestor_node_id']==pages_df.iloc[0]['root_id']].\\\n",
        "    merge(node_df, left_on=['text_node_id'],right_on=['node_id'])[['text_node_id','value']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "549DVdWFCzpU"
      },
      "source": [
        "## Exercises"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile notebook-config.yaml\n",
        "\n",
        "grader_api_url: 'https://23whrwph9h.execute-api.us-east-1.amazonaws.com/default/Grader23'\n",
        "grader_api_key: 'flfkE736fA6Z8GxMDJe2q8Kfk8UDqjsG3GVqOFOa'"
      ],
      "metadata": {
        "id": "jh417BeRJxku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install penngrader-client"
      ],
      "metadata": {
        "id": "APSgWPAJJz6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW WHO\n",
        "#TO ASSIGN POINTS TO YOU IN OUR BACKEND\n",
        "STUDENT_ID = 99999999 # YOUR PENN-ID GOES HERE AS AN INTEGER##PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW WHO"
      ],
      "metadata": {
        "id": "2sTlQ7eYJ0XX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%set_env HW_ID=cis5450_25f_HW9"
      ],
      "metadata": {
        "id": "2yZABc2pJ2ZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from penngrader.grader import *\n",
        "\n",
        "grader = PennGrader('notebook-config.yaml', os.environ['HW_ID'], STUDENT_ID, STUDENT_ID)"
      ],
      "metadata": {
        "id": "KdLuR4QRJ7_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8L1FQOvhFnZh"
      },
      "source": [
        "\n",
        "\n",
        "Can we find *all paragraphs* from the Zuckerberg (position-1) document, that have text children? (Hint: you can get all text nodes in the document, then find their parents). We'll give you a starting point here:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zuckerberg_root_id = pages_df.iloc[1]['root_id']\n",
        "zuckerberg_root_id"
      ],
      "metadata": {
        "id": "h5nZnC9q3j-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZtumsvQ9SKT"
      },
      "outputs": [],
      "source": [
        "# TODO: Return (node_id, type_or_label) as results_df\n",
        "\n",
        "results_df = # something\n",
        "\n",
        "results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quick-check for your columns."
      ],
      "metadata": {
        "id": "5yfUMQO3m4lv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPw6FFHPD8Fh"
      },
      "outputs": [],
      "source": [
        "assert list(results_df.columns)==['node_id', 'type_or_label']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5o-8YsFFb8R"
      },
      "source": [
        "And submit!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yec6Lb6GDACG"
      },
      "outputs": [],
      "source": [
        "grader.grade('zuckerberg', results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHNtg3ZNFcm-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}