{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zackives/upenn-cis5450-hw/blob/main/16_Module_3_Notebook_VI_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cy6hpgnJY-Rp"
      },
      "source": [
        "# Supervised Machine Learning and Linear/Logistic Regression\n",
        "\n",
        "Let's consider **supervised** machine learning, which learns a function from input features to output classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7pUp7HAwQp6"
      },
      "source": [
        "## Autograder setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGM-vv4owbBQ"
      },
      "outputs": [],
      "source": [
        "#PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW WHO\n",
        "#TO ASSIGN POINTS TO YOU IN OUR BACKEND\n",
        "STUDENT_ID = 99999999 # YOUR PENN-ID GOES HERE AS AN INTEGER##PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW WHO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxKJ5J_awSOn"
      },
      "outputs": [],
      "source": [
        "%%writefile notebook-config.yaml\n",
        "\n",
        "grader_api_url: 'https://23whrwph9h.execute-api.us-east-1.amazonaws.com/default/Grader23'\n",
        "grader_api_key: 'flfkE736fA6Z8GxMDJe2q8Kfk8UDqjsG3GVqOFOa'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfX8o5jjwe0v"
      },
      "outputs": [],
      "source": [
        "%set_env HW_ID=cis2450_25f_HW9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3n0LqhuwW2i"
      },
      "outputs": [],
      "source": [
        "!pip3 install penngrader-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEHXUnRqwcP7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from penngrader.grader import *\n",
        "\n",
        "grader = PennGrader('notebook-config.yaml', os.environ['HW_ID'], STUDENT_ID, STUDENT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uC1mN3D7OBCh"
      },
      "source": [
        "## Simple Linear Regression\n",
        "\n",
        "Let's start with a really simple problem, which is fitting a line (linear function) to a set of points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-srfxYshYCI0"
      },
      "source": [
        "Here are the points..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gvay8_1TOCpX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Let's plot a series of points\n",
        "X = 2 * np.random.rand(200, 1)\n",
        "\n",
        "# We are plotting y = 2x + 3 plus a random\n",
        "# value\n",
        "y = 2 * X + 3 + np.random.randn(200, 1)\n",
        "\n",
        "# Let's plot it\n",
        "fig, ax = plt.subplots(figsize=(8,8))\n",
        "ax.scatter(X, y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PV8ec7VjYDz7"
      },
      "source": [
        "Here's a simple machine learning model -- Linear Regression -- that finds the slope + intercept (sometimes called the weight + bias) for the best-fit line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kwuyq8uhYDd4"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Use regression to get slope + intercept for\n",
        "# a line matching the points\n",
        "rgr = LinearRegression()\n",
        "rgr.fit(X, y)\n",
        "\n",
        "# Let's plot it\n",
        "fig, ax = plt.subplots(figsize=(8,8))\n",
        "ax.scatter(X, y)\n",
        "\n",
        "# The line will be from 0 -> 3 non-inclusive\n",
        "X2 = range(0,3)\n",
        "y2 = rgr.coef_*X2 + rgr.intercept_\n",
        "ax.plot(X2, y2.T, color='red')\n",
        "plt.show()\n",
        "\n",
        "print (\"Intercept:\", rgr.intercept_)\n",
        "print (\"Coefficient:\",rgr.coef_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpoE_7p4YTcv"
      },
      "source": [
        "How can we actually do this ourselves?\n",
        "\n",
        "We can use the \"normal equation\" $(X^T X)^{-1} X^T y$ to compute this using a closed form solution.  It's a bit expensive, and it does require that we add the intercept $x_0=1$ to the $X$ matrix..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPhRcyytYUUx"
      },
      "outputs": [],
      "source": [
        "from numpy.linalg import inv\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Insert the x_0 column\n",
        "X_with_bias = np.ones((X.shape[0],2))\n",
        "X_with_bias[:,1] = X.T\n",
        "\n",
        "b = inv(X_with_bias.T.dot(X_with_bias)).dot(X_with_bias.T).dot(y)\n",
        "print('Intercept: ', b[0])\n",
        "print('Coefficient: ', b[1])\n",
        "\n",
        "# predict using coefficients\n",
        "yhat = X_with_bias.dot(b)\n",
        "# plot data and predictions\n",
        "# Let's plot it\n",
        "fig, ax = plt.subplots(figsize=(8,8))\n",
        "ax.scatter(X, y)\n",
        "\n",
        "plt.plot(X, yhat, color='red')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igN5iBFBYaQG"
      },
      "source": [
        "## Linear Regression for Real Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riPVXc34Yclh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
        "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
        "\n",
        "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
        "target = raw_df.values[1::2, 2]\n",
        "\n",
        "feature_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
        "\n",
        "housing_df = pd.DataFrame(data, columns=feature_names)\n",
        "housing_df['Price'] = target\n",
        "housing_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCbFNiSaZFlz"
      },
      "outputs": [],
      "source": [
        "housing_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfbJ1YloZHCH"
      },
      "outputs": [],
      "source": [
        "housing_df.hist(bins=50, figsize=(20,15))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6E10zmXaVvA"
      },
      "outputs": [],
      "source": [
        "housing_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mVce9avacx9"
      },
      "outputs": [],
      "source": [
        "y = target\n",
        "X = data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozAy5XSVaoDp"
      },
      "outputs": [],
      "source": [
        "# Is the data correlated?\n",
        "corr_matrix = housing_df.corr()\n",
        "\n",
        "import seaborn as sn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(15,8))\n",
        "sn.heatmap(corr_matrix, annot=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIG22acSa3F0"
      },
      "source": [
        "Let's build a model!\n",
        "\n",
        "First, maybe we should scale, decompose, etc.\n",
        "\n",
        "Given the above correlation matrix, note there are high correlations between some of the fields. Maybe it's a good idea to use PCA to create new components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAv6dZ_6a4Ha"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Create our training and test sets.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
        "\n",
        "# Normally we should scale and run PCA *only on the training data*, pretending\n",
        "# we can't see the test data\n",
        "scaler = StandardScaler()\n",
        "X2_train = scaler.fit_transform(X_train)\n",
        "\n",
        "# But we obviously need to transform as well\n",
        "X2_test = scaler.transform(X_test)\n",
        "\n",
        "pca = PCA(n_components=13)\n",
        "X3_train = pca.fit_transform(X2_train)\n",
        "X3_test = pca.transform(X2_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwY0xl4nchVT"
      },
      "source": [
        "How much do the components contribute?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzrrJc-kcR_B"
      },
      "outputs": [],
      "source": [
        "pca.explained_variance_ratio_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f2ogYkXci0C"
      },
      "source": [
        "Note that the last 2-3 components aren't super useful. We could easily reduce dimensionality to, say, 11 dimensions. But for now we'll keep things lossless."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTq1o5PJbhDY"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import accuracy_score, mean_absolute_error, mean_squared_error\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "clf = LinearRegression()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "predictions = clf.predict(X_test)\n",
        "\n",
        "print(\"Score:\", clf.score(X_test, y_test))\n",
        "plt.figure(dpi=100)\n",
        "plt.scatter(y_test, predictions)\n",
        "\n",
        "# Different error measures\n",
        "print(\"MAE:\", mean_absolute_error(y_test, predictions))\n",
        "print('MSE:', mean_squared_error(y_test, predictions))\n",
        "print('RMSE:', np.sqrt(mean_squared_error(y_test, predictions)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SZudyJhbpMY"
      },
      "outputs": [],
      "source": [
        "print(clf.intercept_)\n",
        "print(clf.coef_)\n",
        "plt.figure(dpi=100)\n",
        "#plt.bar(housing_data.feature_names,clf.coef_)\n",
        "plt.bar(['c0','c1','c2','c3','c4','c5','c6','c7','c8','c9','c10','c11','c12'], clf.coef_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1Xqr-IVbt7T"
      },
      "source": [
        "## Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUxqHOcIbvlM"
      },
      "source": [
        "As a first step, we can use *lasso* or L1 regularization, which produces *sparse weights*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfBGwIeBb3AE"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "clf = Lasso()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "predictions = clf.predict(X_test)\n",
        "\n",
        "print(\"Score:\", clf.score(X_test, y_test))\n",
        "\n",
        "# Different error measures\n",
        "print(\"MAE:\", mean_absolute_error(y_test, predictions))\n",
        "print('MSE:', mean_squared_error(y_test, predictions))\n",
        "print('RMSE:', np.sqrt(mean_squared_error(y_test, predictions)))\n",
        "\n",
        "print(clf.coef_)\n",
        "plt.figure(dpi=100)\n",
        "#plt.bar(feature_names,clf.coef_)\n",
        "plt.bar(['c0','c1','c2','c3','c4','c5','c6','c7','c8','c9','c10','c11','c12'], clf.coef_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15vvXb-2b_2O"
      },
      "source": [
        "What about L2 (ridge)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NfQkhF9cB7w"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "clf = Ridge()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "predictions = clf.predict(X_test)\n",
        "\n",
        "print(\"Score:\", clf.score(X_test, y_test))\n",
        "\n",
        "# Different error measures\n",
        "print(\"MAE:\", mean_absolute_error(y_test, predictions))\n",
        "print('MSE:', mean_squared_error(y_test, predictions))\n",
        "print('RMSE:', np.sqrt(mean_squared_error(y_test, predictions)))\n",
        "\n",
        "print(clf.coef_)\n",
        "plt.figure(dpi=100)\n",
        "# plt.bar(feature_names,clf.coef_)\n",
        "plt.bar(['c0','c1','c2','c3','c4','c5','c6','c7','c8','c9','c10','c11','c12'], clf.coef_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYdYvJ7rcrjD"
      },
      "source": [
        "And finally, what if we put the two together, in the form of Elastic Net?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBR8Zx0ccuL_"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import ElasticNet\n",
        "\n",
        "\n",
        "clf = ElasticNet()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "predictions = clf.predict(X_test)\n",
        "\n",
        "print(\"Score:\", clf.score(X_test, y_test))\n",
        "\n",
        "# Different error measures\n",
        "print(\"MAE:\", mean_absolute_error(y_test, predictions))\n",
        "print('MSE:', mean_squared_error(y_test, predictions))\n",
        "print('RMSE:', np.sqrt(mean_squared_error(y_test, predictions)))\n",
        "\n",
        "print(clf.coef_)\n",
        "plt.figure(dpi=100)\n",
        "# plt.bar(feature_names,clf.coef_)\n",
        "plt.bar(['c0','c1','c2','c3','c4','c5','c6','c7','c8','c9','c10','c11','c12'], clf.coef_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GgdTH3tc6Qy"
      },
      "source": [
        "## Logistic Regression\n",
        "\n",
        "Logistic regression uses techniques from linear regression but tries to map to Boolean values.\n",
        "\n",
        "### The Logistic Function\n",
        "\n",
        "Let's look at the **logit** or **logistic** function, also called the **sigmoid** function and indicated by $\\sigma$, which is \"near linear\" but flattens at 0 and 1.  Let's look at several variations, also vs a linear and step function..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DT3uzdUndBOj"
      },
      "outputs": [],
      "source": [
        "X = [x * 0.01 for x in range(-400,400)]\n",
        "\n",
        "def logit_1(x):\n",
        "  return 1 / (1 + np.exp(-2*x))\n",
        "\n",
        "def logit_2(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def logit_3(x):\n",
        "  return 1 / (1 + np.exp(-(x * 0.5)))\n",
        "\n",
        "\n",
        "# Let's plot it\n",
        "fig, ax = plt.subplots(figsize=(8,8))\n",
        "ax.plot(X, [1 if x >= 0 else 0 for x in X], color='gray', label='x>=0')\n",
        "ax.plot(X, [x/8+0.5 for x in X], color='brown', label='x/8 + 1/2')\n",
        "ax.plot(X, [logit_1(x) for x in X], color='red', label='1/(1+exp(-2x))')\n",
        "ax.plot(X, [logit_2(x) for x in X], color='blue', label='1/(1+exp(-x))')\n",
        "ax.plot(X, [logit_3(x) for x in X], color='green', label='1/(1+exp(-x/2))')\n",
        "ax.set_title('Boolean step vs linear vs logistic functions',fontsize='xx-large')\n",
        "ax.set_xlim(-4, 4)\n",
        "ax.set_ylim(-0.1, 1.1)\n",
        "ax.legend(fontsize='x-large')\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cemLOyEfdEGS"
      },
      "source": [
        "### Using Logistic Regression in SciKit-Learn\n",
        "\n",
        "We can easily use a logistic regression classifier from SciKit to train and make predictions.\n",
        "\n",
        "For simplicity, we'll start without scaling the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YG_veqLdHKA"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "\n",
        "dataset = load_wine()\n",
        "dataset.feature_names\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn.metrics\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataset.data, \\\n",
        "                                                    dataset.target, \\\n",
        "                                                    test_size=0.40)\n",
        "\n",
        "clf = LogisticRegression(max_iter=10000)\n",
        "clf.fit(X_train,y_train)\n",
        "prediction = clf.predict(X_test)\n",
        "\n",
        "accuracy = sklearn.metrics.accuracy_score(prediction,y_test)\n",
        "print(\"Accuracy: %.1f%%\"% (accuracy*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrhYpXIJdT2Y"
      },
      "source": [
        "Of course, we should scale the data first. We can create multi-stage *pipelines*..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tL-S73AzdZMV"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "scl = StandardScaler()\n",
        "\n",
        "pca = PCA()\n",
        "\n",
        "clf = LogisticRegression(max_iter=10000, penalty='l2')\n",
        "\n",
        "pipe = Pipeline(steps=[('Scale',scl),('PCA',pca),('LogReg',clf)])\n",
        "\n",
        "pipe.fit(X_train,y_train)\n",
        "\n",
        "prediction = pipe.predict(X_test)\n",
        "\n",
        "accuracy = sklearn.metrics.accuracy_score(prediction,y_test)\n",
        "print(\"Accuracy: %.1f%%\"% (accuracy*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh6BHiladtgv"
      },
      "source": [
        "## Gradient Descent\n",
        "\n",
        "Gradient descent consists of taking *steps* towards our optimal values (for weights).\n",
        "\n",
        "For linear regression without regularization, there is a direct closed-form solution. For linear regression with regularization, or for logistic regression, we need to use gradient descent.\n",
        "\n",
        "### Gradient Descent with a Linear Function\n",
        "\n",
        "To look at gradient descent, let's first consider a simple example based on our wine dataset, with a linear value for our prediction.  (We'll relax this in a moment to consider the logistic function.)\n",
        "\n",
        "We can define the cost function to be Mean Squared Error as follows:\n",
        "\n",
        "$MSE = {1 \\over n}\\Sigma^n_{i=1}(\\hat{y}^{(i)} - y^{(i)})$\n",
        "\n",
        "where\n",
        "\n",
        "$\\hat{y}^{(i)} = \\sigma(w^T \\cdot x^{(i)})$.\n",
        "\n",
        "(Recall $\\sigma$ is the sigmoid function.)\n",
        "\n",
        "For linear regression let's set up a simple line-fit:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqsOsI9VeQ9r"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Let's plot a series of points\n",
        "X = 2 * np.random.rand(200, 1)\n",
        "\n",
        "# We are plotting y = 2x + 3 plus a random\n",
        "# value\n",
        "y = 2 * X + 3 + np.random.randn(200, 1)\n",
        "\n",
        "# Let's plot it\n",
        "fig, ax = plt.subplots(figsize=(8,8))\n",
        "ax.scatter(X, y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJ2N0KpieSsN"
      },
      "source": [
        "Test and training sets..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6I3Jwm9HeUJ9"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn.metrics\n",
        "\n",
        "# Change X-vector to include an additional feature with value 1,\n",
        "# for the bias/intercept term\n",
        "\n",
        "X2 = np.ones((X.shape[0],2))\n",
        "X2[:,1] = X.T\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X2, \\\n",
        "                                                    y, \\\n",
        "                                                    test_size=0.30)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4RYfkZEec5e"
      },
      "source": [
        "Let's drill down to one of the features (dimensions) and see how the cost (weight) relates to the (squared) error of the overall cost function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNSq8GnLebiB"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "x_coord = range(-5,15)\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "w = np.random.randn(2)\n",
        "\n",
        "# Here's our cost function, as mean squared error\n",
        "def cost_fn(X, y, p, fn):\n",
        "  if len(X.shape) == 1:\n",
        "    # Single row, return the squared error\n",
        "    return np.square(y - fn(X[p]))\n",
        "  else:\n",
        "    # Matrix, return the mean of the squared errors\n",
        "    return np.average([cost_fn(X[i], y[i], p, fn) for i in range (0, X.shape[0])])\n",
        "\n",
        "# Now let's plot the error function for a *single instance*, for each feature,\n",
        "# for a range of weights and the *linear* function x*w\n",
        "for feature in range(0, 2):\n",
        "  ax = plt.subplot(1, 2, feature+1)\n",
        "  ax.plot(x_coord, [cost_fn(X_train, y_train, feature, lambda x: x*w) for w in x_coord])\n",
        "  ax.set_ylabel('Error for Feature ' + str(feature))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuaiOkd9em1I"
      },
      "source": [
        "Since we have squared error, this should of course be a parabola!\n",
        "\n",
        "The bottom of the parabola is the optimal weight for this particular dimension, and is what we are searching for.\n",
        "\n",
        "Now imagine we are doing this simultaneously in all dimensions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHfQ_fd7e3wP"
      },
      "source": [
        "### Gradient Descent Function\n",
        "\n",
        "The function takes a *step* eta, the weights $w$, and $X$ and $y$. We look at the overall error and step along each dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnoYpfvAe2x4"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(epochs, eta, X, w, y):\n",
        "  \"\"\"\n",
        "  The gradient descent iterates for *epochs* rounds, making a step\n",
        "  of size eta.  It will be adjusting w, based on the prediction for each\n",
        "  instance vs y and the overall error.\n",
        "  \"\"\"\n",
        "  # We'll use this list to accumulate\n",
        "  # the error\n",
        "  overall_error = []\n",
        "  # Iterate over each epoch\n",
        "  for i in range(epochs):\n",
        "    # This is y-hat, the predictions for each\n",
        "    # class label before they are thresholded\n",
        "    # based on comparison with 0.5\n",
        "    predictions = X.dot(w)\n",
        "\n",
        "    # The overall error, as a vector\n",
        "    error = (predictions - y)\n",
        "\n",
        "    # Record the MSE so we can plot it\n",
        "    mean_sq_error = np.sum(error ** 2) / X.shape[0]\n",
        "    overall_error.append(mean_sq_error)\n",
        "\n",
        "    # Now we update the weights.\n",
        "    # The gradient is based on the partial derivative\n",
        "    # of the MSE with respect to w.\n",
        "    gradient = 2 / X.shape[0] * X.T.dot(error)\n",
        "\n",
        "    w = w - eta * gradient\n",
        "\n",
        "  return w, overall_error\n",
        "\n",
        "# Training rounds or epochs\n",
        "epochs = 500\n",
        "eta = 0.01\n",
        "\n",
        "w = np.zeros((2,1))\n",
        "w[:,0] = np.random.randn(2)\n",
        "\n",
        "weights, evect = gradient_descent(epochs, eta, X_train, w, y_train)\n",
        "print (weights)\n",
        "print (evect)\n",
        "\n",
        "# Plot the mean-squared error\n",
        "plt.plot(range(0,epochs), evect)\n",
        "plt.xlabel('Gradient Descent Epoch #')\n",
        "plt.ylabel('Mean-squared error (loss)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcgr9UgifGX4"
      },
      "source": [
        "## Gradient Descent for Training Logistic Regression\n",
        "\n",
        "Let's try this with logistic regression, using the sigmoid function..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inR1t2rlfKlr"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "(X, y) = make_blobs(n_samples=1000, n_features=2, centers=2, \\\n",
        "                    cluster_std=1.10, random_state=42)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt.scatter(x=X[:, 0], y=X[:, 1], c=y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, \\\n",
        "                                                    y, \\\n",
        "                                                    test_size=0.30)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9HlynuBfMNd"
      },
      "source": [
        "First' let's look at the shape of the cost functions for $y_i=1$, $y_i=0$...  Recall that here we use *log loss*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HM54UTtkfR5J"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "def prediction(x):\n",
        "  return 1.0 / (1 + np.exp(-x))\n",
        "\n",
        "x = [(i - 500)/125 for i in range(1000)]\n",
        "\n",
        "# Recall the cost function\n",
        "plt.plot(x,[-math.log(prediction(i)) for i in x])\n",
        "plt.plot(x,[-math.log(1 - prediction(i)) for i in x])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUtGxS-_faZy"
      },
      "source": [
        "The derivative of the log loss function, over the sigmoid applied to the dot product of weights and instance, is almost identical to the derivative of the SSE function with linear regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baiM7t4Vf1gA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# Here is our sigmoid function for making\n",
        "# predictions with logistic regression\n",
        "# or with perceptron-style neural nets\n",
        "def prediction(x):\n",
        "  return 1.0 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def gradient_descent(epochs, eta, X, w, y):\n",
        "  \"\"\"\n",
        "  The gradient descent iterates for *epochs* rounds, making a step\n",
        "  of size eta.  It will be adjusting w, based on the prediction for each\n",
        "  instance vs y and the overall error.\n",
        "  \"\"\"\n",
        "  # We'll use this list to accumulate\n",
        "  # the error\n",
        "  overall_error = []\n",
        "  # Iterate over each epoch\n",
        "  for i in range(epochs):\n",
        "    # This is y-hat, the predictions for each\n",
        "    # class label before they are thresholded\n",
        "    # based on comparison with 0.5\n",
        "    predictions = prediction(X.dot(w))\n",
        "\n",
        "    # The overall error, as a vector\n",
        "    error = (predictions - y)\n",
        "\n",
        "    # Record the log loss so we can plot it\n",
        "    mean_sq_error = log_loss(y, predictions)\n",
        "    overall_error.append(mean_sq_error)\n",
        "\n",
        "    # Now we update the weights.\n",
        "    # The gradient is based on the partial derivative\n",
        "    # of the log loss with respect to w.\n",
        "    gradient = 1 / X.shape[0] * X.T.dot(error)\n",
        "\n",
        "    w = w - eta * gradient\n",
        "\n",
        "  return w, overall_error\n",
        "\n",
        "# Training rounds or epochs\n",
        "epochs = 500\n",
        "eta = 0.001\n",
        "\n",
        "w = np.random.randn(2)\n",
        "\n",
        "weights, evect = gradient_descent(epochs, eta, X, w, y)\n",
        "print (weights)\n",
        "print (evect)\n",
        "\n",
        "# Plot the mean-squared error\n",
        "plt.plot(range(0,epochs), evect)\n",
        "plt.xlabel('Gradient Descent Epoch #')\n",
        "plt.ylabel('Log loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CD2kUY56lsRN"
      },
      "source": [
        "How close do we get to optimal? Let's plot where we set the weight, versus the minimum. Given the size of eta we won't necessarily get to the exact minimum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-ZS6-ZYlPCp"
      },
      "outputs": [],
      "source": [
        "x_coord = range(-10,10)\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Now let's plot the error function for each feature,\n",
        "# for a range of weights and the output of the sigmoid function over x*w\n",
        "for feature in range(0, 2):\n",
        "  ax = plt.subplot(1, 2, feature+1)\n",
        "  ax.plot(x_coord, [cost_fn(X_train, y_train, feature, lambda x: prediction(x*w)) for w in x_coord],\n",
        "          marker='x')\n",
        "\n",
        "  ax.axvline(x=weights[feature], color='red')\n",
        "  ax.set_ylabel('Cost vs weight on feature ' + str(feature))\n",
        "  ax.set_xlabel('Weight w' + str(feature))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7gJu0cDlzRL"
      },
      "source": [
        "Is it good enough to make predictions?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rw_g-JEHlX4r"
      },
      "outputs": [],
      "source": [
        "for item in range(len(X_test)):\n",
        "  predicted_label = 0 if prediction(X_test[item].dot(weights)) < 0.5 else 1\n",
        "\n",
        "  print('Prediction {} vs {}'.format(predicted_label, y_test[item]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p17GfDuPfYDi"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZCvjQKHtT7p"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "Suppose we are training a Logistic Regression classifier.  Someone tells you your classifier is *overfitting* to the data.  You would want to:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ov0KcazkjiVY"
      },
      "outputs": [],
      "source": [
        "# TODO: pick an answer:\n",
        "options = ['Increase L1 regularization',\n",
        "           'Reduce L1 regularization']\n",
        "\n",
        "\n",
        "my_option = options[] # TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MICXh_o6HezG"
      },
      "outputs": [],
      "source": [
        "grader.grade('regularization', my_option)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxSt80L-IeRB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}